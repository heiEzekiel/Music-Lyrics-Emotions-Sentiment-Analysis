{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install transformers\n",
    "# !pip install openpyxl\n",
    "# !pip install seaborn\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Trainer, TrainingArguments\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"Dataset\")\n",
    "output_dir = os.path.join(os.getcwd(), \"models\", \"roberta-base_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31903</th>\n",
       "      <td>worry</td>\n",
       "      <td>reesnicole twitter reunion would awesome meet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44191</th>\n",
       "      <td>fear</td>\n",
       "      <td>wish know really feel aside read nervous twitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10027</th>\n",
       "      <td>worry</td>\n",
       "      <td>bad day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54181</th>\n",
       "      <td>anger</td>\n",
       "      <td>wish could bottle squeal delight take whenever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37636</th>\n",
       "      <td>relief</td>\n",
       "      <td>fear creep already dismiss let thing right coz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                            content\n",
       "31903     worry  reesnicole twitter reunion would awesome meet ...\n",
       "44191      fear    wish know really feel aside read nervous twitch\n",
       "10027     worry                                            bad day\n",
       "54181     anger  wish could bottle squeal delight take whenever...\n",
       "37636    relief  fear creep already dismiss let thing right coz..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df = pd.read_csv(os.path.join(data_dir,\"emotions_processed_df.csv\"))\n",
    "processed_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30348</th>\n",
       "      <td>neutral</td>\n",
       "      <td>good morning six hundred and fifteen plan day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16126</th>\n",
       "      <td>worry</td>\n",
       "      <td>jesamine appropriate tweet everything ok last ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28361</th>\n",
       "      <td>worry</td>\n",
       "      <td>speak new tweeples get acquaint never speak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26079</th>\n",
       "      <td>happiness</td>\n",
       "      <td>evanmcbroom sure mayor brainard thrill hear fan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17131</th>\n",
       "      <td>empty</td>\n",
       "      <td>start get annoy socialscope need update</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          labels                                               text\n",
       "30348    neutral  good morning six hundred and fifteen plan day ...\n",
       "16126      worry  jesamine appropriate tweet everything ok last ...\n",
       "28361      worry        speak new tweeples get acquaint never speak\n",
       "26079  happiness    evanmcbroom sure mayor brainard thrill hear fan\n",
       "17131      empty            start get annoy socialscope need update"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df = processed_df.rename(columns={\"sentiment\": \"labels\", \"content\": \"text\"})\n",
    "processed_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'love',\n",
       "       'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger', 'fear',\n",
       "       'happy'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df[\"labels\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels     0\n",
       "text      15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels    0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df = processed_df.dropna()\n",
    "processed_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(processed_df['labels'])\n",
    "X = processed_df.iloc[:,1:2]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'anger', 1: 'boredom', 2: 'empty', 3: 'enthusiasm', 4: 'fear', 5: 'fun', 6: 'happiness', 7: 'happy', 8: 'hate', 9: 'love', 10: 'neutral', 11: 'relief', 12: 'sadness', 13: 'worry'}\n",
      "0 anger\n",
      "1 boredom\n",
      "2 empty\n",
      "3 enthusiasm\n",
      "4 fear\n",
      "5 fun\n",
      "6 happiness\n",
      "7 happy\n",
      "8 hate\n",
      "9 love\n",
      "10 neutral\n",
      "11 relief\n",
      "12 sadness\n",
      "13 worry\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(processed_df['labels'])\n",
    "le_name_mapping = dict(zip(le.transform(le.classes_), le.classes_))\n",
    "print(le_name_mapping)\n",
    "\n",
    "class_name = []\n",
    "\n",
    "#print both key and value from mapping \n",
    "for key, value in le_name_mapping.items():\n",
    "    print(key, value)\n",
    "    class_name.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (â€¦)olve/main/vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 899k/899k [00:01<00:00, 797kB/s]\n",
      "Downloading (â€¦)olve/main/merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 482kB/s]\n",
      "Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [00:00<00:00, 272kB/s]\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding and truncation of data\n",
    "inputs = tokenizer(X_train['text'].tolist(), padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = EmotionDataset(inputs, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(processed_df[\"labels\"].unique())\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (â€¦)\"pytorch_model.bin\";: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501M/501M [00:01<00:00, 431MB/s] \n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=14, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 46702\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8757\n",
      "  Number of trainable parameters = 124656398\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8757' max='8757' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8757/8757 54:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.739200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.227200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.921300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.850200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.774200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.689400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.635300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.658800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.643800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.583000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.464100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.514900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.459600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.469300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.451300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.502800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.364700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.396500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.355300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.331500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.403400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.391500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.331600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.363700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.363500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.334400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.348500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.305700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.285500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.358900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.339800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.296500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.331800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.294500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.326100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.326600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.252800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.338200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.373100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.317700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.345600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.330800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.412400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.358900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.312200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.263700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.287000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.331500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.316100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.329500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.267800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.281700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.241200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.202200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.172300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.195400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.233600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.162600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>1.146900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.176200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.132900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.140400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.114800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.139500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.144400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.118700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.143800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>1.114700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-5000\n",
      "Configuration saved in ./results/checkpoint-5000/config.json\n",
      "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-5000] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-10000] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-2000] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-2500] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-3000] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-3500] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-4000] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-4500] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-5500] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-6000] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-6500] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-7000] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-7500] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-8000] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-8500] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-9000] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-9500] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-10500] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-11000] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-11500] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-12000] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-12500] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-13000] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-13500] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-14000] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-14500] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-15000] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-15500] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-16000] due to args.save_total_limit\n",
      "Deleting older checkpoint [results/checkpoint-16500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8757, training_loss=1.3719615912320366, metrics={'train_runtime': 3250.8703, 'train_samples_per_second': 43.098, 'train_steps_per_second': 2.694, 'total_flos': 3.686740930544026e+16, 'train_loss': 1.3719615912320366, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=5000,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=100,               # how often to log\n",
    "    save_steps = 5000,\n",
    "    save_total_limit = 2       \n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=14, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model_class, tokenizer_class, pretrained_weights = (transformers.RobertaForSequenceClassification, transformers.RobertaTokenizer, 'roberta-base')\n",
    "\n",
    "model = model_class.from_pretrained(output_dir)\n",
    "tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_check_result(test_encoding):\n",
    "    input_ids = torch.tensor(test_encoding[\"input_ids\"]).to(device)\n",
    "    attention_mask = torch.tensor(test_encoding[\"attention_mask\"]).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0))\n",
    "    y = np.argmax(output[0].to(\"cpu\").numpy())\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emotions = []\n",
    "for i in X_test[\"text\"]:\n",
    "    test_encoding1 = tokenizer(i, padding=True, truncation=True)\n",
    "    input_ids = torch.tensor(test_encoding1[\"input_ids\"]).to(device)\n",
    "    attention_mask = torch.tensor(test_encoding1[\"attention_mask\"]).to(device)\n",
    "    test_emotions.append(to_check_result(test_encoding1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    '''\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    '''\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Set size\n",
    "    fig.set_size_inches(12.5, 7.5)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.grid(False)\n",
    "    \n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy 0.586\n",
      " Precision 0.5687 \n",
      " Recall 0.586 \n",
      " F1 0.5719\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 562    0    0    0    2    1    3    3    2    5   16    0   23   18]\n",
      " [   0    0    0    0    0    0    6    0    3    1    9    1    9   16]\n",
      " [   0    0    0    0    0    1   12    0    5    4   84    2   13   45]\n",
      " [   0    0    0    0    0    2   49    1    4   10   54    0   10   29]\n",
      " [  20    0    0    0  454    0    0    2    0    0    3    0    9    2]\n",
      " [   1    0    0    0    0   20  129    0    4   33  102    2   20   44]\n",
      " [   1    0    0    0    0   16  528   10    4  154  192   17   30  105]\n",
      " [   3    0    0    0    4    0    4 1383    0   15    2    0   10    2]\n",
      " [   2    0    0    0    0    0   11    0   95    3   32    3   41   83]\n",
      " [   2    0    0    0    2    3  188   97    5  619   87    4   53   52]\n",
      " [   2    0    0    0    2   18  235    6   28   61  889   12  104  343]\n",
      " [   0    0    0    0    0    1   98    4    6   19   83   18    8   78]\n",
      " [  20    0    0    0   23    6   63   15   58   41  165    8 1531  334]\n",
      " [   6    0    0    0    8   16  177    8   62   72  336   12  245  743]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAIUCAYAAABfBIt+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAD8F0lEQVR4nOzdd3gU1f7H8fcXQm9JqCmUJJQUQiAJHaQoPfQqvdgVsd3ftSIiYgGlCHrVq4KI0iEElCblCgohICBVQicJiJEqkJDN+f2xS8gmATYQ2BC+r+eZh+zMmfnMObPZHM7MzogxBqWUUkoplTvkc/YOKKWUUkqpa7RzppRSSimVi2jnTCmllFIqF9HOmVJKKaVULqKdM6WUUkqpXEQ7Z0oppZRSuYh2zpS6z4nIWhF5xPZzPxFZkcPbryIiRkRccnK7N8kUEflaRE6LSPRtbKepiOzLyX1zFhGpJCIXRCS/s/dFKXVj2jlT6g4TkcMi8qeIFEs37xERWevE3cqSMWamMaa1s/cjBzQBWgHexph6t7oRY8zPxpgaObdbd4btPfbQjcoYY44aY4obYyx3a7+UUrdGO2dK3R35gRG3uxHbiJD+3t5cZeCwMeYfZ+9IbnA3Ry2VUrdPP+SVujvGAS+JiGtWC0WkkYhsFpGztn8bpVu2VkTeEZENwEXA13aa8CkR2S8i50XkbRHxE5FfROSciMwRkYK29d1EZImInLKd5lsiIt7X2Y/BIrLe9vP/2U6DXZ2uiMg027JSIvKliCSISJyIjLl6ukxE8ovIeBH5S0QOAh1u1DAiUlFEFtj2L1FEptjm5xOR10XkiG3k8RsRKWVbdvVU6SAROWrLes22bBjwX6Chbb/fSl+vdLlGRKrafm4vIrttbRknIi/Z5jcXkePp1gmwHY8zIrJLRDqlWzZNRKaKyFLbdjaJiN916nx1/4eIyDHbcXlCROqKyA7b9qekK+8nIqtt7fOXiMy8+l4SkRlAJSDKVt//S7f9YSJyFFidbp6LiLiLyHER6WjbRnERiRWRgTc6Vkqpu0M7Z0rdHTHAWuCljAtExB1YCkwGSgMfAUtFpHS6YgOAx4ASwBHbvDZAGNAA+D/gc6A/UBGoCTxsK5cP+BrraFIl4BKQ9of/eowxH9hOgxUHAoBTwGzb4mlAClAVqAO0Bh6xLXsUiLDNDwd6XC/D1qFbYqtTFcALmGVbPNg2tQB8geJZ7HcToAbwIDBSRAKMMV8CTwC/2vb/zZvVFfgSeNwYUwJr263OYl8LAFHACqAcMByYKSLpT3v2Ad4C3IBY4J2b5NYHqgG9gYnAa8BDQBDQS0SaXY0H3gU8sR6LisAoAGPMAOAo0NFW3w/Sbb+ZrXyb9KHGmL+BocAXIlIOmABsM8Z8c5P9VUrdBdo5U+ruGQkMF5GyGeZ3APYbY2YYY1KMMd8De4GO6cpMM8bssi2/Ypv3gTHmnDFmF7ATWGGMOWiMOQv8iLVzhDEm0Rgz3xhz0RhzHmuHoRkOEpEiwCJgkjHmRxEpD7QHnjPG/GOM+RPrH/c+tlV6ARONMcdsnYB3b7D5elg7HP+ybeuyMebqCFc/4CNbnS4ArwB9xP4U3VvGmEvGmO3AdiDE0XplcAUIFJGSxpjTxpitWZRpgLWD+J4xJtkYsxprx/LhdGUWGmOijTEpwEyg9k1y37bVeQXwD/C9MeZPY0wc8DPXjmGsMWalMSbJGHMKawfekWM4ytaulzIusGXOBX7Cejwfd2B7Sqm7QDtnSt0lxpidWP+Yv5xhkSfXRsOuOoJ1FOmqY1ls8mS6ny9l8bo4gIgUFZHPbKcHzwH/A1zF8W/tfQnsM8a8b3tdGSgAJNhOv50BPsM6mnS1Pun3N2Pd0qsIHLF1ZjLK2C5HABegfLp5J9L9fBFbnW9Bd6wdlCMisk5EGl5nf44ZY1Iz7FP645Td/XH0GJYXkVm2U67ngG+BMjfZNmT9vknvc6wjhdOMMYkObE8pdRdo50ypu+tNrKf90v9Bj8fa4UmvEhCX7rW5jcwXsZ76q2+MKQk8YJsvN1tRRF4GqgPD0s0+BiQBZYwxrrappDEmyLY8AWun66pKN4g4BlSSrC9Yz9gulbCeSj2ZRdmb+QcoevWFiFRIv9AYs9kY0xlrB3MRMOc6+1NR7L+QkfE43Sljsb4Hgm3HsD/2x+9674/rvm9snfPPgW+Ap65ef6eUcj7tnCl1FxljYrFet/Vsutk/ANVFpK/tYu3eQCDWUbacUALrKMwZ2/VtjlyDhYi0s+1n1/SnxYwxCVivu/pQREraLtz3S3d91BzgWRHxFhE3Mo8UpheNtTP3nogUE5HCItLYtux74HkR8RGR4lg7KLOvM8p2M9uBIBGpLSKFsV2vZatnQbHe362U7ZTxOSA1i21swjoa9n8iUkBEmmM99Twri7I5rQRwATgrIl7AvzIsP4n1urzseBVr520o1i+sfJON0VSl1B2knTOl7r7RQNo9z2ynkyKwjnAlYr24P8IY81cO5U0EigB/ARuBZQ6u1xsoC+yRa9/Y/I9t2UCgILAbOA3MAzxsy74AlmPtEG0FFlwvwHbPrY5Yv1hwFDhuywX4CpiB9TTsIeAy1ovws80Y8wfWdl8F7AfWZygyADhsO2X4BNbr3TJuI9m2r+2wtuUnwEBjzN5b2adsegsIBc5i/fJIxjZ9F3jddpo505dOMhKRMOAFrPtvAd7H2lG7UUdaKXWXiDG3c7ZEKaWUUkrlJB05U0oppZTKRbRzppRSSimVi2jnTCmllFIqF9HOmVJKKaVULqIPw71DpGAxI0XcnZJdp2r5mxdSKgfcr18nuukN4u4gbfO7L9WZje7Eim/buuUvY0zGJ5rkWvlLVjYmJdPDMG6LuXRquTGmbY5u1AHaObtDpIg7hRo855TsDUtfdEquuv/cr9/2FnHeX8xUJ/YULE7MLuDivBM9l69YnJadz4nvtVJF8t/o6R65jkm5RKEavXJ0m5e3TXXkSRw5TjtnSimllMoDBCRvXK2VN2qhlFJKKZVH6MiZUkoppe59AjjxNHBO0s6ZUkoppfIGPa2plFJKKaVymo6cKaWUUipvyCOnNXXkTCmllFIqF9GRM6WUUkrlAXorDaWUUkopdQfoyJlSSiml8ga95kzdilbhVdj+3yHs/HooL/Wql2l5xbIlWPZBT36dOoDoTwfSpq4PYH10yWcvtmHzfway6dMBNK3lne3sFcuXUSuoBkH+VRn3wXuZliclJdG/b2+C/KvStFF9jhw+nLZs3PvvEuRflVpBNVi5Yrlma/ZNs0OC/KkZUI3x18ke0LcPNQOq8UDjBpmyawZUIyTI/57Mdmab167pT3BANcaPyzp7YL8+BAdUo1mTa/VOTEykXeuWlHMvwQsjnsl2LsDKFcsIrRVASFB1Phr3fpbZg/v3ISSoOi2aNuTIEWt2zOZoGtcPpXH9UBrVq0NU5MJsZzuzzVetWEbdkEBCa9Zgwvis6z10wMOE1qzBQw805OiRw3bLjx07infZUnw88cNbyg6rFUDtm7R57aDqtEzX5ls2R9OkfihN6ofS+BbbPNcSrKc1c3JyFmOMTndgkpLepnDr8XZT0bYfmgNxp43/wC9MifYfme0HTpraj3xlV+a/S7eb4ZNXmsKtx5vaj3xlDiecMYVbjzcjPl5lpi//3RRuPd5U7DnVbPnjhCnSZnymjMKtx5tLV0ym6cLlFOPj62t27ztgzv6TZIKDa5mt23fZlZk4eap55NHHzaUrxkz/9nvTvWcvc+mKMVu37zLBwbXMmQuXzZ4/DhofX19z4XJKljmafX9lX0xOzTSdv3TF+Pj6ml17Y82ZC5dNcHAts2XbTrsyEyZPMcMefcxcTE4102d8Z7r36GUuJqeaLdt2muDgWub0+Utm974DxsfX15y/dCXLHGdmO7PN/0lKzTSdu3jF+Pj4mp17Ys3p85dNzeBaJmbbTrsyEyZNMcMeecz8k5Rqptnq/U9Sqvnz7/Nm5er/mUkff2Ief+KpLLeflnPJkmk6fSHZVPHxNdt37zd/nb1kagbXMtFbf7cr8+HEKWboI4+Zc5cs5qvpM0237j3NuUsWcyLxvPn7fJI5d8li/jh43JQpWzbtdcbJmW1++mJKpumv80mmio+v+W3XH+bkmYsmKLiW+XXLDrsy4yZ8bAYPe8ycvphi/jt9punavafd8k5dupnOXbub0WPfzzLj9MUUc/aSJdP0t63Nt+3eb07Z2nzT1t/tyoyfOMUMeeQxc/aSxXxpyz57yWISEs+bxPNJ5uwli9lna/OrrzNOQIyz/5Zm6+9usfKmcP3/y9HJWW2gI2e3SKyy1X51a1TgQPwZDp84y5WUVOau3UdEw6p2ZYwxlCxaEIBSxQqR8Pc/APhXKs3abUcBOHX2EmcvXCasegWHszdHR+PnVxUfX18KFixIz959WBIVaVdmSVQk/QYMAqBb9x6sXf0TxhiWREXSs3cfChUqRBUfH/z8qrI5OlqzNTtLMZvts3v06p0pe2nUYvrbsrt278HaNdeye/TqbZcds/neyHZ2m/vepN5LohanZXftdq3exYoVo1HjJhQqXNjhvMzZfvj4WLO79+zN0iWL7cosXRLJw/0GAtClWw/Wrl2NMYaiRYvi4mK9uuZy0uVsP1DemW2+JcZa7yq2enfr0YsfMtT7x6WLebj/AAA6d+3OOlu9AZYujqRSlSr4BwRmq85gHf1K3+bdsmjzH5ZE0jddm6/LoTbP3cR6WjMnJyfJc50zEVkkIltEZJeIPGabd0FE3hGR7SKyUUTK2+b72V7/LiJjRORCuu38S0Q2i8gOEXnLNq+KiOwTkW+AnUDF7OybZ+niHD91Pu113F/n8SpT3K7MO9/+Sp+WAcR++xgL3+7GC1N/AuD3g38S0cCP/PmEyuVLUqdaebzLlnA4Oz4+Dm/va7vr5eVNXFxc5jIVrWVcXFwoWaoUiYmJxMVlXjc+3n5dzdbstO3GxeHlfe20e1brW8tkzs64355eXsTH3SPZzj7eFe3rnZBVdvp6l7Rm364EB9otIT4+U/bftuzN0ZuoFxpMw/AQJk7+JK3j4AhntnlCfDxeXunr7U1CfHyG7Gtl0tf7woULTProA/796kiH8zLWyctu370yHe+E+Hj793m6No+J3kT90GAahYcwIZttru6OPNc5A4YaY8KAcOBZESkNFAM2GmNCgP8Bj9rKTgImGWOCgeNXNyAirYFqQD2gNhAmIg/YFlcDPjHGBBljjqQPFpHHRCRGRGJM8j+3tPO9mvvz7cpdVO3/OV3fWMCX/9ceEZi+fCdxf11gw5T+jHuyBRt3x2OxmFvKUEqp3KJuvfpEb/2dtes38eG497l8+bKzd+mOe/+dt3hy+HMUL1785oXvgPB69dm09XfWrN/ER3mtzfPINWd5sXP2rIhsBzZiHdmqBiQDS2zLtwBVbD83BObafv4u3TZa26bfgK2Av207AEeMMRuzCjbGfG6MCTfGhEvBYpmWxydesBvt8ipTgri/LtiVGdS2JvP/9wcAm/YkULhgfsqULIIl1fB/n62lwVMz6DUqEtfihdkf9/fNW8PG09OL48ePpb2OizuOl5dX5jLHrGVSUlI4d/YspUuXxssr87qenvbrarZmp23Xy4u442n/18lyfWuZzNkZ9zs+Lg5Pr3sk29nH+5h9vT2yyk5f73PW7Nvl4UC7eXh6Zsp2z5Bdwz+A4sWLs3vXToezndnmHp6exMWlr/dxPDw9M2RfK5O+3jGbo3nztZep5e/Hp1Mn89G49/j806nZqnec3b7HZTreHp6e9u/z67R5sWy2ea6npzVzHxFpDjwENLSNkv0GFAaumKsn+sHCzW8hIsC7xpjatqmqMeZL27JbGxIDYvadoKqXK5XLl6SASz56Nq/B0o0H7Moc+/M8zWtXAqBGRXcKF3Th1NlLFCnkQtFC1t1uGVqZFEsqe4863jkLr1uX2Nj9HD50iOTkZObOnkWHiE52ZTpEdGLmjOkALJg/j2YtWiIidIjoxNzZs0hKSuLwoUPExu6nbr3M3zTVbM0GCAu3z543Z3am7PYRHfnWlr1w/jyaNb+WPW/ObLvs8Lr3Rraz2/zATerdIaJjWvbCBdfqfbvCwutyMDaWw4et2fPnzqZ9h452Zdp36MT3M78BYNGCeTRr1gIR4fDhQ6SkpABw9MgR/ti3l8qVqzic7cw2Dw2ry4HYWI7Y6r1g3hzaZah32/Yd+f7bGQBELpzPA7Z6/7hqHTv2HmDH3gM8+fSzvPCvl3nsyacdzw63Zl9t8wXXafPv0rX5A9dp8/3ZbHN1d+S1E82lgNPGmIsi4g80uEn5jUB3YDbQJ9385cDbIjLTGHNBRLyAK7e7c5ZUw/NTVxM1tjv58+Vj+oqd7DmSyBsDG7H1j5Ms3XiAlz9fyyfPtWZ4t1CMgUfHLwOgrGtRot7pTqoxxCdeYNgHP2Qr28XFhQmTptCxQxssFguDBg8lMCiI0aNGEhoWTkTHTgweOoyhgwcQ5F8VNzd3ZsycBUBgUBDde/aiTq1AXFxcmDh5Kvnz59dszb5u9kcTP6ZTh7ZYUi0MHDQkc/aQYQwbPJCaAdVwc3Pnm2+/T8vu1qMnoSFBuOS31uFeynZmm3848WM6R7TFYrEwcPAQAgODePutkYSGhtOhYycGDRnGI0MGEhxQDTd3d6bP+D5t/YDqPpw/d47k5GSioiJZvHQ5AQ5eqO7i4sK4CZPp2rEdFouFAYOGEBAYxJjRbxIaGkb7iE4MHDyUx4YOJCSoOm5u7nw9w3qi4tdf1jNh/AcUKFCAfPny8dGkKZQuU+aeafMPPppE907tsVgs9Bs4mIDAIMaOfpPaoeG0j+jIgMFDeWLYIEJr1sDNzY0vv/nu5ht2MHv8hMl0s7V5f1ubvzP6TerY2nyArc1r29r8K1ubb0zX5pIvHx9ms81zt7zzhAC5NqB07xORQsAirKct9wGuwChgiTGmuK1MDyDCGDNYRKoB3wJFgGVAP2OMl63cCOAR26YvAP2xjrotMcbUvNm+5CtV0RRq8FwO1Sx7Ti990Sm56v6Tlz4/ssOZ33BLTXVem1ucmF3AxXl/dC9fsTgtO58T32uliuTfYowJd9oOZFO+4p6mUO1hObrNyxvGOKUN8tTImTEmCWiXxaLi6crMA+bZXsYBDYwxRkT6ADXSlZuE9QsDGd20Y6aUUkqpu0zIM08IyFOds1sQBkwR63+DzwBDnbs7SimllLpleeS05n3dOTPG/AyEOHs/lFJKKaWuuq87Z0oppZTKK/LOFwLyRi2UUkoppfIIHTlTSimlVN6QT78QoJRSSimVOwh6WlMppZRSSuU8HTlTSimlVN6QR+5zpiNnSimllFK5iI6c3SF1qpZngz5GSeVxcacvOy3b272I07LPXbrtR+3esqIFHX/+Y06LPXnBadnVKhS/eaE75NCf/zgt+/dTZ52Wfe/RW2kopZRSSqk7QEfOlFJKKZU35JFrzrRzppRSSqm8QU9rKqWUUkqpnKYjZ0oppZS694nkmdOaOnKmlFJKKZWL6MiZUkoppfKGPHLNmXbOlFJKKZU36GlNpZRSSimV03TkTCmllFJ5gD4hQCmllFJK3QHaOXOCFcuXUSuoBkH+VRn3wXuZliclJdG/b2+C/KvStFF9jhw+nLZs3PvvEuRflVpBNVi5Yvk9kavZ91/2/1avoHWjEB6sX5PPJo/PtDz61/V0fqgh/p4l+DFqYdr8uGNH6fxQQzq2rE+7B8L4bvoX2c52Zr1Xr1xOo9Ag6ocEMPmjD7LMfnRwX+qHBNC2RWOOHrFmz5v9HS0bh6dNFUoVYueObdnKXrliGXWCAwgJrM6H497PMntQ/z6EBFanRdOGafVevWolTRvWpX5YCE0b1mXdmtXZrTYb1q6ia8swOjWrzdeffJRp+bf/nUL3h+rRq20jHu/bkfjjR9OWRc37js7N69C5eR2i5n2X7ez7td47flnLv7s3519dm7Jk2tRMy1fPn8FrfVrxRt+2jHmkG3EH/0hbdnT/HkYP7cIrvR7ktT6tSE5y3jNyc9zV22nk1OQkua5zJiJVRGTnHc5oLiJL7mTG9VgsFp579mkio37ktx27mTvre/bs3m1XZtpXX+Lm6sauvbEMH/E8r736bwD27N7N3Nmz2Lp9F4uXLGPE8KewWCy5Olez78/sUS8/z3+/W8SPP29lycK57N+3x66Mp1dF3p/0OR279babX7Z8BeYsXUvU6k3M+3Edn3/8ISdPxN8z9X75xRF8Nz+KnzdvZ+G82ezba5/93Tdf4+rqxqbte3j86Wd5+81XAejRuy+rN8SwekMMUz7/mkqVfahZq3a2sl8cMZwFkUvZvG0n8+bMYu8e++xvpn2Fq6sb23f/wdPDRzDy9ZcBKF2mDHPmR7Jpy3Y+++/XPDpskMO5V7PfH/kiH0+bx/yV0SxbPJ+D+/falakRWItvo9YyZ9kvPNSuM5PeHQnA2TN/8/mk9/hm0U/MiFzN55Pe49zZ01rvm0i1WPjmg9d5cdJ03p3zExtXLLbrfAE0bNOFd2at5O3vltF+wBN8P+Ft636npPDZyBEMfnks7875iVf+MwcXlwLZqnuuJVhPa+bk5CS5rnN2O0Qkv7P34WY2R0fj51cVH19fChYsSM/efVgSFWlXZklUJP0GWD8ounXvwdrVP2GMYUlUJD1796FQoUJU8fHBz68qm6Ojc3WuZt9/2Tu2xlDZx49KVXwoWLAgHbr04Kdl9v8X8q5UGf+gYCSf/UdQwYIFKVSoEADJSUmkpqY6nOvsem+N2YyPrx9VfKzZXbr3YtnSKLsyy5ZG0evhAQB07NKd9WvXYIyxK7Nw3my69OiZrXrHbI7G188vrd7de/ZmSdRiuzJLoyLp238gAF269WDtmtUYYwipXQcPT08AAgKDuHzpEklJSQ5n79y2Be/KvnhX8qFAwYK06diNtSuW2pWp2+gBihQpCkBwnbr8aetw/7puNfWbtKCUqzslS7lRv0kLfln7k9b7Jg7u2kb5ilUo510ZlwIFqd+qI1vXrbArU6R4ibSfky5fShsF2rnpf1SsGkCl6oEAFHd1I1/+XP+n876TWztnLiIyU0T2iMg8ESkqIg+KyG8i8ruIfCUihQBE5LCIvC8iW4GeItJaRH4Vka0iMldEitvKtRWRvbZy3a4GiYi7iCwSkR0islFEatnmjxKR6SLys4gcEZFuIvKBLX+ZiNzSfzXi4+Pw9q6Y9trLy5u4uLjMZSpay7i4uFCyVCkSExOJi8u8bny8/bq5LVez77/sEyfi8fD0SntdwdMrW6NfCXHHiWhejwdCq/PYMy9QvoKnw+s6td4JcXh6e6e99vT04kS8fb0TEuLwspVxcXGhRMlS/P13ol2ZyPnz6NrDfkTxZhLi4/Cy23cvEuIz1js+rX4uLi6UKmmtt132wvmE1A5N6yA74tTJeCqkO97lPLz482TCdcsvmjODxs1bAfDnyXgqeF5rs/IeXvx5Mhvvlfu03qdPncC9/LXfC/fyHpw+dTJTuVVzpvNSlybMmTyW/i+9BcCJIwcRgXHD+zOyf3uWfvOpw7m5n+jI2R1WA/jEGBMAnANeAKYBvY0xwVi/ZfpkuvKJxphQYBXwOvCQ7XUM8IKIFAa+ADoCYUCFdOu+BfxmjKkFvAp8k26ZH9AS6AR8C6yx5V8COmTcaRF5TERiRCTm1F+nbrMJlLo/eXh5s2RtNKs2/s7C2TP568/Mf3Tyqi2boylStAgBgTXvevae3bsY+dorTJpy5/5YL104m907fmPgY8/esYzsysv1fqjXIMYvWk+v4a+w+KvJgPV07B/bY3ji7cm89t/5bFm7nF3R6+/qfqmby62ds2PGmA22n78FHgQOGWOunlSfDjyQrvxs278NgEBgg4hsAwYBlQF/2/r7jfUcwrfp1m0CzAAwxqwGSotISduyH40xV4DfgfzAMtv834EqGXfaGPO5MSbcGBNetkzZLCvm6enF8ePH0l7HxR3Hy8src5lj1jIpKSmcO3uW0qVL4+WVeV1PT/t1r8dZuZp9/2VXqOBpN3pxIj4uW6NfV5Wv4Ek1/0A2b/rF4XWcWm8PL+KPH097HR8fRwVP+3p7eHgRZyuTkpLC+XNncXcvnbZ80fw52R41A/Dw9CLObt/j7EYvATw9PdPql5KSwtlz1noDxB0/zsO9uvPZl9Pw9fPLVnbZ8p6cSHe8/0yIo1x5j0zlNq1fw5dTxjPxv7MoaBuhKlfekxPx19rsZEIc5co7/l65X+vtVrYCf6cbafv7ZAJuZctft3z91p3YutZ62tO9vAc16tSjhKs7hQoXIaRRC47su6OXed9d+oWAO8pkeH3mJuX/sf0rwEpjTG3bFGiMGXYb+5EEYIxJBa6YaxeHpHKL94gLr1uX2Nj9HD50iOTkZObOnkWHiE52ZTpEdGLmjOkALJg/j2YtWiIidIjoxNzZs0hKSuLwoUPExu6nbr16uTpXs++/7OA6YRw+GMuxI4dJTk5m6aJ5PNgm00BzlhLij3P50iUAzp45zZboX/H1q+ZwtjPrXScsnIMHYzly2Jq9aP4c2rSPsCvTpn0Ec76fAUDUovk0adYcsf0BSE1NZfHCeXTp3svhzKvCwutyIDY2rd7z586mQ0RHuzLtIzrx3bfWEwOLFsyjWfMWiAhnzpyhR9eOvDVmLA0bNc52dlBIKMcOHyDu2GGuJCezPGoBzVq1tyuzd+d23nn1OSb+dxbu6f7j2rBZSzb+vJpzZ09z7uxpNv68mobNWmq9b8InMISTRw9xKu4oKVeS2bQyijoPtLIrc+LoobSft6//ifKVqgAQ3OABjsfuI+nyJSwpKezduhFPH8d/x3K9PHJaM7fehLaSiDQ0xvwK9MV6evJxEalqjIkFBgDrslhvIzD1ajkRKQZ4AXuBKiLiZ4w5ADycbp2fgX7A2yLSHPjLGHNO7lCP2cXFhQmTptCxQxssFguDBg8lMCiI0aNGEhoWTkTHTgweOoyhgwcQ5F8VNzd3ZsycBUBgUBDde/aiTq1AXFxcmDh5KvkdvJDTWbmafX9mv/nuRwzt0wmLxUKPhwdSzT+Qie+PJjgklAfbRrDjtxieGtKHc2fOsGbFD0weN4Yf/7eFA/v38d6bryAiGGMY9uQIamTjFJ+z6/3uuIn06doBiyWVhwcMwj8giPfHjCIkNIy27TvSd+AQnnlsMPVDAnB1c+Ozr68N4v+64Wc8vbyp4uPrcGb67PETJ9OlYztSLRYGDBpCQGAQY956kzphYXSI6MTAwUN5dOhAQgKr4+buztffWG/f8PmnUzl4IJb3x47h/bFjAIhcsoyy5co5nP3v0eN5emA3Ui0WOvXqj1/1AD796B0Cg+vQrFV7Jr77Bhcv/sP/PWX9IkYFL28m/ncWpVzdeeTZ/6N/pxYAPPrsvynl6q71von8Li4M+L+3GffsAFItFh7o1Btvvxos+M+HVAkIJrRZa1bNmcau6PW4uBSgaMlSPPqm9VYfxUq60qbvI4waGIGIENK4BbWbPOhwtro7JOM3hZxNRKpgPX0Yg/X6sN1YO2MNgfFYO5SbgSeNMUkichgIN8b8ZVu/JfA+cPXKzteNMYtFpC0wEbiItUPmZ4yJEBF34CvA17bsMWPMDhEZBVwwxoy3bfeCMebqlwvslmUlLCzcbNgUkyNtolRudfzvS07L9nYv4rTsc5euOC27aEHnfbNu/4kLTsuuVqG407KdWe/fT511WvagupW2GGPCnbYD2ZTPtbIp1Py1HN3m5cjHndIGuW7kzBhzGOs1Yhn9BNTJonyVDK9XA3WzKLcsq+0aY/4GumQxf1SG18Wvt0wppZRSKqfkus6ZUkoppVS2iT5bUymllFJK3QE6cqaUUkqpvMGJt7/ISdo5U0oppVSecKfutHC36WlNpZRSSqlcREfOlFJKKXXPE3TkTCmllFJK3QE6cqaUUkqpe5/YpjxAO2dKKaWUygMkz5zW1M6ZUvc4Zz6CrUTh+/MjpHAB5z1CyZlP3Ju66ajTsid1CXJatodrYadl/30p2WnZynnuz09WpZRSSuU5eWXkTL8QoJRSSimVi+jImVJKKaXyhLwycqadM6WUUkrlCXmlc6anNZVSSimlchEdOVNKKaXUvS8P3edMR86UUkoppXIRHTlTSiml1D1P9Ca0SimllFK5S17pnOlpTaWUUkqpXERHzpRSSimVJ+jImbplK5Yvo1ZQDYL8qzLug/cyLU9KSqJ/394E+VelaaP6HDl8OG3ZuPffJci/KrWCarByxfJ7IleznZcdEuRPzYBqjL9O9oC+fagZUI0HGjdIy05MTKRtq5aUdSvB8yOeyXYuwOpVy2kcFkSD2gF8/NEHWWY/NrgvDWoH0K5lY44esWbPn/MdDzYJT5s8XAuxc8e2bGU7s81XrVhGWK0AagdV56Nx72eZPbh/H2oHVadl04YcsdV7y+ZomtQPpUn9UBrXq0NU5MJsZ69csYzQWgGE3CQ7JKg6LdJlr/5pJQ80qkuD8BAeaFSXdWtXZzs7qEJxxrStxth21WjnXybT8kZVXJnQyZ+RrfwY2cqPpj5uacuea1qZyV0CGN6kUrZzAVYuX0admv7UCqjGh+OyPt4D+/WhVkA1mjexf5+3a92S8u4leOE23udNwmvSsE4AH08Yl2X240P60bBOAO0fbMIxW5tfuXKFZ58YRotGoTStV4vJWfyO3Ez0zz8xuF0DBrapy/dfTMq0fN60Txka0ZhHOzfjX0O6cTLuGADbNq3n8a7N06Z2Id5sWPVDtvPVNSLSVkT2iUisiLycxfJKIrJGRH4TkR0i0v5m29TOWRZExFVEnroT27ZYLDz37NNERv3Ibzt2M3fW9+zZvduuzLSvvsTN1Y1de2MZPuJ5Xnv13wDs2b2bubNnsXX7LhYvWcaI4U9hsVhyda5mOy/7+RHPsCjqB7Zu38Xc2bMyZ3/9Ja5uruzcs5/hzz7H669aP1MKFy7MyFGjGft+5j82jma/8uIIvpsXxf+it7Nw/mz27bXP/u6br3F1dWPjtj08/tSzjHnzVQC69+rLT+tj+Gl9DFM++5pKlX2oWat2trKd2eYvPjeceZFLif5tJ/PnzmLvHvvsb6Z9haubG9t2/cFTw0fw5mvWNg8IqsnaDdGs37SV+ZE/8NzwJ0lJScl29vzIpWz+bSfzbpC9fdcfPJ0uu3TpMsyeF8nGmO3854uveWzoIIdzAUSgX6gnE38+zBvLY6lXqRQeJQtlKrf52FlGrzzA6JUH+PnQ6bT5y/b9xZebjmcr8yqLxcILI55hweIfiLn6Ps9Q7+lff4mrqys79uzn6Wef443Xrr3P33hzNO+8d+vv81dfGsHMeYtZt2k7i+bNZt/ePXZlvp/xNaVcXfn1tz089tSzjBn1GgBRi+aTnJzEml+2snztRmZ8/d+0jpuj2R+//TJjP5/Fl1EbWLN0IUdi99mVqRoQzCdzV/JF5Dqatu7I5+PfAqB2/SZ8tnAtny1cy7ivF1K4SBHCGje/pTbIjUQkRycH8vIDU4F2QCDwsIgEZij2OjDHGFMH6AN8crPtaucsa67AHemcbY6Oxs+vKj6+vhQsWJCevfuwJCrSrsySqEj6DbB+QHbr3oO1q3/CGMOSqEh69u5DoUKFqOLjg59fVTZHR+fqXM12TnbMZvvsHr16Z8peGrWY/rbsrt17sHaNNbtYsWI0atyEwoULO5yX3m9bNuPj60dlH2t2l269WL40yq7M8h+i6NV3AAARXbqzft0ajDF2ZRbOm02X7j2zle3MNt+yORpfPz98bPXu1rM3S5cstivzw5JI+vYbCECXbj1Yt3Y1xhiKFi2Ki4v1KpPLSZezfWomJkN29yyyly6J5OF02Wtt2SG16+Dh6QlAQGAQly5fIikpyeFsH/ci/Hkhib/+uYIl1RB99Cy1PUs4vP7eP//hckqqw+XTs9bb/n2+NIv3+dXj3bVbzr7Pq/j6UbmKNbtz914s/8H+fb7shyh6PWx7n3fuxs+297mIcPGff0hJSeHy5UsULFiA4iVLOpy9b8dWPCtVwbNiFQoULEjz9l3YsPpHuzK16zehcJGiAASEhPHXyfhM2/nfiijqNn0wrZy6JfWAWGPMQWNMMjAL6JyhjAGuHuBSQOaDkUGe6JyJSH8RiRaRbSLymYjkF5ELIjJORHaJyCoRqScia0XkoIh0sq03WEQibfP3i8ibtk2+B/jZtjdORL4RkS7p8maKSMbGd0h8fBze3hXTXnt5eRMXF5e5TEVrGRcXF0qWKkViYiJxcZnXjY+3Xze35Wq2k7Lj4vDy9r7h+tYymbNvV0J8HJ5e17I9vLxISLD/LEpIuFbGxcWFEiVL8fff9tmRC+bRpUfvbGU7+3h72a3vRUKG7IT4ePs2L1mKv21tHhO9ifqhwTQKD2HC5E/SOmuOSMhQb08vL+KzyPa+TvZVkQvnU7t2KIUKZR75uh63IgU4ffFK2uvTl1JwK1IgU7lQ75KMal2VJxpWzHL5rbAeywzv86yOd7p6lyqZM+/zEwnxeHlda3MPTy9OJMRlKpP+fV6yZEn+/juRiM7dKFqsGCE1KhNesypPDH8eNzd3h7P/+jOBchW80l6XLe9J4smE65ZfNn8mdZs+mGn+2h8W0rJ9N4dzcz25AxOUEZGYdNNjGVK9gGPpXh+3zUtvFNBfRI4DPwDDb1aVe/4LASISAPQGGhtjrojIJ0A/oBiw2hjzLxFZCIwBWmEddpwOXP1vZT2gJnAR2CwiS4GXgZrGmNq2jGbA88AiESkFNAIyjf3bDtpjABUr3dr1E0op2BoTTZGiRQgIrOnsXblrwuvVZ9PW39m3dw9PPDKEVm3a3fKozq3Ys3sXI19/hUVLluX4trfHnyf66FlSUg0P+LoxtJ4XH647nOM594rftmwmX/78bNt7mLNnTtOlXUseaN6SylV8czxr1eK57Nu5nY9m2I8oJv55gkN/7CG8SYscz3SmO/CFgL+MMeG3uY2HgWnGmA9FpCEwQ0RqGmOuO2ScF0bOHgTCsHasttle+wLJwNVPmd+BdcaYK7afq6Rbf6UxJtEYcwlYADTJGGCMWQdUE5GyWBt5vjEm0wUhxpjPjTHhxpjwsmXKZrmznp5eHD9+rZMdF3ccLy+vzGWOWcukpKRw7uxZSpcujZdX5nU9PTN20LPmrFzNdlK2lxdxx69dx5PV+tYymbNvl4enF/Fx17IT4uLw8PC0L+NxrUxKSgrnz53F3f1a9qL5c+jaPXujZuD84x1nt34cHhmyPTw97dv83FncM7R5Df8AihUvzu5dOx3O9shQ7/i4ODyzyD5+ney448fp27s7n/93Gr6+fg7nApy+dAW3otdGwtyKuHD60hW7Mv8kW0hJtZ62/vnQaSq7FclWxvVYj2WG93lWxztdvc+ey5n3eQUPT+LirrV5QnwcFTy8MpVJ/z4/d+4c7u6lWThvFi0ebE2BAgUoU7Ycdes3YvtvWx3OLlPOgz9PXBulO3UyntLlPTKV2/LLOr77bAJvfzKDggXtR0PXLYuk8UPtcSmQM6OY97E4oGK61962eekNA+YAGGN+BQoDmb85k05e6JwJMN0YU9s21TDGjAKumGsXsaQCSQC2nmr6EUP7C10yv77qG6A/MAT46lZ3NrxuXWJj93P40CGSk5OZO3sWHSI62ZXpENGJmTOmA7Bg/jyatWiJiNAhohNzZ88iKSmJw4cOERu7n7r16uXqXM12TnZYuH32vDmzM2W3j+jIt7bshfPn0ax5yxz5X2ft0HAOHojlyGFr9qIFc2jdPsKuTOv2Ecz5bgYASxbNp/EDzdOyU1NTWbxwHl2698p2tjPbPDS8LgdiYzlsq/eCubNp36GjXZn2HTrx3cxvAFi0YB4PNGuBiHD48KG0LwAcPXKE/fv2UrlyFYezw8LrcjBd9vzrZH+fLruZLfvMmTP07NaRt94eS4NGjR3OvOrw35coX7wQZYoVIH8+oV6lUmyPP29XplThax+5tT1LkHDe8WvabiQsvC4HMrzP22fxPr96vBcuyNn3+aEDsRy1tXnk/Dm0aWf/Pm/TLoI539ve55ELaGJ7n3t5V2LD/9YCcPGff9gSs4mq1Wo4nF0juA5xRw6RcPwIV5KTWfvDIhq1aGtXZv/uHUwc9RKjp87ArXTmwYLVSxfSskMeOqXJtScE3M0vBACbsQ7e+IhIQawX/C/OUOYo1oGjq2f7CgOnbrTRe/60JvATECkiE4wxf4qIO+D41ajQyrbOJaALMBQ4n8U2pgHRwAljzG5ukYuLCxMmTaFjhzZYLBYGDR5KYFAQo0eNJDQsnIiOnRg8dBhDBw8gyL8qbm7uzJg5C4DAoCC69+xFnVqBuLi4MHHyVPLnz5+rczXbedkfTfyYTh3aYkm1MHDQkMzZQ4YxbPBAagZUw83NnW++/T5tff9qPpw/d47k5GSiFkcStXQ5AYEZv4B0/eyx4yfycLcOWCypPNx/EP4BQbz/zihq1wmjTfuO9B0whGceG0yD2gG4urnx2Vffpq3/64af8fTyprJP9k/xOLvNx0+YTLeO7bBYLPQfNISAwCDeGf0mdULDaB/RiQGDh/LY0IHUDqqOm5s7X834DoCNv6xnwvgPKFCgAJIvHx9OmkLpMjf8j3Wm7HETJtPVlj3Alj1m9JuE2rIH2rJDbNlf27I//89UDh6I5f13x/D+u2MAWBS1jLLlyjmUnWrgu63xPPdAFfKJsOHQaeLPJdE5qByHT19ie/x5HqxWmhDPEqQawz/JFr6Ovjba9X8tfPAoUYhCLvn4IKIG0zfHsevkBYfr/eHEj+kS0dZa78FDCAwM4u23RhIaGk6Hjp0YNGQYjwwZSK2Aari5uzNtxrX3eWD1a+/zJVGRRC5dTkBANt7n4ybycPcILBYLffoPpkZAIB+88xYhdUJp074jDw8YwvDHh9CwTgCubu785ytrR23II0/w3NOP0qxBbYwx9Ok3kMCawQ7lAuR3cWH46+/y8iO9SE1NpW23h6lSzZ9pk9+jes3aNGrZls/HvcWli//w9vPDACjn4c3bn1h/z07EHeXUiThq1W3kcKbKmjEmRUSeAZYD+YGvjDG7RGQ0EGOMWQy8CHwhIs9jHQAanG7wKEtyk+X3BBHpDbyCdSTwCvA0sMoYU9y2fBRwwRgz3vb6gjGmuIgMxtohK4V1KPJbY8xbtjLfAbWAH40x/7LNWwYsMsb852b7FBYWbjZsisnJaiqVJWf+Dp+75PjtHnJaqaLOOx2TfIvfLswJzrzF5vOLb/n/pbdtUpcgp2Wfz3Ca9m7alXDOadkPBZTdkgPXW901Bcr4GbdO7+boNk993dspbZAXRs4wxswGZmeYXTzd8lEZyhdP9/K4MaZLFtvsm/61iBQFqgHfZyyrlFJKqVwgbzwgIE9cc3bHichDwB7gY2PMWWfvj1JKKaXyrjwxcnarjDHTsF5LdrNyq4DKd3p/lFJKKXWLRJ+tqZRSSiml7oD7euRMKaWUUnlHXhk5086ZUkoppfKEvNI509OaSimllFK5iI6cKaWUUuqed/UJAXmBjpwppZRSSuUiOnKmlFJKqbwhbwycaedMqXtdqhOfwLbxcKLTslsHlHdatjM//y9fsTgt+8Gqrk7Lzp/Pea3uzOyyxQo5Lfueo/c5U0oppZRSd4KOnCmllFIqT9CRM6WUUkopleN05EwppZRSeYKOnCmllFJKqRynI2dKKaWUyhvyxsCZds6UUkoplTfoaU2llFJKKZXjdORMKaWUUvc8EX22plJKKaWUugO0c+YEK5Yvo1ZQDYL8qzLug/cyLU9KSqJ/394E+VelaaP6HDl8OG3ZuPffJci/KrWCarByxfJ7IleznZO9cvky6tT0p1ZANT4cl3X2wH59qBVQjeZNGqRlJyYm0q51S8q7l+CFEc9kOxdg64bVPNWpCU9ENGT+lx9nWr5sznSe7d6C53o9xCuDOnHswD4Azp35m9eHdadPAz8+H/vqLWWvWL6MkCB/agZUY/x12nxA3z7UDKjGA43t6922VUvKupXg+Vus98oVywitFUBIUHU+Gvd+ltmD+/chJKg6LZo25MgRa3bM5mga1w+lcf1QGtWrQ1Tkwmxnr165nIahQdQLCWDyRx9kmf3o4L7UCwmgbYvGHLVlA+zauYN2Dzalab0QmjWow+XLl7OVvf2XNbzYrRnPd27C4q+nZlq+at4M/t3rIV55uA2jhnbj+ME/7Jb/lRDHkCY1WPLNf7KVC879HXNmm29Yu5JOzUOJaBrCl1M/yrR8y6YN9G7flFAfN1YuXWS3bMI7b9D1wXp0aRnOeyP/hTFOfAZcDrs6epZTk7Pkms6ZiHQRkcB0r9eKSHgObHe0iDx0u9vJKRaLheeefZrIqB/5bcdu5s76nj27d9uVmfbVl7i5urFrbyzDRzzPa6/+G4A9u3czd/Ystm7fxeIlyxgx/CksFsees+esXM12XvYLI55hweIfiNm+i7mzZ7Fnj3329K+/xNXVlR179vP0s8/xxmsvA1C4cGHeeHM077w3zuG8jNmfjX2VkZ/M5OOF6/h52aK0ztdVD7TvxuT5a5g4ZxVdhzzNV+NHAVCwYGH6Pv1/DH5h5C1nPz/iGRZF/cDWq/XO2OZff4mrmys79+xn+LPP8fqr1+o9ctRoxr5/6/V+8bnhzI9cyubfdjJv7iz2Zmjzb6Z9haubG9t3/cHTw0fwpq3NA4Nqsm5DNBs2bWVB5A+MGP4kKSkp2cr+94sj+H5+FOs3b2fBvNns22ufPfObrynl6kb09j08/vSzvP2mtfObkpLCU48OZtzEKfwcvZ2FS1dRoEABh7NTLRa+fu91/m/yN4ybt5pflkdm6nw1atuF9+es4t3vl9Nx0BN8+9Fou+XfThhNSKMWDmemr7czf8ec1eYWi4Wxr7/IJ9Pns/CnzSxbPI8Df+y1K1PB05u3P/yUdp172s3fFrOJbTEbmbfiV+av3MSuHVuJ2bje4ezcTjtnOa8LEHizQtlljBlpjFmV09u9VZujo/Hzq4qPry8FCxakZ+8+LImKtCuzJCqSfgMGAdCtew/Wrv4JYwxLoiLp2bsPhQoVooqPD35+VdkcHZ2rczXbOdkxm6PxTZfdo1dvlmbIXhq1OC27a7cerF1jzS5WrBiNGjehcOHCDuelt3/nb3hUrEIF78oUKFCQJm07s2mt/ahE0eIl0n6+fOli2odg4aJFCQytT4FCt5Yds9m+zXv06p2pzZdGLab/1Xp3z7l6W9vcDx8fa3b3nr1ZumSxffaSSB7uNxCALt16sHbtaowxFC1aFBcX6yXAl5MuZ/uPwtaYzfj4+lHFlt21ey+WLY2yK7NsaRS9Hx4AQMcu3fl57RqMMaz9aSWBQcHUDA4BwL10afLnz+9wduyubZSvWIXy3pVxKVCQhq07sWXtCrsy6Y93UrrjDbB5zTLKelbE2696tuoMzv0dc2ab79wWQ8UqvnhX9qFAwYK07didtSuW2pXxqliZ6gE1yZfP/s+8iHVE78qVZJKTk0i5kkLpMuUczlZ3xx3tnIlIfxGJFpFtIvKZiOQXkQsi8o6IbBeRjSJSXkQaAZ2AcbayfrZN9LSt/4eINLVtc7CITEmXsUREmtu2PU1EdorI7yLyvG35NBHpYft5pIhstpX5XGyfECLyrIjsFpEdIjLLNm+UiEwXkZ9F5IiIdBORD2zbXiYijv83J534+Di8vSumvfby8iYuLi5zmYrWMi4uLpQsVYrExETi4jKvGx9vv25uy9VsJ2ZX9LZfP6ts72vZpUpas2/X33+eoEwFr7TXpct58PfJE5nK/TDrax7v0IDpE8bwyL/H3HYuQHxcHF7eGeqdod2sZTK3+e1KyHC8Pb28MrV5Qny8XZuXLFmKv23Zm6M3US80mIbhIUyc/ElaZ80RJxLs6+3h6UVCfPx1y7i4uFCiZCn+/juRA7H7ERF6denAg03r8fHE8dmq9+k/T1C6vGfaa/fyHvx9KvPxXjFnGs91asx3k8cy8F/WkbPLF/8havqndH/s+WxlXuXM3zFntvmfJxKo4Hktu5yHJydPxt9gjWtCwupTt1FTHgqvzkPh1WnU7EF8q9XIVn6uJjk8Ockd65yJSADQG2hsjKkNWIB+QDFgozEmBPgf8Kgx5hdgMfAvY0xtY8wB22ZcjDH1gOeAN28SWRvwMsbUNMYEA19nUWaKMaauMaYmUASIsM1/GahjjKkFPJGuvB/QEmvH8VtgjW3bl4AOjrWEUior7fsM4bOlGxn43GvM/WKis3fH6erWq0/01t9Zu34TH457P9vXIN2qFEsK0Rt/4dMvpxO1fC0/REXyv7Wrczynda/BTFy8gYeHv8Ki/04GYP5nH9G+7yMULlosx/Nys7vV5lk5evgAh2L3sWLTHlZG7yX6l3Vs3fTLXclWjruTI2cPAmHAZhHZZnvtCyQDS2xltgBVbrCNBQ6WAzgI+IrIxyLSFjiXRZkWIrJJRH7H2ukKss3fAcwUkf5A+gs9fjTGXAF+B/IDy2zzf89qf0TkMRGJEZGYU3+dynInPT29OH78WNrruLjjeHl5ZS5zzFomJSWFc2fPUrp0aby8Mq/r6Wm/7vU4K1eznZh97Lj9+lllH7+WffacNft2uZerwF8nro1AJP6ZgHv5Ctct37RtFzatWXbd5dnh6eVF3PEM9c7QbtYymdv8dnlkON7xcXGZ2tzD09Ouzc+dO4t7huwa/gEUL16c3bt2OpxdwcO+3gnxcXh4el63TEpKCufPncXdvTSenl40aNSE0qXLULRoUR5q3ZYd239zONutXAUS043a/H0yAfey1z/eDdt0JsZ2mjt25298N3ksz0Y0ZNl3XxL59RSWz57mcLYzf8ec2eblKnhwIv5a9p8J8ZQv73mDNa5ZvWwJwXXqUrRYcYoWK07j5q3YvtXx07m5nV5zdnMCTLeNhNU2xtQwxowCrphrXw2xcON7rSVlUS4F+/0uDGCMOQ2EAGuxjn79125nRAoDnwA9bKNfX1xdF+so2FQgFGtn8mpWkm3bqRn2OzWr/TbGfG6MCTfGhJctUzbLCoXXrUts7H4OHzpEcnIyc2fPokNEJ7syHSI6MXPGdAAWzJ9HsxYtERE6RHRi7uxZJCUlcfjQIWJj91O3Xr0sc3JLrmY7JzssvC4H0mXPmzOb9hmy20d0TMteuGAezZq3zJEPo2pBtUk4eoiTx49y5Uoy65dFUq9ZG7sy8UcOpv0c879VeFTyue1csNY7NkO9M7Z5+4iOfHu13vNzrt5h4XU5GBvL4cPW7PlzZ9O+Q0f77A6d+H7mNwAsWjCPZs1aICIcPnwo7QsAR48c4Y99e6lcuYrD2XXCwjl4MJYjtuyF8+fQpn2EXZk27SOY/f0MAKIWzadJs+aICC0ebM2e3Tu5ePEiKSkp/LLhZ2rUCHA42y8whBPHDvNn3FFSriTz64rFhDVrZVcm4eihtJ9/W/8TFSpZ6/bmlwuYvORXJi/5lbZ9h9F5yDO06T3Y4Wxn/o45s82DQsI4euggx48e5kpyMsui5tOsVXuH1q3g6c2WjRtISUnhypUrbNm4AZ+qeeS0puSdztmdvAntT0CkiEwwxvwpIu5AiRuUP3+T5VcdBp4SkXyAF1APQETKAMnGmPkisg/racj0rnbE/hKR4kAPYJ5tOxWNMWtEZD3QByjuWBWzz8XFhQmTptCxQxssFguDBg8lMCiI0aNGEhoWTkTHTgweOoyhgwcQ5F8VNzd3ZsycBUBgUBDde/aiTq1AXFxcmDh5qsMXkTorV7Odl/3hxI/pEtEWi8XCgMFDCAwM4u23RhIaGk6Hjp0YNGQYjwwZSK2Aari5uzNtxvdp6wdW9+H8uXMkJyezJCqSyKXLCQhw7Ps6+V1cePSVsbz15MNYUi081KUPlarW4LupH1A1KIR6zdvww6yv2L7xZ/IXKEDxEqUY8fbktPUfbVeXSxcukHIlmU1rljHqP99T0c+xPx4uLi58NPFjOnVoiyXVwsBBQzK3+ZBhDBs8kJoB1XBzc+ebb6/V27/atXpHLY4kaulyAgIdq7eLiwvjJkyma8d21jYfNISAwCDGjH6T0NAw2kd0YuDgoTw2dCAhQdVxc3Pn6xnfAfDrL+uZMP4DChQoQL58+fho0hRKlynjUO7V7PfGTaR31w5YLKn0HTAI/4Ag3hszitqhYbRt35F+A4fw9GODqRcSgJubG599bf2IdHVz44mnR9CmeUNEhAdbt6VVW8f+0IP1eA/+v7d575n+pFosNO/cG2+/Gsz9dDy+gbUIa9aaFbOnsTN6PS4uLhQrUYon35rg8PZvVm9n/o45q81dXFx45e1xPDmgK6kWC116D6BqjQCmfjiGoOBQmrduz87tW3j+0X6cO3uGdat+5JOPxrLwp2hadehC9C//o0frBghCo+YP0bxVu+w1vLrj5E7e30REegOvYB3pugI8DawyxhS3Le8BRBhjBotIY6yjWUlYO05fAi8ZY2JsHa8YY0wV20X832I9ZboHcANGAaexXmd2dVTtFWPMjyIyDVhijJknImOAh4ETwB/AEeAdYA1QCuto37fGmPdEZBRwwRgz3ravF9Ltt92yrISFhZsNm2Juq/2UcoQl1Xn3KFq196TTslsHlHdadorFeW1++Yrjt3vIaav2O+94d63lffNCd8j5S1eclp1w5u5ce5iVkEoltxhjbvuWVndL4QrVjHf/yTcvmA0HPmzvlDa4o49vMsbMBmZnmF083fJ5wDzbzxuwv5VG83Tl/sJ2jZft1GK/60SGZrEPg9P9/DrwehbrNclivVEZXhe/3jKllFJKqZyiz9ZUSimlVB6Qd56tqZ0zpZRSSuUJeaRvlqueEKCUUkopdd/TkTOllFJK5Ql55bSmjpwppZRSSuUiOnKmlFJKqXuf6DVnSimllFLqDtCRM6WUUkrd8wTIly9vDJ1p50wppZRSeYKe1lRKKaWUUjlOR87ukFQDl5Od8wy8wgUdf3ivyhluzbN6KtjdcXrtGKdltwms4LTsO/lc4Jsp4OK8/9c6M7tLsJfTsp2piBM/U6tVKH7zQiqN3kpDKaWUUkrlOB05U0oppdS9Lw/dSkM7Z0oppZS65wl6WlMppZRSSt0BOnKmlFJKqTxAdORMKaWUUkrlPB05U0oppVSekEcGzrRzppRSSqm8QU9rKqWUUkqpHKcjZ0oppZS69+Wh+5zpyJkTrFqxjLq1AwkNrsGE8e9nWp6UlMTQgQ8TGlyDh5o15OiRw3bLjx07ine5Unw88cNs5a5YvoxaQTUI8q/KuA/eyzK3f9/eBPlXpWmj+hw5fC133PvvEuRflVpBNVi5Ynm2cu/n7Fb1q7H9uxHsnPU8L/V/INPySuVd+WHiEKKnPcPyj4fhVbZk2vxfvnyKjV8/zZYZw3mkc91sZ9+vbb5i+TJCgvypGVCN8dfJHtC3DzUDqvFA4waZsmsGVCMkyP+erPe91uaJiYm0bdWSsm4leH7EM9nOvZrtrHqvXLGMOsEBhARW58NxWX+WD+rfh5DA6rRo2jAte/WqlTRtWJf6YSE0bViXdWtWZzvbme9zdRcYY+67CXgW2APMvFMZteuEmdP/pGSa/jqXZKr4+Jrfdv5hTp6+aIJq1jK/xuywKzNuwsdm8LDHzOl/Usx/p800Xbv3tFveqUs307lrdzP6nfezzLh0xWSaLlxOMT6+vmb3vgPm7D9JJji4ltm6fZddmYmTp5pHHn3cXLpizPRvvzfde/Yyl64Ys3X7LhMcXMucuXDZ7PnjoPHx9TUXLmedc79mF278WqapaNPXzYHjica/53hTotlIs31/vKndb6JdmfmrfzfD3p5nCjd+zbQZ/qWZ+eNvpnDj10yJZiNNyeYjTeHGr5nSD71lDsf/bXw6vZdlzv3a5heTUzNN5y9dMT6+vmbX3lhz5sJlExxcy2zZttOuzITJU8ywRx8zF5NTzfQZ35nuPXqZi8mpZsu2nSY4uJY5ff6S2b3vgPHx9TXnL13JMkfbPGfa/NTp82bVmv+ZSVM+MY8/+VSW288NbX7+siXTdOafZOPj42t27N5vEs9dMjWDa5nNv/1uV+ajSVPM0EceM+cvW8zX38w03Xr0NOcvW8z6jTHmj4PHzPnLFrNpy3bj4emZZcb5yxanvs+BGGf/vc7OVNSzugkdvTpHJ2e1wf06cvYU0MoY0+9WNyAit3RKeEtMNL6+flTx8aVgwYJ069GLH5Ystivz45LFPNxvAACdu3Zn3drVVzuVLI2KpFLlKvgHBGYrd3N0NH5+VfHxteb27N2HJVGRdmWWREXSb8AgALp178Ha1T9hjGFJVCQ9e/ehUKFCVPHxwc+vKpujozX7JuoGeHPgeCKH409zJcXC3FW/E9EkwK6Mf5WyrNt6EIB1Ww8S0dQfgCspFpKvWAAoVCA/+fJlb6z+fm3zmM322T169c6UvTRqMf1t2V2792DtmmvZPXr1tsuO2Xxv1PtebfNixYrRqHETChcu7HBebqq3r59fWnb3nr1ZEmX/Wb40KpK+/QcC0KVbD9ausX6Wh9Sug4enJwABgUFcvnSJpKSkbGU7632e24nk7OQs913nTET+A/gCP4rIayLylYhEi8hvItLZVqaKiPwsIlttUyPb/Oa2+YuB3beSnxAfj5d3xbTXnl7eJCTE25WJT1fGxcWFkiVL8XdiIhcuXGDSRx/w71dHZjs3Pj4O73S5Xl7exMXFZS5TMV1uqVIkJiYSF5d53fh4+3U1OzPPsiU5/ufZtNdxp86lnba86vfYE3RuZu1od34gkJLFCuNesggA3uVKET3tGfYv+BcfzvyZhMTzDmffr20eHxeHl7f3Dde3lsmcnXG/Pb28iI+7R+p9j7b57XJmvRPi4+w+y728vEjIWO/4+LQMFxcXSpXMXO/IhfMJqR1KoUKFHM525vtc3R33XefMGPMEEA+0AIoBq40x9Wyvx4lIMeBPrCNroUBvYHK6TYQCI4wx1e/unsP777zFk888R/Hixe92tLpDXpmyjKa1q/DrV0/RtE4V4v48iyXVOkp6/M+z1Bs8hZq9J9C/bR3KuRVz8t4qpXLSnt27GPnaK0ya8qmzdyXPEJEcnZzlvuucZdAaeFlEtgFrgcJAJaAA8IWI/A7MBdKfQ4w2xhzKamMi8piIxIhIzF9/ncoy0MPTk7jjx9Jex8cdx8PD066MZ7oyKSkpnDt3FvfSpYmJiebN11+mVoAfn06dzEfj3+Pz/0x1qKKenl4cT5cbF3ccLy+vzGWOpcs9e5bSpUvj5ZV5XU9P+3U1O7P4U+fwLlcq7bVX2ZLEnTpnVyYh8Tx9XvuehkM/4c3PVwFw9sLlTGV2HTpJ45AqDmffr23u6eVF3PHjN1zfWiZzdsb9jo+Lw9PrHqn3Pdrmt8uZ9fbw9LL7LI+Li8MjY709PdMyUlJSOHvuWr3jjh/n4V7d+ezLafj6+TmcC859n6u7437vnAnQ3RhT2zZVMsbsAZ4HTgIhQDhQMN06/1xvY8aYz40x4caY8DJlymZZJjSsLgcOxHLk8CGSk5NZMG8O7Tp0tCvTtkNHvp85A7AOeT/QrAUiwo8r17FjzwF27DnAk08/ywsvvcxjTzztUEXD69YlNnY/hw9Zc+fOnkWHiE52ZTpEdGLmjOkALJg/j2YtWiIidIjoxNzZs0hKSuLwoUPExu6nbr16DuXez9kxe+OoWrE0lT3cKOCSn54PBbN0w167MqVLFU3739m/BjzA9KVbAWtHrnBB62WNriUK06hWZf44+tc9UW9nZoeF22fPmzM7U3b7iI58a8teOH8ezZpfy543Z7Zddnjde6Pe92qb3y5n1/tAbGxa9vy5s+kQYf9Z3j6iE999+w0AixbMo1lz62f5mTNn6NG1I2+NGUvDRo2zXW9nvs9zu7xyzdn9fp+z5cBwERlujDEiUscY8xtQCjhujEkVkUFA/pwKdHFx4YMPJ9G9c3ssFgv9Bg4mIDCIsW+/Se3QcNp36MiAQUN54pFBhAbXwM3NjS+nf5cjuRMmTaFjhzZYLBYGDR5KYFAQo0eNJDQsnIiOnRg8dBhDBw8gyL8qbm7uzJg5C4DAoCC69+xFnVqBuLi4MHHyVPLnd7xJ7tdsiyWV5z9aQtRHg8ifLx/Tl25hz6E/eWPYg2zdG8fSDXt5oI4Pox9vhQHWbzvMcx9FAVCjclnee6YdBoMgTPx+PbsOnrwn6u3s7I8mfkynDm2xpFoYOGhI5uwhwxg2eCA1A6rh5ubON99+n5bdrUdPQkOCcMlvrcO9VO97sc0B/Kv5cP7cOZKTk4laHEnU0uUEBDr2hSdn13v8xMl06diOVIuFAYOGEBAYxJi33qROWBgdIjoxcPBQHh06kJDA6ri5u/P1N9bP8s8/ncrBA7G8P3YM748dA0DkkmWULVfujrf57b7P1d0hV78FeD8RkcNYR8T+ASYCjbCOIh4yxkSISDVgPmCAZcDTxpjiItIceMkYE3GzjDqh4WbN+k13ZP9vpnBB/UW729yav+607NNrxzgt25mc+dmVVx4Rk133a5unWFKdlp0/m9/UzklFC+bbYowJd9oOZFMx7xqm5tOf5+g2o19t7pQ2uC9HzowxVdK9fDyL5fuBWulm/ds2fy3Wa9OUUkoplYsI+oQApZRSSil1B9yXI2dKKaWUymuce/uLnKQjZ0oppZRSuYiOnCmllFIqT8gjA2faOVNKKaVU3qCnNZVSSimlVI7TkTOllFJK3fucfFf/nKQjZ0oppZRSuYiOnCmllFLqnme9CW3eGDrTzplSSiml8gTtnKkbyif6jMv7iTOfb/nP5RSnZS/eE++07IfrVHJadmqq854xefqfZKdl/x5/zmnZzaqXcVp20hXnPVvz7KUrTstWzqOdM6WUUkrlCXlk4Ey/EKCUUkoplZvoyJlSSiml8gS95kwppZRSKrfQ+5wppZRSSqk7QUfOlFJKKXXPEyTPnNbUkTOllFJKqVxEO2dKKaWUyhNEcnZyLFPaisg+EYkVkZevU6aXiOwWkV0i8t3NtqmnNZVSSimlboGI5AemAq2A48BmEVlsjNmdrkw14BWgsTHmtIiUu9l2tXOmlFJKqTwh392/5qweEGuMOQggIrOAzsDudGUeBaYaY04DGGP+vNlG9bSmE6xYvoxaQTUI8q/KuA/ey7Q8KSmJ/n17E+RflaaN6nPk8OG0ZePef5cg/6rUCqrByhXL74lczXZO9k8rl1O/ThB1Q/yZ9OEHWWYPG9SXuiH+tG7RiKNHrmXv2rmDti2b0LhuCE3r1+by5cvZyv7917W80qMFL3d7gKXTP8m0fM38b3nj4da82a8dYx/tTtzBP9KWHdu/h3eGduH13g/xxsOtuZKUvWxnH+/aNf0JDqjG+HFZZw/s14fggGo0a9IgLTsxMZF2rVtSzr0EL4x4Jtu5AGtWreCBesE0DgtkysRxWWY/ObQ/jcMCiXioKceOWrOTk5N54elHebBxGK2a1uWX9euynR2zfjWPRDRkaLt6zPnv5EzLF0z/lMc6NeHJrs14eVh3TsYfS1v25UejeaLLAzzR5QHW/bgo29krli8jJMifmgHVGH+d4z2gbx9qBlTjgcb2bd62VUvKupXg+Vtsc2f+jq1bvYKHGobQol5N/jN5fKbl0b+up9ODDanuUYIfoxamzd/9+3Z6tGtO26ZhtG9WjyWL5mUrN7e7A6c1y4hITLrpsQyRXsCxdK+P2+alVx2oLiIbRGSjiLS9WT3u+86ZiDwrIntEZObdyLNYLDz37NNERv3Ibzt2M3fW9+zZvduuzLSvvsTN1Y1de2MZPuJ5Xnv13wDs2b2bubNnsXX7LhYvWcaI4U9hsVhyda5mOy/73y8+y+wFUWzYvIMF82axb6999sxvvsLV1ZXN2/fyxNMjeGvkqwCkpKTw5CODGD9pKhs2byfyh58oUKCAw9mpFgvffvAGz0+azpjZq9i0fLFd5wugQZvOvP39Ct6a+SPtBjzB7InWZ5NaUlL44s3nGPDyWMbMXsW/P51NfhfHs53d5i+MeIaFi39gy/ZdzJ09iz177LOnf/0lrq6u/L5nP888+xxvvGa9PKVw4cK88eZoxr6XuVPlaPbr/zeCGXMiWfPrNiLnz+GPvXvsysz6dhqlXF3ZsGU3jz45nLGjXgfgu2++AuCnDVv4fsFS3n7jZVJTHX+WpMViYeqYf/P2p9/z2eL1rP1hAUcO7LMr4xcQzOTZK/h04TqatIrgqw9HAxC9biUHdu9g6rzVTPzuR+ZP+4R/LpzPVvbzI55hUdQPbL3a5hmP99df4urmys49+xn+7HO8/uq1Nh85ajRj37/1NnfW75jFYmHUv5/nq+8XsXz9VqIWzGX/Pvvj7elVkQ8mf07Hbr3t5hcpWpRxU//Lsp+38PXsRYx5/V+cO3vmltrgPvGXMSY83fT5LWzDBagGNAceBr4QEdcbrXDfd86Ap4BWxph+dyNsc3Q0fn5V8fH1pWDBgvTs3YclUZF2ZZZERdJvwCAAunXvwdrVP2GMYUlUJD1796FQoUJU8fHBz68qm6Ojc3WuZjsne2tMND6+flTxsWZ37d6bH5dE2ZX5cWkUffoOAKBTl+78vHY1xhjW/LSSwJrB1AwOAcC9dGny58/vcPbBXdso512Fcl6VcClQkPqtO7LtfyvtyhQpXiLt56RLF8F2JmLXpv/hXdWfStUDASju6ka+bGQ7s81jNkfjmy67R6/eWWQvTsvu2q0Ha9dYs4sVK0ajxk0oVLiww3npbduymSo+flSuYs3u3K0nK360P94rfoiiZ5/+AHTo3I31/1uDMYb9+/bQ6IHmAJQpW46SpUqx/bctDmf/8ftWPCv54FGxCgUKFKRZu65sXL3MrkxIvSYULlIUAP+QcP46GQ/A0QP7qBnekPwuLhQuWgyf6oFsWb/a4eyYzfbHO6s2Xxq1mP5X27x75jYvfItt7szfse1bY6js40elKj4ULFiQiK49WLVsiV0Z70qV8Q8KJl8++z/zPn7V8PGtCkD5Cp6ULlOOxMS/sl3/3Mg62iU5OjkgDqiY7rW3bV56x4HFxpgrxphDwB9YO2vXdV93zkTkP4Av8KOInBWRl9It2ykiVWzTHhH5wvYtixUiUuRWM+Pj4/D2vnYcvby8iYuLy1ymorWMi4sLJUuVIjExkbi4zOvGx2d8D+SuXM12TnZCQjyeXt5prz29vEhIsF8/IT4eL2/77L8TEzkQ+wciQs8u7WnRpC6TJ2Q+ZXIjZ06dwL28R9prt3IenD51IlO5n+ZO599dmzL343fp9+JbAJw4eggR4cPhAxg1oD0/fvOfbGU7/XhXvNbmXl7eJGSVnb7NS1qzb1dCQjwe6Y53BU8vEhLi7cqcSFfGml2S038nEhAUzMofl5KSksLRI4f4fdtvxMcddzj7rz9PULbCtbM4Zcp7kPhnwnXLr1gwk/CmDwLgUyOILetXc/nSRc6eTmTH5vWcOpGNNo+Lw8vbvs0zHjNrmczH+3Y583fs5Il4PLyutXkFDy9OZjjejti+dTNXriRTuYpvttdVaTYD1UTER0QKAn2AxRnKLMI6aoaIlMF6mvPgjTZ6X38hwBjzhO3cbwvgRhcdVAMeNsY8KiJzgO7At3djH5W621JSLGz69RdWrv2VIkWL0i2iNbXrhPJA85Y5mvNgz0E82HMQG5ctIuqrj3lk1EekWlLYv20zb0yPomDhIox/6mEq+9cksF6THM1W1/TpP5jYP/bRvmUjvCtWIqxeg2yN4mTH6qi5/LFrOx9MWwRAWOMW/LFzGy/270Apt9L4h4Rna6T0XnW3fsdu5M+TCbz49COM+/iLTKNr97J8d/n7AMaYFBF5BlgO5Ae+MsbsEpHRQIwxZrFtWWsR2Q1YgH8ZY274P4S8c0TurEPGmG22n7cAVbIqJCKPXb1o8NRfp7LckKenF8ePX7t2MC7uOF5eXpnLHLOWSUlJ4dzZs5QuXRovr8zrenpmvO4wa87K1WznZHt4eNqNfsTHxeHhYb++h6cnccfts91Ll8bTy4uGjZpQukwZihYtykNt2rF9228OZ7uWrcDfJ6+NnJz+MwG3shWuW75e6078tm4FYB1lq16nPiVc3SlUuAjBjVtwZN9Oh7OdfryPXWvzuLjjdqMbGfcvJSWFc+es2bfLw8OThHTH+0R8HB4ennZlKqQrY80+h5t7aVxcXBg1dhwr/hfNVzPnce7sWXz9bnjGxU6ZchXsRrv+OplA6XIemcr99us6Zn0+kVEff0PBgoXS5j/8+PNMnb+Gsf+dBwa8Kvs5nO3p5UXccfs2z3jMrGUyH+/b5czfsfIVPO1GZU8kxFE+w/G+kfPnz/FI3268+Ooo6oTXc3i9e4ETTmtijPnBGFPdGONnjHnHNm+krWOGsXrBGBNojAk2xsy62Ta1c3ZNCvbtkf5ChKR0P1u4zoijMebzqxcNli1TNsuQ8Lp1iY3dz+FDh0hOTmbu7Fl0iOhkV6ZDRCdmzpgOwIL582jWoiUiQoeITsydPYukpCQOHzpEbOx+6tZz7BfLWbma7ZzsOmF1OXggliOHrdkL58+mbYcIuzJt20cw67sZACxeNJ+mzVogIrR8sDW7d+/k4sWLpKSk8Mv6/1HDP8DhbJ/AEE4eO8SpuKOkXElm04ooajdtZVfm5NFDaT/v2LCachWrAFCzQTOOH9hL0uVLWFJS2Ld1E54+jncUnNnmYeF1OZAue96c2Vlkd0zLXrhgHs2at8yRx82EhIZz6GAsR49YsyMXzKVVW/vj3apdBHNnWQf8l0YuoHHT5ogIly5e5OI//wDwvzWrcHHJT/VsHO/qNesQf/QgJ44f4cqVZNb9uJAGLdrYlYnd8zuT33qJN6fMwLX0tc9Gi8XCuTN/A3Bo3y4O/bGbsEbNHc4OC7c/3lm1efuIjnx7tc3n51ybO/N3rFadMA4fjOXYkcMkJyezZOE8HmzTwaF1k5OTeXJwH7r26ke7jl0dr7C6q+7r05oZHAYiAEQkFPC5EyEuLi5MmDSFjh3aYLFYGDR4KIFBQYweNZLQsHAiOnZi8NBhDB08gCD/qri5uTNjprWTHRgURPeevahTKxAXFxcmTp7q8OkHZ+VqtvOy3xs/iZ5dOpCaaqHvgMH4BwTx7phR1K4TRrsOHek3cChPPTqYuiH+uLq58cXX1i8su7q58eQzz9GqWUNEhIdat6V12/YOZ+d3caH/v0bz0bMDSU210KRjL7z8qrPwsw+pElCLOg+04qe509kdvZ78LgUoVrIkj7z5EQDFSpaiTd9HeHtQR0SE4EYtCGny4D3T5h9O/JjOEW2xWCwMHDyEwMAg3n5rJKGh4XTo2IlBQ4bxyJCBBAdUw83dnekzvk9bP6C6D+fPnSM5OZmoqEgWL11OQECgw9lvfzCRfj06kmqx0LvfIGoEBDJu7FuE1AmjdbsI+vQfzIgnhtI4LBBXN3c++e83APz115/069GRfJKPCp6eTPrPVw7XGazH+8lX3+P1x3tjsVho3bUvlav6882U96geVJsGLdry5YejuHzxH8a+MAyAsh7ejJoyA0vKFV4aaO1MFS1egn+9N5X8Lo7/WXJxceGjiR/TqUNbLKkWBg4akvl4DxnGsMEDqRlQDTc3d7759lqb+1dL1+aLI4laupyAQMfb3Fm/Yy4uLrz53kcM7t2JVIuFHn0HUt0/kAnvjSa4digPtY1gx28xPDm4D2fPnmH1ih+Y9MEYlv28hR8i57P51/Wc+TuR+bOsHccPJn9OoO3LCfe6PPJoTcQY4+x9cCoROQyEA/8AkVjvT7IJaAi0sxVbYoypaSv/ElDcGDPqRtsNCws3GzbF3KG9Vuqafy6nOC178Z7sX4ScUx6uU8lp2ampzvvcPP1PstOyf48/57TsZtXLOC37YpLjt1TJaWcvXXFatl+5oluMMeFO24FsKlU5wDR5dXqObvOHJ+o7pQ3u+5EzY0yVdC9bX6dYzXTls/e1GqWUUkrdcQIIeWPo7L7vnCmllFIqb7jb39a8U/QLAUoppZRSuYiOnCmllFLq3peN21/kdjpyppRSSimVi+jImVJKKaXyhDwycKadM6WUUkrd+wTIl0d6Z3paUymllFIqF9GRM6WUUkrlCXlk4ExHzpRSSimlchMdOVNKKaVUnpBXbqWhnTOl7nGFCzr+cO6ctvHweadl9w5x3vMt8znxNuT/OPE5j16lijgt2+LE55kWcHHeSaYC+fUE1/1IO2dKKaWUuueJ5J1rzrRzppRSSqk8QW+loZRSSimlcpyOnCmllFIqT8gb42Y6cqaUUkoplavoyJlSSiml8gS9lYZSSimlVC5hfbams/ciZ+hpTaWUUkqpXOS6I2ci8jFw3bv+GWOevSN7pJRSSimVXSL3xWnNmLu2F0oppZRSCrjBaU1jzPT0EzA3w2t1i1YsX0atoBoE+Vdl3AfvZVqelJRE/769CfKvStNG9Tly+HDasnHvv0uQf1VqBdVg5Yrl90SuZjsne+XyZdSp6U+tgGp8OC7r7IH9+lAroBrNmzRIy169aiVNGoRTL7QWTRqEs3bN6mxnB5Uvzlttq/J2u6q0qVEm0/KGlV0Z36kGr7fy5fVWvjT2cU1b1qByKUa3rcrotlVpULlUtrNXLF9G7Zr+BAdUY/wN6h0cUI1m6eqdmJhIu9YtKedeghdGPJPt3KvZzjre/1u9gjZNatOqYTCffzw+0/LNv66na6tGBHqXZNmShXbLArxK0PmhBnR+qAFPDOqZ7ez1a1bSsVkdOjQJ4cupH2ZaHrNxPb3aNaFOFVdWLF1kt+yjd16n64N16dwijPdG/gtjsveYppUrllEnOICQwOp8OO79TMuTkpIY1L8PIYHVadG0od37vGnDutQPC6Fpw7qsu4X3+aoVywirFUDtoOp8dJ3swf37UDuoOi2bNuTIEWv2ls3RNKkfSpP6oTSuV4eoyIWZ1r2ZtT+toHm9YJqGBzJ14rhMyzf98jPtWzTAp1wxli5eYLds7vczeKBuEA/UDWLu9zOynZ2bXX1KQE5NznLTa85EpKGI7Ab22l6HiMgntxMqIlVEZOftbMOBjF/u5PZvlcVi4blnnyYy6kd+27GbubO+Z8/u3XZlpn31JW6ubuzaG8vwEc/z2qv/BmDP7t3MnT2Lrdt3sXjJMkYMfwqLxbHn7DkrV7Odl/3CiGdYsPgHYrbvYu7sWezZY589/esvcXV1Zcee/Tz97HO88drLAJQuU4a5CxYTvXUHn305jUeHDnQ4F6wX5T4c6sHHPx9h1LID1K1UCo8ShTKVizl2ljErDzJm5UE2HDoDQNEC+YkILMd7Px3ivZ8OEhFYjqIFHL809mq9Fy7+gS03qffve/bzTLp6Fy5cmDfeHM3Y9zL/oXM025nHe/SrL/DfmQtZum4LSxbNJXbfHrsyHt4VeXfSZ0R07ZVp/cKFixC5aiORqzbyn+lzs13vsa+/yKffLGDR6s38GDmPA3/stc/2qsiYj/5Duy722dtiNrItZiPzVmxkwapodm7fQszG9dnKfnHEcBZELmXztp3MmzOLvRmO9zfTvsLV1Y3tu//g6eEjGPn6tff5nPmRbNqync/++zWPDhuU7Xq/+Nxw5kUuJfq3ncyfe51sNze27fqDp4aP4E3bey0gqCZrN0SzftNW5kf+wHPDnyQlJSVb2a//3wimz4nkp1+2sXjBHP7Ya3+8Pb0r8uGUL+jcvbfd/DOn/2biuHdYvOJnFq9cz8Rx73DmzOls1T03E9upzZyanMWRT72JQBsgEcAYsx144A7uU44wxjRy9j5kZXN0NH5+VfHx9aVgwYL07N2HJVGRdmWWREXSb4D1g6Jb9x6sXf0TxhiWREXSs3cfChUqRBUfH/z8qrI5OjpX52q2c7JjNkfjmy67R6/eLM2QvTRqcVp21249WLvGmh1Suw4enp4ABAYGcfnSJZKSkhzO9nEvwp8XkvnrnytYjCHm2FlCvEo4tG5QhWLsOXmBi1csXLySyp6TFwiqUNzh7KzqnbnNs653sWLFaNS4CYUKF3Y4Lz1nHu8dv8VQuYovFSv7ULBgQTp07sFPy5fYlfGuWBn/wGDy5cvZ74Ht3BZDpSq+eFf2oUDBgrTt1J01K+yzvSpWpnpAzUyP1hERkpKSuJKcTHJyEilXUihdpqzD2dbj7ZfW5t179mZJ1GK7MkujIunb3/ofjC7derB2zepM7/OAW3ifb7ma7WPN7tazN0uX2Gf/sCSSvv2uZa9ba80uWrQoLi7Wq4ouJ13Odidg29bNVPHxo3IVa3bHrj1Z8WOUXZmKlaoQEJT5eK9bvZKmzR/E1c0dV1c3mjZ/kHU/rchWvrrzHPotNcYcyzDL8f/SXV9+EflCRHaJyAoRKSIij4rIZhHZLiLzRaQogIhME5H/iEiMiPwhIhG2+YNFJFJE1orIfhF58+rGReSC7d/mtuXzRGSviMwU22+CiISJyDoR2SIiy0XEwzb/WRHZLSI7RGSWbV4zEdlmm34TEcf+2mQQHx+Ht3fFtNdeXt7ExcVlLlPRWsbFxYWSpUqRmJhIXFzmdePj7dfNbbma7cTsit7262eV7X0tu1RJa3Z6ixbOJ6R2KIUKZR75uh7XIgU4ffFK2uvTF6/gWiTz5a2hXiV5o5UfjzX0xs223LVIAU5fSrfupRRcixRwODureifcpN4ls6j3rXDm8T55Ip4KXtfqXd7Di5MnEhxePynpMt3aNKFXh+asyvBH/ubZCZT39LLL/tPB7JCw+tRt2JQHw6vxYFg1GjV7EN9q/g5nJ8TH4WXXbl4kxGds8/ibvs8jb+F9Hp9VdobjnRAfn1bm6nvtb1t2TPQm6ocG0yg8hAmTP0nrrDniREI8numOt4enFycT4h1f19N+3RMOrpvbXb2VRk5OzuLIu+GYiDQCjIgUAEYAe26yjiOqAQ8bYx4VkTlAd2CBMeYLABEZAwwDPraVrwLUA/yANSJS1Ta/HlATuAhsFpGlxpiMX2aoAwQB8cAGoLGIbLJtu7Mx5pSI9AbeAYYCLwM+xpgkEXG1beMl4GljzAYRKQ5czoE2UCrX2r17FyNffZnIpdm//ulmdiScZ/Oxs6SkGpr6ujG4nhcT1h3J8RzlmDWb91Lew5NjRw4xqEd7qgcEUamK7x3PPXroAIdi97Ey2noa9LG+ndiyaQNh9Rvf8eyr9uzexcjXXmHRkmV3LRMgvF59Nm39nX179/DEI0No1aYdhW9x1FblPY6MnD0BPA14Ye3c1La9vl2HjDHbbD9vwdr5qikiP4vI70A/rB2qq+YYY1KNMfuBg8DV/16tNMYkGmMuAQuAJllkRRtjjhtjUoFttqwaWDt1K0VkG/A6cPW/EzuAmSLSH7h6IcAG4CMReRZwNcZkukBARB6zje7FnPrrVJaV9vT04vjxawORcXHH8fLyylzmmLVMSkoK586epXTp0nh5ZV7X09N+3etxVq5mOzH72HH79bPKPn4t++w5azZA3PHj9O3Zjc+/mo6vn5/DuQBnLl3Brei10S63ogU4c8n+1+WfZAspqdYLv9cfPE1ltyLX1k03UuZWxIUz6UbSbiarenvcpN7n0tX7djjzeJev4MmJuGv1PpkQR/kKHo6v72E9vVexsg/1GjVl987t2cj24GS60aqTCXGUczD7p+VR1KpTj6LFilO0WHGatGjN9q2On8718PQizq7d4vDwzNjmnjd8nz/cqzuffTkt2+9zz6yyMxxvD0/PtDJX32vuGd5rNfwDKFa8OLt3OX4ZdgUPT+LTHe+E+Li0Y+jQuvH261ZwcN17wX1zzZkx5i9jTD9jTHljTFljTH9jzO2fA4D0J/ctWEfxpgHPGGOCgbeA9P+NyPgVHnOT+TfLEmCXMaa2bQo2xrS2lekATAVCsY7GuRhj3gMeAYoAG0Qk09i7MeZzY0y4MSa87HWumwivW5fY2P0cPnSI5ORk5s6eRYeITnZlOkR0YuYM6xdiF8yfR7MWLREROkR0Yu7sWSQlJXH40CFiY/dTt169LHNyS65mOyc7LLwuB9Jlz5szm/YZsttHdEzLXrhgHs2aW7PPnDlD9y4RvPXOuzRslP0RjMOnL1GueEFKFy1AfhHCK5Zie/x5uzIlC18btA/xLEHCOeuv6K4T/xBYoThFC+SjaIF8BFYozq4T/9xWvTO3edb1vl3OPN7BtcM4fOgAx44eJjk5maWR82jZpoND6549c5pk27VWfyf+xdbNG6majVOLQSFhHDl8gONHD3MlOZlli+fTvJVj2R6eFYnZtJ6UlBSuXLlCzMb1+Fat4XC29XjHprX5/Lmz6RDR0a5M+4hOfPftNwAsWjCPZs1bpL3Pe3TtyFtjxt7S+zz0avZha/aCubNp3yFDdodOfDfzWvYDzazZhw8fSvsCwNEjR9i/by+VK1dxODukTjiHDsZy9Ig1O2rhXFq1i3Bo3WYtW/HzmlWcOXOaM2dO8/OaVTRr2crh7NxOcnhylpue1hQRX2AS0ABrx+dX4HljzME7sD8lgATb6dN+QPoT+D1FZDrgA/gC+7CermwlIu7AJaAL1tOSjtgHlBWRhsaYX22Z1bGesq1ojFkjIuuBPkBxESltjPkd+F1E6mIdudt73a1fh4uLCxMmTaFjhzZYLBYGDR5KYFAQo0eNJDQsnIiOnRg8dBhDBw8gyL8qbm7uzJg5C4DAoCC69+xFnVqBuLi4MHHyVPLnz5+rczXbedkfTvyYLhFtsVgsDBg8hMDAIN5+ayShoeF06NiJQUOG8ciQgdQKqIabuzvTZnwPwGefTuHggVjee+dt3nvnbQAily6nXLlyDmWnGpj1WwIjHqhMPhE2HDpNwrkkOgaV5cjfl9mRcJ6WVd0J8SyBxcDFZAvTNlt/1S9esbB09yleech6Sm3p7lNcvOL4Ja5X693ZVu+BN6h3sK3e0231Bgio7sP5c+esf/CiIlm8dDkBAYEOZzvzeI8c+yGPPNwZi8VC9z4DqVYjkEkfvE3NkFAebNOBHdu28MzQPpw7c4Y1K3/k43HvsHRdDAf27+PN/xuO5MuHSU3l0WdepGqNgGxlv/r2eJ7s3wWLJZUuvQdQtUYAU8ePIbBWHVq07sDObVt47tG+nDt7hnWrfuTTj95h4U+badWhC9G/rKN7q/qICI2bPUTzVu2zlT1+4mS6dGxHqsXCgEFDCAgMYsxbb1InLIwOEZ0YOHgojw4dSEhgddzc3fn6m+8A+PzTqRw8EMv7Y8fw/tgxAEQuWUZZB9/nLi4ujJ8wmW4d22GxWOhvy35n9JvUCQ2jfUQnBgweymNDB1I7qDpubu58NcOavfGX9UwY/wEFChRA8uXjw0lTKF0m8y1nbpT99vsTGdCzIxaLhd59B1HDP5AP332L4NphtG4XwfatMTw6sDdnz55m1fIf+Oi9t/npl99wdXPn2ZdeoeND1g7piJdexdXN3eFsdXfIze4pIyIbsY4iXf0E6wMMN8bUv+VQkSrAEmNMTdvrl4DiwEng/4BTwCaghDFmsIhMw3qNVzhQEnjBGLNERAZj7ZCVwnpK8ltjzFu2bV4wxhQXkebAS8aYq18imALEGGOmiUhtYLJtfRes30ydBqyxzRPbNt8T6xMTWgCpwC5gsDHmul/tCQsLNxs26X181Z1nSc3efaFy0guRu29e6A6Z0NmxTtOdkM+JVwof/eui07KTUlKdlu1TtqjTsp34K8bZi46f1s9plUoX3mKMCXfaDmRTWb8g03ns7Bzd5pd9gp3SBo58IaCoMSb9Xeq+FZF/3U6oMeYw1uu9rr5Of8fET6+z2ipjzBNZzD9ujOmSRUZx279rgbXp5j+T7udtZH1bkEzXrRljhl9nv5RSSimlcsyNnq15dZzzRxF5GZiF9bRmb+CHu7BvSimllFIOyyOP1rzhyNkWrJ2xq1V9PN0yA7xyp3YqI2PM4OvMn4b1NKRSSimlVJ5w3c6ZMcbnbu6IUkoppdTtcObtL3KSQ7ckFpGaQCDpbm1hjPnmTu2UUkoppVR25ZG+mUO30ngTaI61c/YD0A5YD2jnTCmllFIqhzkyctYDCAF+M8YMEZHywLd3dreUUkoppRwnCPnyyNCZI49vumR77FGKiJQE/gQq3mQdpZRSSil1CxwZOYuxPfz7C6zf4LyA9SkBSimllFK5g9xH15wZY56y/fgfEVkGlDTG7Lizu6WUUkoplT15/tuaIhJ6o2XGmK13ZpeUUkoppe5fNxo5+/AGywzQMof3RSl1C/I78TmPk7oGOS3bre4zNy90h5zePMVp2d7uRZyW7cxnijrTzZ5BfSeVLVnIadn3IkcupL8X3OgmtC3u5o4opZRSSikHb0KrlFJKKZWbCffBNWdKKaWUUveSvHLmPa+cnlVKKaWUyhMceXyTAP0AX2PMaBGpBFQwxkTf8b1TSimllHLQ/TRy9gnQEHjY9vo8MPWO7ZFSSiml1H3MkWvO6htjQkXkNwBjzGkRKXiH90sppZRSymEi99cXAq6ISH6s9zZDRMoCqXd0r5RSSimlsul+Oq05GVgIlBORd4D1wNg7uldKKaWUUvcpR56tOVNEtgAPYr2NSBdjzJ47vmdKKaWUUtmQR85q3nzkzPbtzItAFLAY+Mc2T92iFcuXUSuoBkH+VRn3wXuZliclJdG/b2+C/KvStFF9jhw+nLZs3PvvEuRflVpBNVi5Yvk9kavZmn03s1s1CmD7wjfYGfkmLw1plWl5JQ83fvjPcKJnv8LyL0bgVc4VgAfCq7Fx1stp0+mNE+jYvFa2sp3d5rVr+hMcUI3x47LOHtivD8EB1WjWpEFadmJiIu1at6ScewleGHFrj8Rydr2dmR0S5E/NgGqMv072gL59qBlQjQca27d521YtKetWgufvwTZXd4Ex5oYT8Duww/bvfiAF2HWz9e7mBFQBdjp7P9JPoaFh5tIVk2m6cDnF+Pj6mt37Dpiz/ySZ4OBaZuv2XXZlJk6eah559HFz6Yox07/93nTv2ctcumLM1u27THBwLXPmwmWz54+DxsfX11y4nJJlTm7J1WzNvlPZhWs/nWkqGvqMOXD0T+PfYaQpEf6s2b7vmKnd7W27MvNXbDHD3vjGFK79tGnz6CQzM2pTpu14PPAvk3jmgnFr8FyWOc6s9z9JqZmmcxevGB8fX7NzT6w5ff6yqRlcy8Rs22lXZsKkKWbYI4+Zf5JSzbQZ35nuPXqZf5JSzZ9/nzcrV//PTPr4E/P4E09luf2r0/36XruYnJppOn/pivHx9TW79saaMxcum+DgWmbLtp12ZSZMnmKGPfqYuZicaqbb2vxicqo5dfq8WbXmf2bSlE/M408+leX2r07OrDcQ4+y/pdmZKlQNMv9eui9HJ2e1wU1HzowxwcaYWrZ/qwH1gF/vSE/xPrA5Oho/v6r4+PpSsGBBevbuw5KoSLsyS6Ii6TdgEADduvdg7eqfMMawJCqSnr37UKhQIar4+ODnV5XN0Y7dbs5ZuZqt2Xczu27NKhw49heH4xK5kmJh7vKtRGQY/fL39WBd9D4A1m3+g4jmwZm20/WhOqzYsJtLl6/cE/WO2RyNb7rsHr16Z5G9OC27a7cerF1jzS5WrBiNGjehUOHCDufllno7u839btLmS6MW0/9qm3fP3OaF78E2V3dHtp8QYIzZCtS/A/tyu/KLyBcisktEVohIERF5VEQ2i8h2EZkvIkUBRGSaiPxHRGJE5A8RibDNHywikSKyVkT2i8ibtvmjReS5q0Ei8o6IjLiVnYyPj8Pbu2Laay8vb+Li4jKXqWgt4+LiQslSpUhMTCQuLvO68fH26+a2XM3W7LuZ7VmuFMdPnk57HXfyNF5lS9mV+f2PODq3rA1A55YhlCxeBPdSxezK9GwTypxlWxzOTauTM9u8orfd+glZZXunyy5pzb5dTq+3s7Lj4vDytm/zjOtby2TOvl3OrHduly+HJ2dx5AkBL6R7mQ8IBeLv2B7dumrAw8aYR0VkDtAdWGCM+QJARMYAw4CPbeWrYB0F9APWiEhV2/x6QE2s19ltFpGlwFfAAmCiiOQD+tjK2RGRx4DHACpW0svylMqNXpmwkAn/7kn/TvXZsDWWuJOnsViu3R2oQpmSBFXzZOWvu524l0qpW3HffCEAKJFuKgQsBTrfyZ26RYeMMdtsP2/B2vmqKSI/i8jvWB9BFZSu/BxjTKoxZj9wEPC3zV9pjEk0xlzC2iFrYow5DCSKSB2gNfCbMSbTf3+MMZ8bY8KNMeFly5TNcic9Pb04fvxY2uu4uON4eXllLnPMWiYlJYVzZ89SunRpvLwyr+vpab/u9TgrV7M1+25mx/95Fu/ybmmvvcq7EXfqrF2ZhFNn6fPSf2n48Pu8OSUKgLMXLqUt794qlMWrd5CSkr3bOTq9zY8dt1vfI6vs4+myz1mzb5fT6+2sbC8v4o7bt3nG9a1lMmffLmfWW90dN+yc2W4+W8IY85ZtescYM9MYc/ku7V92JKX72YJ1VHAa8IwxJhh4C0h/gt9kWN/cZP5/gcHAEKwjabckvG5dYmP3c/jQIZKTk5k7exYdIjrZlekQ0YmZM6YDsGD+PJq1aImI0CGiE3NnzyIpKYnDhw4RG7ufuvUyDeDlqlzN1uy7mR2z6whVK5WlsmdpCrjkp2ebUJau3WFXprRrsbS7iP9raBumR260W96rbRhzlsU4nJkb6h0WXpcD6bLnzZmdRXbHtOyFC+bRrHnLHLmb+v36XgsLt8/Oqs3bR3Tk26ttPj9vtHluJiLky+HJWa57WlNEXIwxKSLS+G7uUA4rASSISAGsI2fpT6z3FJHpgA/gC+wD6gCtRMQduAR0AYbayi8ERgMFgL63ukMuLi5MmDSFjh3aYLFYGDR4KIFBQYweNZLQsHAiOnZi8NBhDB08gCD/qri5uTNj5iwAAoOC6N6zF3VqBeLi4sLEyVPJnz9/rs7VbM2+m9kWSyrPvz+HqE+eJn8+YXrkRvYcPMEbT3Zg6+6jLF33Ow+EV2P08E4YA+u3xvLcu3PS1q/k4Y53BTd+3hLrcGZuqLeLiwsfTvyYzhFtsVgsDBw8hMDAIN5+ayShoeF06NiJQUOG8ciQgQQHVMPN3Z3pM75PWz+gug/nz50jOTmZqKhIFi9dTkBA4D1Rb2dmfzTxYzp1aIsl1cLAQUMyZw8ZxrDBA6kZUA03N3e++fZam/tXS9fmiyOJWrqcgMDc3+bq7hBjMg4U2RaIbDXWZ2p+CngBc4F/ri43xiy4O7t4cyJSBVhijKlpe/0SUBw4CfwfcArYhHUUcLCITAMuA+FASeAFY8wSERmMtUNWCvAGvjXGvJUu5z/AGWPMyzfbp7CwcLNhU/b/962Ucoxb3Vu7P1ROOL15itOyU1Oz/sy+G/LllWfjZNP1/k7eDc58VmSRArLFGBPutB3IJs/qweaRj3O2a/J22+pOaQNHnq1ZGEgEWmI9xSe2f3NN58x2TVjNdK/Hp1v86XVWW2WMeSKL+ceNMV0yzrR9EaAB0PPW91QppZRSd0pe+f/DjTpn5Wzf1NzJtU7ZVc77b4QTiEggsARYaPsCgVJKKaXUHXGjzll+rKcGs+qH3tOdM2PM4OvMn4b1SwQZ5+/Gel2aUkoppXIhAadexJ+TbtQ5SzDGjL5re6KUUkoppW7YOcsb3U+llFJK3RfyyMDZDTtnD961vVBKKaWUuh2Sd74QcN2b0Bpj/r6bO6KUUkoppRy7lYZSSimlVK4neeSKLGc+dF0ppZRSSmWgI2dKKaWUuudZb6Xh7L3IGdo5U0oppVSeoJ0zdUMG5z2PzZnPYlP3l8vJFqdlO/P5lnO3HXNadvda3k7LTjhz2WnZ5UsWclr2sb8vOS1736lzTstWzqOdM6WUUkrlCXllcEK/EKCUUkoplYvoyJlSSiml7nl56QsBOnKmlFJKKXWLRKStiOwTkVgRefkG5bqLiBGR8JttU0fOlFJKKXXvk7v/bE0RyQ9MBVoBx4HNIrLYGLM7Q7kSwAhgkyPb1ZEzpZRSSuUJ+URydHJAPSDWGHPQGJMMzAI6Z1HubeB9wKGvPGvnTCmllFIqa2VEJCbd9FiG5V5A+nvrHLfNSyMioUBFY8xSR0P1tKZSSiml7nl36AsBfxljbnqN2PWISD7gI2BwdtbTkTOllFJKqVsTB1RM99rbNu+qEkBNYK2IHAYaAItv9qUAHTlTSimlVJ7ghHvQbgaqiYgP1k5ZH6Dv1YXGmLNAmWv7J2uBl4wxMTfaqI6c3WUrli8jJMifmgHVGP/Be5mWJyUlMaBvH2oGVOOBxg04cvgwAImJibRt1ZKybiV4fsQzt5xdK6gGQf5VGXed7P59exPkX5WmjeqnZQOMe/9dgvyrUiuoBitXLNdszb6hVSuWUbd2IKHBNZgw/v0ss4cOfJjQ4Bo81KwhR48ctlt+7NhRvMuV4uOJH2Y725n13vHLWv6ve3Ne6tqUqGlTMy1fPX8Gr/Zpxet92/L2I92IO/gHAKfijzGsSTVe79uW1/u25et3X8l29orly6hd05/ggGqMH5d1vQf260NwQDWaNbH/bGnXuiXl3Evwwi1+tqz7aQUPNqhFi7pBfDppXKbl0b+sp2PLhlSrUJwfFi9Im7/79+10b9eMNk1CadesLksWzs12tjPr/fPqFbRtUpvWDYP5/OPxmZZv/nU93Vo1Isi7JMuWLLRbFuhVgi4PNaDLQw14clDPbGdvXb+aJzs24fEODZn35ceZlv84ZzrPdmvBcz0f4uVBnTh6YB8A5878zWvDutO7vh+fjX0127m5m5Avh6ebMcakAM8Ay4E9wBxjzC4RGS0inW61JvdN50xEqojIzmyU7yIigTm5DxaLhedHPMOiqB/Yun0Xc2fPYs9uu2/bMu3rL3F1c2Xnnv0Mf/Y5Xn/VesuUwoULM3LUaMa+n/mDz9Hs5559msioH/ltx27mzvo+c/ZXX+Lm6sauvbEMH/E8r736bwD27N7N3Nmz2Lp9F4uXLGPE8KewWBx/pqJm33/Z/3rhWeYuXMLGLb8zf+5s9u6xz54x/StKubqx9fd9PPnMc4x6w74z8vrLL/FQ67YOZ+aGeqdaLHzzweu8NGk67835iY0rFqd1vq5q2KYLY2etZMx3y+gw4Am+m/B22rJyXpUZ890yxny3jCGvvJvter8w4hkWLv6BLVc/WzK0+fSvv8TV1ZXf9+znmWef443Xrn22vPHmaMa+d+ufLW++/Bxfz4pk+YbfiFo4l/379tiV8fSuyAcff06n7r3t5hcuWpTxU75k+fqtTJsdyduv/x/nzp65Z+o9+tUX+GLmQpas28LSRXOJzVBvD++KvDvpMyK69sq0fuHCRVi0aiOLVm3k0+nZ65RaLBY+G/sqb346kymL1vHzj4vSOl9XNWvfjckL1jBx7iq6Dn6ar8aNAqBgwcL0e/r/GPziyOxVWF2XMeYHY0x1Y4yfMeYd27yRxpjFWZRtfrNRM7iPOme3oAuQo52zmM3R+PlVxcfXl4IFC9KjV2+WREXalVkatZj+AwYB0LV7D9au+QljDMWKFaNR4yYULlz4lrI3R9tn9+zdJ1P2kqhI+tmyu3XvwdrV1uwlUZH07N2HQoUKUcXHBz+/qmyOjtZszc7SlphofH39qOJjze7Woxc/LLH/jPpxyWIe7jcAgM5du7Nu7WqMMQAsjYqkUuUq+Adk/9fPmfU+sGsb5SpWoZx3ZVwKFKRBq45sXbfCrkyR4iXSfk66fCnHngMYszka35t8tiyJWpxW767dMn+2FLrFz5btWzdTuYoflar4ULBgQSK69GTlj0vsynhXqkxAUDD5xP5Pjq9fNXz8qgJQvoInpcuWJfGvvxzOdma9d/wWQ6UqvlSsbK13+849+Gl5hnpXrEyNwGAkX87+qd2/8zcqVKpCBe/KFChQkKZtOxO9xn6kt2j699qli2nvtcJFixIYWp+ChW6t3rmZYD2tmZOTs9xvnbP8IvKFiOwSkRUiUkREHhWRzSKyXUTmi0hREWkEdALGicg2EfGzTctEZIuI/Cwi/tkNj4+Lw8vbO+21l5c38fFxWZSxXlvo4uJCyVKlSExMvK1KA8THx+Htfe2aRS8vb+Li4jKXqZg5Oy4u87oZ91uzNfuqhPj4tPcwgKeXNwkJ8Rmy4+3f5yVL8XdiIhcuXGDSRx/w71dv7X/1zqz36VMnKF3eM+21e3kPTp86mancqjnTealLE2ZPHkv/l95Km38q/hiv92vHO4/1ZN9vDt2nMkOd7D9bErKqd4Y2z4nPlhMJ8Xh4Xcv28PTiZILj7XbV9q2buZKcTGUfX4fXcWa9T56wr3cFDy9OnkhweP2kpMt0b9OE3h2as+rHqGxlJ548QZny1+7WULq8B4l/nshUbumsr3m8fQOmTRjDoy+PyVaGcq77rXNWDZhqjAkCzgDdgQXGmLrGmBCs54uHGWN+ARYD/zLG1DbGHAA+B4YbY8L+v73zjq+i+PrwcyAUAYGEmoTekwCBJFTpFkroXZRuF8Hys7wW7CiCCvbesNAhBJAmxU4VkKaA1ARQQTok5mbeP2aT3JsESeCGG8J5+OyH7O7sfmdm9+6ePTNzBvgf8Fb6k4vIbSmxUP7++69LVCRFyTuMff5p7hxxL8WKFfN1VnKM6/oOZvzs7+l7z/8R89FrAJQsXZZXY3/muS++ZsB9T/D24yM5c/KEj3N66fjz4AHuv2s4L732Lvm87GXKrSxdvY0ZC79n/FsfM2b0Q+zd/YfXNaL7D+Xd+T8z+N7HmPreBK+fP9chNpSGNxdfcWX8CtLYZYxZ7/y9FqgC1HU8Yb8CNwFh6Q8SkWJAc2CaiKwH3gUC06czxrxnjIkyxkSVLl0mg3hQcDBx+/enrsfF7ScoKDiTNDaeXVJSEsePHaNUqVLZL2l67aBg9u9Pi5MXF7ef4ODgjGn2ZdQODs54bPp8q7ZqpxAYFJR6DwPEx+0nMDDII02QW5qkpCSOHz9GQKlSrFmziicff4T6IdV5+83XeGX8i7z3TsaO9bmx3P5lynP4UJqH8MihA/iXKXfO9E1v6Mq65bbZs0DBQlxd0h+AqiH1KVuhMgf2Zv1lbcvk+WwJzKzc6ercG8+W8oFBHIhL0z4QH0e5wKzX24kTxxk+oCcPPPoUDaOaZEvbl+UuV96z3AcPxFGufIbXwrmPd34TFStXpXHzlmzZtCHLx5YqV56/D6V5CA8fOkCpsuXPmb5lx+6sXLYgy+e/nPHBDAE5Uw6fKfuGBLe/XdhQIp8AI4wx9YCngcwa4vMBRx0vWsoSkl3xyKhG7Nixnd27dpGYmMj0qVOI7uw5mKNT5y58PulTAGbNmE7rNu280i8lqpGn9rQpkzNoR3fuyheO9swZ02nd1mpHd+7KtCmTSUhIYPeuXezYsZ1GjRurtmpnSkRkI3bu3MGe3VZ75vSpdIzu4pGmQ3QXvvpiEgAxs2bQqnVbRISvF69g49adbNy6kzvvHsn9/3uE2+64+7Iod7XQcA7t3cVfcXtJ+jeRnxfH0rDV9R5pDu7dlfr3hu+/oVylKgAc/+cwyc7ggz/37+HQvl2UDa6cZe3IqEbsPM+zJbpzl9Ryz5rpvWdL/YZR7N61g317dpOYmMjc2dO4rkN0lo5NTEzkjsH96NF3AJ269sy2ti/LXa9BJHt27WT/Xlvu+THTadc+a+U+dvQfEhPs6+ifw3/zy+qfqVEz6z1laoY14MCeXRzav5d//03kuwUxNG7T3iNN/J40437Nt0sIrFQ1y+dXfI/GObMB4g6ISAGs5yzlc+SEsw9jzHER2SUifYwx08T+susbY7L+qYPt7/DKhNfpGt0BV7KLQYOHEhoWxjNPjSYiMorOXboyZOhwhg8ZRN2Qmvj7B/DZ51+lHl+nZlVOHD9OYmIisXNiiJ23kJDQrHWa9vPz49WJb9Aluj0ul4vBQ4Zl1B42nGFDBhJWpwb+/gFM+mIyAKFhYfTq05eG9UPx8/Njwmtvkj9//myVW7WvLO2XXp5Ir26dcLlc3DRoCCGhYYx59kkaRETRKboLAwcP445bBhNRrzb+/v58+OmXWT5/bi13fj8/Bj30LC+NHIhxuWjVtR8VqtdmxjsvUzWkHhGtb2DJ1E/YvOp78vsVoGjxEtz25CsA/PbLSma+8zL5/Qog+fIx5JExFCtRMlvlfnnC63Tr3AGXy8WgIUMJDQ3j2adHExERRXSXrgweOpxbhg6iXkhN/AMC+HRS2rMlpJbbsyU2hjnzFhKSxQEZfn5+PPXCqwzu24XkZBd9bhxMrTqhvPriM9RrEMF1HTqz4Zc13Dm4H8eOHeWbRfOZ+NJzLPx+HfNjZrD6p+85euQIMyZ/DsC4198jtF74ZVHuJ8a8zPAbu5HsctGr/yBq1g7ltZeepW54BO3aR/Pr+rWMGNaf40ePsmzx17wx7nnmrljDzu2/8eRD95AvXz6Sk5O5dcQD1Kid9e/9/H5+3PboGJ6680aSXS6u7d6fSjVq88WbL1EjNJwmbdsz76uP2LDyO/yce+3e515LPf7WDo04ffIkSf8msnLpAp569ysqVa+dZf3cSsqAgLyApIyQyuuISBVgrjGmrrP+P6AYcAh4CPgLO1v81caYISJyDfA+1tvWG0gG3sY2ZxYAJhtjnjmXXkRklPnh59U5V6D/wFsjwBTlfJxNzHqoCW9TuGDWDSdvM239vvMnyiF61a9w/kQ5xKHjCedPlEOUK17IZ9r7jpzxmfZvfx33mXa3+oFrL2bqoktNlZD65rFPsje44nzc1rSKT+rgivGcGWN2Y6dQSFl3jxj4dibpfyBjKI3sB15SFEVRFOWS4Mt+Yt7kijHOFEVRFEXJ2+QR2+yKGxCgKIqiKIqSq1HPmaIoiqIolz1C3vE45ZVyKIqiKIqi5AnUc6YoiqIoyuWP5J1oBWqcKYqiKIqSJ8gbppk2ayqKoiiKouQq1HOmKIqiKMplj5B34pyp50xRFEVRFCUXoZ4zRVEURVHyBHnDb6bGWY5hJ2DNK7eJomRO3D++m3PQv2hBn2n3Dvfd/JYDJ63zmfYbver5TDtfPt89T8tc7bt7rczVpX2mrfgONc4URVEURckT5BWfiBpniqIoiqLkASTPtFjpgABFURRFUZRchHrOFEVRFEW57NG5NRVFURRFUZQcQT1niqIoiqLkCfJKnzM1zhRFURRFyRPkDdNMmzUVRVEURVFyFeo5UxRFURTl8kfyTrOmes4URVEURVFyEWqc+YBFCxdQP6w2YXVqMO6lFzPsT0hI4OYB/QirU4OWzZuwZ/fu1H3jxr5AWJ0a1A+rzeJFCy8LXdW+8rS/X7aYzq0a0vGacD544+UM+9f8/D19OrQgvHJJFs2dnbp91Q/f0uuG5qlLRPXSfLMgNlvay5YspGWjulwTEcIbr47LsD8hIYE7ht3ENREhdL6uBfv27gYgMTGR++6+lWubR3Bdiyh+/H5FtnTB1nl4WB3qhtRk/DnqfOCA/tQNqUmra5pmqPO6ITUJD6tzQXXeILg4E3uF8XrvMLrXL5dhf5sapfjwxvqM6xbCuG4hXFurVOq+0kUL8ET7mkzoGcqrPUIpUyx70xUtXbKQFlF1adYwhNfPUee3D72JZg1D6HRtC/bt2Q3Av//+y8g7htO2eQQtG9fntVdeyl6h8e19/s3ihTRuGEZU/TpMeDlj3hMSEhg+aABR9etwfZvm7HXKvXfPboJLX03rZpG0bhbJAyPvuqy0cyspoTS8ufiKK844E5GTvtR3uVzcO/JuYmK/5peNW5g2+Su2btnikeaTjz7Ev6Q/m7ft4J5R9/HYow8DsHXLFqZNmcy6DZuZM3cBo+65C5fLlat1VfvK1H7u8Qd4e9JM5ixbzfyY6ez8fZtHmsDgijz3yjt06t7XY3vja1oxY9GPzFj0Ix9NmUvhwkVo3vrabGk/9uAoPp82h2U/b2D2jCn8vm2rR5qvJn1MiRIl+WHdVm69cyTPP/UYAF9++iEA3/y4jsmz5vPM4w+TnJycLe37Ro1gdux81m3YzLQpkzPW+ccfUtK/JJu2bueekffy+KOPALbOp0+dwtr1m4iZ+zX3jrw7W3WeT+CWZpV4ftF27pu5hRbVAqhQsnCGdD/u+ocHY7byYMxWvvn9cOr2e1pVJebXg9w7cwv/F7uNY2f+zVa5H/3fKL6YPocVKzcwe/oUfsuszkuW5KdftnLbXSN5zqnz2NkzSExMYNmP61i4/GcmffxBquGWVW1f3ucP3T+SqTNj+XHNRmZOm8y2rZ7an3/6ESVLlmTNxm3cefconn7i0dR9VapWZ8VPa1nx01pefu2tLOv6Wju3IyJeXXzFFWec+ZrVq1ZRvXoNqlarRsGCBenTrz9zY2M80syNjeGmgYMB6NmrN8uXfoMxhrmxMfTp159ChQpRpWpVqlevwepVq3K1rmpfedq/rl9DpSrVqFi5KgUKFqRjt14sXTTXI01wxcrUDq37n5NZL5o3m5Ztr+eqq4pkWfuXtaupUq06lavYcnfr2ZeF8z09b4u+jqXPjQMBiO7Wk+9XLMMYw++/beWalm0AKF2mLMVLlGDDL2uzrL1mtWed9+7bL0Odz4udw81Onffo1Zvly9LqvHfffh51vmZ11uu8RumiHDx+lj9PJJKUbPjhj39oVKlklo6tULIw+fIJG+NPAHA2KZlEl8mydoY675WxzhfMj6WvU+edu/XkO6fORYTTp06RlJTE2bNnKFiwAMWKF8+yti/v83VrVlG1WnWqVLXaPXr34+t5nuX+el4s/W+y5e7aoxffLl+KMVmv29yorVwarljjTCzjRGSTiPwqIv2c7ZNFJNot3Sci0ltE8jvpV4vIRhG5/UJ04+PjqFChYup6cHAF4uLiMqapaNP4+flRvEQJDh8+TFxcxmPj4z2PzW26qn3laf954ADlA4NT18uVD+bPAweyfHwKX8+ZQcfuvbN1zMED8QQFp+U9MCiYgwc8834wPp6g4AqAU+7ixfnnyGFC69Zn0YK5JCUlsXfPLn5d/wvxcfuzrB0fF0dwhQqp65nVm02Tsc7TX6+g4GDi47Je5wFFC/D3qTRv1+FTiQQUKZAhXdMq/rzcPYQH2lajVFG7P7B4IU4nJPFgu2qM6xbCwEbB/IfNnIGDB+IJPl+dH8hY50eOHKZzt54UKVqU8NqViapbgzvuuQ9//4Asa/vyPj8QH+9xvYOCgzmQ7vgD8fEEpbveRw5bj+XePbto0zyKLu3b8dMP32dZ19fauR3x8uIrruTRmj2BBkA4UBpYLSLfAlOAvsA8ESkIXAvcCQwHjhljGolIIeAHEVlkjNmVckIRuQ24DaBipUqXsiyKkqf469BBtm/bzDWtr7tkmv1vHsL237fRsW0zKlSsRFTjpuTPn3e+X9fsO8r3fxwhKdlwfe3SjGhZhacXbCd/PqFO+at5MGYLf59M5P621WhToxRLtx8+/0kvkl/WriZf/vys37abY0f/oXvHdrRq047KVarluLYvKVc+kA1b/yCgVCnW/7KWgf1788PqDRTPhtfwctRWsk7eefJknxbAV8YYlzHmELACaAR8DbR1DLCOwLfGmDPADcAgEVkPrARKATXdT2iMec8YE2WMiSpTukymokFBwezfvy91PS5uP8HBwRnT7LNpkpKSOH7sGKVKlSI4OOOxQUGex54LX+mq9pWnXTYw0MNzcuhgHGUDA7N8PMCC2Jlc26ELBQpk9P78F+UDg4iPS8v7gfg4Dy8eQPmgoFSPWFJSEsePH8c/oBR+fn48PWY8i79bzcdfzuDYsWNUq14ry9pBwcHE7U/ztGVWbzZNxjpPf73i4+IICs56nR859S+li6bVVamiBTly2rPf2MkEF0nJtlnrm9//plrpogAcPvUvuw+f5s8TiSQbWLXnKNVKZ70puXxgEHHnq/PAjHUeEFCKWdMn0/baGyhQoACly5SlUZPmbPhlXZa1fXmfBwYFeVzv+Lg4AtMdHxgURHy66x1QqhSFChUioJQdkNGgYSRVq1Zj547fLwvt3I6IdxdfcSUbZ5lijDkLLAfaA/2wnjSwHs57jDENnKWqMWZRds8f1agRO3ZsZ/euXSQmJjJtymSiO3f1SBPduStfTPoUgJkzptO6bTtEhOjOXZk2ZTIJCQns3rWLHTu206hx41ytq9pXnnbd8Ej27trJ/r27+Tcxka9jZtD2+ujzH+jG1zHT6NStT7aOAWgQEcWunTvYu8eWO2bmVG7o2NkjzQ0dOjPtq0kAzIuZyTWt2iAinDl9mtOnTgHw7bIl+Pn5UatOSJa1I6M863z61CkZ6rxT5y587tT5rBnTad0mrc6nT53iUedRjbJe5zv+PkVgicKULVYQv3zCNdX8Wb33qEeaklelNZREVSpJ3NEzAOz8+xRFC+WneGG7v27g1ew/ejbL2ql1vtup8xlTaZ+uztt37MxUp87nxsykhVPnwRUq8cO3ywE4feoUa9espEbN2lnW9uV93jCyEX/s3MEep9yzpk+hYyfPcnfo1JnJX9hyz5k1g5at2yIi/P3XX6mDD3bv+oOdO3dQJRveQl9q52bsaE3x6uIrruRmze+A20XkUyAAaAU86OybAtwCRAFDnG0LgTtFZKkx5l8RqQXEGWNOZUfUz8+PVye+QZfo9rhcLgYPGUZoWBjPPDWaiMgoOnfpypBhwxk2ZCBhdWrg7x/ApC8mAxAaFkavPn1pWD8UPz8/Jrz2Jvnz58/Vuqp9ZWo/+ux4br+pO67kZHr0G0iN2iG8Me45wsIb0vaGaH5dv5Z7bxnA8WNHWb74a9585Xlilq4GIG7fHg7GxxHVrEWWNd21n3tpAgN6dSbZ5aLfTUOoHRLKuDFPE94gghs6daH/wKGMvGMo10SEUNI/gLc+tC+wv//+kwG9OpMvXz7KBwbx2jsfZVv7lQmv0zW6A65kF4MGD81Y50OHM3zIIOqG1MTfP4DPPv8KsHXes3cfIsLD8Mtvr1126jzZwAc/7eXx9jXJJ8LS7X+z/+hZ+jUMZOffp1mz7xidQsvSqFJJXMZwMsHFG9/tTj32s1X7ebJDTUD44/Aplvz2d7bKPWbcBG7s1RmXy0X/m22dv/T804Q3jKB9py7cOHAo99w+lGYNbZ2/85Gt86G33MG9d99K66YNMMbQ/6ZBhNatly1tX97nY1+eSJ/u0bhcLgYMHEKd0DBeePYpGkRE0jG6CzcPHsadtwwhqn4dSvr788EnXwDw4w/f8eJzT1OggB/58uXj5Ylv4h+Q9b52vtRWLg1ypY3eEJGTxphiYsfIvoRtujTAc8aYKU6aAsAhIMYYM9TZlg94DuiCNdD/ArobY45lphMZGWV+WLkmx8ujKL5k5yHfRabxL5q9WFze1c5ec6s3GTgp681+3uaNXlk3nLxNSR9e79MJST7T9iWlihVYa4yJ8nU+skrNsHDz6pRsN2j9J13qlfdJHVxxnjNjTDHnf4P1lD2YSZp/sd40923JwKPOoiiKoiiKkiNcccaZoiiKoih5EUF8GgDDe+iAAEVRFEVRlFyEes4URVEURckT+DL8hTdR40xRFEVRlMuelFAaeQFt1lQURVEURclFqOdMURRFUZTLHx9H9fcm6jlTFEVRFEXJRajnTFEURVGUPEFe8ZypcaYoiqIoSp5A45wpiqIoiqIoXkc9Z4pymZOc7Lv5cQv6+e77LqCY7+ZaTHIl+0z7gxsb+Ex71KzNPtN+p4/v5vV0+fA3tv/IGZ9pX24IkC9vOM7Uc6YoiqIoipKbUM+ZoiiKoih5grzS50yNM0VRFEVR8gR5ZbSmNmsqiqIoiqLkItRzpiiKoihKniCvNGuq50xRFEVRFCUXoZ4zRVEURVEue/JSKA01zhRFURRFyQOINmsqiqIoiqIo3kc9Z4qiKIqiXP6IhtJQLoJFCxdQP6w2YXVqMO6lFzPsT0hI4OYB/QirU4OWzZuwZ/fu1H3jxr5AWJ0a1A+rzeJFCy8LXdX2nXaDunWoF1KT8eMy1x50U3/qhdSkdYumqdqHDx+m4w3tKBtwNfePGpFtXYBvly6i/TUNuK5pPd59fXyG/at/+p7u1zcnJLg4C2JnZdh/8sRxWjasydP/d3+2tX1Z54sXLaBhvRDCQ2vx8rixmWoPvrk/4aG1aNuyWar20iWLadmsEU0iw2nZrBErli3NtvaSRQtoFB5KRN3avDo+c+1hA28kom5trmvVjL17dnvs37dvLxXKlOD1CS9nW7teYDFe7Fybl7rUJjq0TIb9Lar683rPUJ7pWJNnOtakdfUAACqVLMwTN1RnTKdaPNexJo0rlci29qKFCwgPq0PdkJqMP8f1HjigP3VDatLqGs/7vMP17SjjfzX3XeB9vnTxQppFhNE4PITXXnkpU+1bhwygcXgIHdpe41HnmzdtpOO1LWnZOJzWTRty9uzZbGn/sHwJPdpF0rV1Az5+65UM+z//4A16XdeYvh2ac/uALsTv35u6L3b6l3Rr05BubRoSO/3LbOkql4Yr1jgTkSoiMuACjz15oboul4t7R95NTOzX/LJxC9Mmf8XWLVs80nzy0Yf4l/Rn87Yd3DPqPh579GEAtm7ZwrQpk1m3YTNz5i5g1D134XK5crWuavtO+/5RI5g1Zz5rN2xm2pTJbN3qqf3pxx9SsmRJft26nREj7+WJxx4BoHDhwjzx5DOMeXFclvXSaz/9f/fz/pezmP/tWubOmsaO37Z6pAkMrsiLE9+lc4++mZ5jwthnaNT0mgvS9mWdPzDqHmbGzGP1+k1MnzqZbenq/LNPPqJkSX82bPmdu+8ZxejHbZ2XKl2aqTNiWLl2A+9+8DG3Dh+c7XI/eN9Ips2ey8/rfmXGtCkZtCd98hElSvqzbtNv3HnPvTz1+P957H/84f9x3Q0dsqUL1lMxKCqYl5ft4v/m/U7TyiUJKl4oQ7pVe48y+uvtjP56Oyt2HgEgwZXMez/t49H5vzN++S5uigyiSIGsv5ZcLhf3jRrB7Nj5rEu5z9Nf748/pKR/STZt3c49I+/l8UfT7vPRTz3DmLEXfp8//MAovpoRy/erNzBz+hR+2+ap/cVnH1OipD+rNmzl9rtH8uyTjwKQlJTEXbcOYdyEN/hu1QZmzVtCgQIFsqU9dvQDvP7JdGYsXsWCOTP4Y/s2jzS1Q+vzeexypi74kes6dmPiC6MBOHb0CO9NfJHPZn/DpJilvDfxRY4f++eC6iA3Il5efMUVa5wBVYBMjTMRybHm3tWrVlG9eg2qVqtGwYIF6dOvP3NjYzzSzI2N4aaB9uHcs1dvli/9BmMMc2Nj6NOvP4UKFaJK1apUr16D1atW5Wpd1faN9prVq6jmpt27b79MtOekavfo2Zvly6x20aJFaX5NCwoVLpxlPXc2/rKGylWrUalyVQoWLEh0994sWTjXI02FSpWpE1qPfPkyPoI2bfiFv//6ixatr822tu/rvHqqdq8+/ZgbO8cjzbzYGAbcPAiA7j17s3zZUowxhDdoSGBQEAAhoWGcPXOGhISELGuvXWO1q1S12j1792X+XE/tr+fN4cabBwLQrUcvViy32gDz5sRQqUoV6oSEZlkzhWqlinDoZCJ/nUrElWxYuecoERWKZ+nYQycSOXQiEYCjZ5I4fjaJqwtn/fG7ZrXn9c7sPp8XO4ebU+7zXhnv88IXeJ+vW7OaqtXS6rxHr74smBfrkWbBvFj63WjrvEv3Xny3fBnGGJZ/s5jQsHrUrRcOQECpUuTPnz/L2pvWr6VC5WpUqFSVAgUL0r5LT5YvmueRplHzVlx1VREA6jVsxJ8H4wH4acVSmrRoS4mSARQv4U+TFm35cfk3F1QHSs5x2Rlnjsdrq4i8LyKbRWSRiFwlItVFZIGIrBWR70SkjpP+ExHp7XZ8itfrRaCliKwXkftEZIiIzBGRpcA3IlJMRL4RkXUi8quIdPNG/uPj46hQoWLqenBwBeLi4jKmqWjT+Pn5UbxECQ4fPkxcXMZj4+M9j81tuqrtQ+2KFTyOP5CZdgU37eJW+2I5dCCe8kFp2uUDgzl04ECWjk1OTubFp/6PR54cc0HavqzzA/FxBHscH8yB+PTa8R51XiKTOo+ZNYPwBhEUKpTR+3Ru7XiCg9O0g4IrcCA+PoN2SpqU633k8GFOnjzJxFde4uFHR2dZzx3/qwpw5NS/qetHTv+Lf5GMXqCoiiV4rmNNRrSoREAm+6uVugq/fMKfjrGWFeLj4giu4Hmfp79mNk3G632xHDzgqR0YFJyhzt3T+Pn5cXXxEhw5cpidO7YjIvTtHs21LRvz+oSMTf//xV+H4ikfFJy6XjYwmD8Pnfs3NnvqJK5pcz0Afx7y/H2WCwzmz0Px5zr0ssKG0hCvLr7ich0QUBO40Rhzq4hMBXoBQ4E7jDHbRaQJ8BbQ7j/O8QjwP2NMZwARGQJEAPWNMUcc71kPY8xxESkN/Cwic0zKp2YmiMhtwG0AFStVuvhSKsoVxhcfv0fra2/wePFcSWzdspnRj/0fs+cuuGSaY59/mjvvuZdixYrlmMYvccf5ec9RkpINbWoEcGvTioxd+kfq/hKF/bitWSXe/2kf53zA5iGSXEms+vlHFi7/kauuKkKvLu0JbxBBqzb/9cq6MObNmsKWjb/wwZT5Xj93biSPjAe4/DxnDruMMeudv9dimyibA9NEZD3wLhB4AeddbIw54vwtwBgR2QgsAYKBcv91sDHmPWNMlDEmqkzpjJ1iAYKCgtm/f1/qelzcfoKDgzOm2WfTJCUlcfzYMUqVKkVwcMZjg7L4EvOVrmr7UHvffo/jAzPT3u+mfdxqXyzlAoM4GJ+mffBAHOUCs/ZzXL92JZ9//C5to0J48ZnHmD3tS8Y990SWtX1Z54FBwcR5HB9HYFB67SCPOj/mVudx+/dzY99evPvhJ1SrXj3LulY7iLi4NO34uP2pzaTu2ilpUq53QKlSrFm9iicfe4T6darz9puv8cq4F3nv7TezrP3PmX8JKJrmCQsoUoB/Tv/rkeZUooukZGt2rdh5hCoBV6XuK+yXj/vbVGX6hoPsPHw664UGgoKDidvveZ+nv2Y2TcbrfbGUD/TUPhAfl6HO3dMkJSVx4vgxAgJKERQUTNPmLShVqjRFihThuhs6sHHDL1nWLlMuiINuHsI/D8RRtlzG39jK75fx4RvjmfDBZAo6ntiy5Tx/n4cOxFG2XFCGYxXfcrkaZ+6dMVxAAHDUGNPAbQlx9ifhlFNE8gEF/+O8p9z+vgkoA0QaYxoAh4AL65zgRlSjRuzYsZ3du3aRmJjItCmTie7c1SNNdOeufDHpUwBmzphO67btEBGiO3dl2pTJJCQksHvXLnbs2E6jxo1zta5q+0Y7MqoRO920p0+dkol2l1TtWTOn07qN1b5Y6jWIZPcfO9m3ZzeJiYnMmz2da2+IztKxL7/1MSvW/sayNVt5ZPTzdO8zgAcffzbL2r6v8x2p2jOmTSG6cxePNJ06d+XLzz8DYPbM6bRu0xYR4ejRo/Tu0YWnnxtDs+bZHwgREWm19+y22jOnT6VjtKd2h05d+OrzSYBtOm3V2mp/vWQFG7ftZOO2ndx590juf/ARbrvz7ixr7zp8mnJXF6R00QLkzyc0qVySX+KOe6Qp4daPLCK4OPHH7cjE/PmEka0q88Ouf1iz71i2yx0Z5Xm9M7vPO3Xuwucp9/kM793nDSOj+OOPtDqfNWMq7Tt19kjTvlNnpnxl6zx29gxatG6DiND22hvYumUTp0+fJikpiR9/+I7atUMyk8mUsPAI9u3eSdy+3fybmMjC2Jm0vr6TR5ptmzbw/KP3MuGDyQS4OQuatW7Hz98t5fixfzh+7B9+/m4pzVp732PnM/LIiIDLtVkzPceBXSLSxxgzTewvr74xZgOwG4gEpgJdgZRPvBPA1f9xzhLAn8aYf0WkLVDZGxn18/Pj1Ylv0CW6PS6Xi8FDhhEaFsYzT40mIjKKzl26MmTYcIYNGUhYnRr4+wcw6YvJAISGhdGrT18a1g/Fz8+PCa+9meVOpL7SVW3fab884XW6de6Ay+Vi0JChhIaG8ezTo4mIiCK6S1cGDx3OLUMHUS+kJv4BAXw66avU40NqVeXE8eMkJiYSGxvDnHkLCcliZ3E/Pz9Gj3mZ4Td2w+Vy0fvGQdSsE8rEsc9St0EE17aPZuMva7l7WH+OHz3KssVf89q455n/7Zosl++/tH1Z5+MnvEb3Lh1JdrkYOHgoIaFhPPf0kzSMjCS6c1cGDRnGrcMGER5aC/+AAD7+zIYxeO/tN/lj5w7GjnmOsWOeAyBm7gLKlC2bZe2XXplIr66dcLlc3DRoCCGhYYx55kkaRETRqXMXBg4Zxh3DBxNRtzb+/v58+Jl3QigkG5i0Jp4H21Yjn8C3f/xD3LEEetQrx+4jZ/gl7jg31C5Nw+DiuIzhVKKLD362npsmlUpQu2wxihXyo0U1fwA++Gkfe49mLayEn58fr0x4na7RHXAluxg0eGjG6z10OMOHDKJuSE38/QP47PO0+7xOTbf7fE4MsfMWEhKa9fv8xXET6NcjGpcrmQEDB1MnJIwXn3uKBhGRdOjUhZsGDeXu24bQODwEf39/3v34cwBK+vtzx92jaN+mGSLCtTd04PoOnc6j6Kn98DPjuXtQT5JdLrr2vZnqtUJ4+5XnCa3XkNbXd2LCC09w+vQpHrrLDoYoH1yBCR9MpkTJAG4Z+RA3d20LwK0jH6ZEyYAsayuXBvmPLlS5EhGpAsw1xtR11v8HFAM+Bd7GNmcWACYbY54RkXJADHAVsAC42xhTTEQKAAuBUsAnwD9AlDFmhHPe0kCsc+41QFOgozFmt4icNMb8ZweNyMgo88PKi3/ZKMr5SE723W847p8zPtOuWKqIz7STXMm+0/bh9R41a7PPtN/pU89n2ifPJvlMe/8R3/3GIqqUWGuMifJZBrJJSL2G5pPZy716zqY1SvqkDi47z5kxZjdQ123dfZhLhiA9xphDWMMqhYed7f+SccDAJ27H/Q00O0cecq7nrKIoiqIoF4TOEKAoiqIoiqJ4ncvOc6YoiqIoipIZecRxpp4zRVEURVGU3IR6zhRFURRFyRvkEdeZGmeKoiiKolz22NBkecM602ZNRVEURVGUXIR6zhRFURRFufwRDaWhKIqiKIqi5ADqOVMURVEUJU+QRxxnapwpiqIoipJHyCPWmRpninKZky+f755GWw4d95l2kP9VPtP2JUku382tOTQy2GfapxJcPtMuXCC/z7RPJ/qu3ErWEJEOwEQgP/CBMebFdPvvB24BkoC/gGHGmD3/dU7tc6YoiqIoSh5AvP7vvIoi+YE3gY5AKHCjiISmS/YLEGWMqQ9MB14633nVOFMURVEURbkwGgM7jDF/GGMSgclAN/cExphlxpjTzurPQIXznVSbNRVFURRFyRPkQCiN0iKyxm39PWPMe27rwcA+t/X9QJP/ON9w4OvziapxpiiKoiiKkjl/G2OivHEiEbkZiAJany+tGmeKoiiKolz2CD4ZrBkHVHRbr+Bs80BErgMeA1obYxLOd1Ltc6YoiqIoSt5AvLycn9VATRGpKiIFgf7AHI8siTQE3gW6GmP+zMpJ1ThTFEVRFEW5AIwxScAIYCGwFZhqjNksIs+ISFcn2TigGDBNRNaLyJxznC4VbdZUFEVRFCVPkJXwF97GGDMfmJ9u22i3v6/L7jnVc6YoiqIoipKLUM+ZoiiKoih5ghwIpeET1HPmAxYtXED9sNqE1anBuJdezLA/ISGBmwf0I6xODVo2b8Ke3btT940b+wJhdWpQP6w2ixctvCx0VfvK0173w1Lu6tqCOzo3Y8aHr2fYv2Dqp4zs1ZZ7+17H/w3uyr6dvwFw/OgRHh/ei/5Nq/PemEezrQuweOECGtatQ/2Qmrw8LvNyD7qpP/VDatKmRdPUch8+fJiON7SjXMDV3D9qxIVpL1pAw3ohhIfW4uVxYzPVHnxzf8JDa9G2ZbNU7aVLFtOyWSOaRIbTslkjVixbmm3tbxYvpGnDMBqF12HiyxkDkCckJHDL4AE0Cq9D+7bN2btnd+q+zZs20rFdC1o0CqdVkwacPXs2W9qrvvuGQR2acNMNjfjyvYkZ9k/9+C2GRDdneNdW3D+kBwfj0sJCHYrfz4PDejO4UzOGRDfn4P692dL2ZbkXL1pARP0QwsNq8co5rveQm/sTHuZcb0d7zepVXNMkgmuaRNC8cUNiY2ZlSxfg52+XcGP7xvS7LpJJ707IsH/yR29yc8emDO7SglGDunvUeas6pRnStRVDurbi4TsGZFs7N3PpxwPkDGqcASJy0vk/SESmZyH9OBHZLCLjsqvlcrm4d+TdxMR+zS8btzBt8lds3bLFI80nH32If0l/Nm/bwT2j7uOxRx8GYOuWLUybMpl1GzYzZ+4CRt1zFy5X1uZd85Wual+Z2u+OeZTRb33B67NW8N2C2anGVwqtOvXktRnLmDB1CT2G3s1H458CoGDBwgy4+yGG3D86kzNnTfv+USOYOWc+azZsZtqUyWzd6lnuTz/+kJIlS7Jx63buHnkvTzz2CACFCxfmiSef4fkXs/2zTtV+YNQ9zIyZx+r1m5g+dTLb0ml/9slHlCzpz4Ytv3P3PaMY/bjVLlW6NFNnxLBy7Qbe/eBjbh0+ONvajzwwkskzY/lh9UZmTZ/Mb9s8tb/47CNKlizJ6g3buOPuUTwz2hq/SUlJ3HXLYMZNfJPvV29g9vxvKFCgQLa0Jz7zMC++P4VP5v7AN/NmsnuH5/WuGVKPd6Yv4cM539K6fRfeda43wAsP30W/4SP4dP5PvD11ESVLlb5syv3AvfcwI2Yeq3/ZxPRp57je/v5s2Gyv95POvRYaVpcVP6zih5XrmBkzn1H33ElSUlK2tF95+iHGvz+Vz+f/xJK5M9i1Y5tHmlqh9flg5lI+jf2eNh268tZLT6buK1T4Kj6Z8y2fzPmWse98mWVd5dJxxRhnYvnP8hpj4o0xvbNwutuA+saYB7Obj9WrVlG9eg2qVqtGwYIF6dOvP3NjYzzSzI2N4aaB9uHcs1dvli/9BmMMc2Nj6NOvP4UKFaJK1apUr16D1atW5Wpd1b7ytLdv+oXAilUoX6EyBQoUpEWHbqxc7ul9K1Ls6tS/z545jThtEYWLFCE0ogkFChXOsp47a1avoppbuXv37ce8dOWeFzsntdw9evZm+TJb7qJFi9L8mhYULnwx2tVTtXv16cfcWM9BWfNiYxhw8yAAuvfszfJlSzHGEN6gIYFBQQCEhIZx9swZEhLOGwoplXVrVlGlWnWqVLXa3Xv14+u5sR5pvp4XS78BAwHo0r0X3y232su+WUxo3XrUrRcOQECpUuTPn/WJvrdtXEdQpaoEVaxCgYIFadepBz984xkAvWHTlhS+qggAoeFR/HXwAAC7d/yGy+Ui6po2AFxVtFhqutxe7tTrXTXtes+bm+56z43hxpvcrrejXaRIEfz8bK+iswlnU+//rLJ141oqVK5KcCVb59dF9+T7JZ51HuFW52ENovjrUHy2NC5LvO0286HrLE8bZyJSRUR+E5HPgE3AEyKyWkQ2isjT50i/yfk7v+MhS0l/u7N9DnZI7FoR6ZfdPMXHx1GhQlq8uuDgCsTFxWVMU9Gm8fPzo3iJEhw+fJi4uIzHxsdniHWXq3RV+8rTPvLnQUqXD05dL1U2kCOHDmZIN3/yx9we3ZRPX32OWx5+Lsvn/y9smdKmrQsOrkB8ZuWukFbuEsVtuS+WA/FxBHvUWzAH4tNrx59XO2bWDMIbRFCoUKGsax+IJzg4rdxBwcEcOOCpfTA+PjV/Kdf7yOHD7NzxOyJCn+6daNeiEa+/Oj7LugB/HzpA2cCg1PUy5YP4+9CBc6afP/0LmrS6FoD9u3dS7OrijL5nMLf2aMs7Lz2ZLS+tL8t9IN1vLCg4OMO9diDd9S5e3GoDrF61ksYR9WgWFc6E195KNdaywl+HDlDW7TdWpnwQf/1Hnc+d9jlNWqUNGExMOMvwnu24rc/1fLt4XpZ1lUvHlTAgoCYwGCgO9MZOUirAHBFpZYz59hzHDQeOGWMaiUgh4AcRWWSM6SoiJ40xDdIfICK3Yb1qVKxUKQeKoih5h079h9Kp/1BWzJ/JtPcnMOq513ydJZ+zdctmRj/2f8yeu+CSabqSXKz86UcWLf+Jq4oUoVfnGwhvGEGrNu28rrV4zlR+27yeCZPmONpJ/Lr2Z96btYxygRV4+r5bWDDrK6J73+x17fRcynJnRqPGTVi17ld+27aV228ZyvXtO16w1/a/WBgzlW2bfuGNL+ambpu+bANlygcRt3c3owZ3o3rtUIIrVfW6ti/wRSiNnCBPe84c9hhjfgZucJZfgHVAHazhdi5uAAaJyHpgJVDqPOkxxrxnjIkyxkSVKV0m0zRBQcHs35/WMTMubj/BwcEZ0+yzaZKSkjh+7BilSpUiODjjsUFBnseeC1/pqvaVpx1Qtjx/H0zzIBz+8wAB5cqfM33LDt1Zucw7xogt0/7U9bi4/QRlVu79aeU+dtyW+2IJDAomzqPe4ggMSq8ddE7tuP37ubFvL9798BOqVa+ePe3AIOLi0sodHxdHYKCndvmgoNT8pVzvgFKlCAoOpmnzFpQqXZoiRYpwXfuObFz/S5a1S5cL5M8DaU1mfx2Mp3S5wAzp1v64gs/feZXn3/qcggWtV7BM+SCq16lLUMUq5Pfzo8V1ndi+ZeNlUe7AdL+x+Li4DPdaYLrrffy41Xandp0QihUrxpbNm7KsXaZcIH+6/cb+OhhPmUzqfPUPy/ns7ZcZ+86XqXUOtt4BgitVoWHjFvyejTrPzQh2tKY3F19xJRhnp5z/BXjBGNPAWWoYYz78j+MEuMctfVVjzKKLzUxUo0bs2LGd3bt2kZiYyLQpk4nu3NUjTXTnrnwx6VMAZs6YTuu27RARojt3ZdqUySQkJLB71y527NhOo8aNc7Wual952jXDGnBg7y4O7d/Lv/8m8v2CGBq3bu+RJn7PH6l/r/l2CYFe+mqPjGrETrdyT586hU7pyt2pc5fUcs+aOZ3Wbdplu8/PubV3pGrPmDaF6M5d0ml35cvPPwNg9szptG7TFhHh6NGj9O7RhaefG0Oz5tdkW7thZCN27dzBnt1We/aMKXSI7uyRpkOnzkz5chIAsbNn0KK11W577Q1s3bKJ06dPk5SUxI/ff0utOiFZ1q5TryFxe/7gwP49/JuYyNL5s2jeroNHmu1bNvLKkw/w/Fuf418q7cO1dr2GnDxxnKNH/gbgl5+/o3L12pdFuSOjGvHHjh3s3p12vTtFp7ve0V356gu36+1o7969K3UAwN49e/j9t21Urlwly9p16kWwb/cfxO+zdb5k3kyuudazzn/fspFxo+/nxXe+9Kjz48eOkpho+zMePXKYX9etpEqNrNe5cmm4Epo1U1gIPCsiXxhjTopIMPDvf8xztRC4U0SWGmP+FZFaQJwx5tQ50mcJPz8/Xp34Bl2i2+NyuRg8ZBihYWE889RoIiKj6NylK0OGDWfYkIGE1amBv38Ak76YDEBoWBi9+vSlYf1Q/Pz8mPDam1nuwOorXdW+8rTz+/lx6/+N4ek7b8SV7OK67v2pVKM2X775EjXCwmncpj3zJ3/Ehp+/I3+BAhS7ugSjnk1r0ry1YyPOnDxJ0r+JrFy2gKfe+YqKWXxh+/n58fKE1+neuQMul4uBQ4YSGhrGs0+PJiIiiuguXRk8dDi3DB1E/ZCa+AcE8Mmkr1KPD61VlRPHj5OYmMjc2Bhi5i0kJCQ0y9rjJ7xG9y4dSXa5GDh4KCGhYTz39JM0jIwkunNXBg0Zxq3DBhEeWgv/gAA+/syOlHvv7Tf5Y+cOxo55jrFjbP+7mLkLKFO2bJa1Xxg/kb7do0lOdnHjwCHUCQnjxeeeokHDSDpEd+GmQcO469YhNAqvg7+/P+99/AUAJf39uXPEvdzQuhkiwnU3dOCGDp2ypAv2eo984kUeGt6H5ORkOvYaQNWadfjotReoXbcB17TryDvjnuLM6VM8de9wAMoFBvP821+QP39+7nzoaR4Y0hNjDLXCwuncZ2CWtX1Zbj8/P8a9+ho9unS091rK9X7mSSIiIunkXO/bhg0iPKwW/v4BfDzJXu+ffvyeV8e/RIECBciXLx+vTHyDUqWzPkrVz8+P+0e/xP3De5PschHd+yaq1Qzhg4ljqFO3IS2u7cibY5/kzOlTPDFyqK3zoAqMfedL9uz8jXGj70ckH8Ykc/Nto6hao06WtXM7eaNRE8QY4+s85BgiUgWYa4yp66yPAm5xdp8EbjbG7HT6kBVzT++M7HwO6IK93n8B3Y0xx1LS/5d2ZGSU+WHlmpwpmKLkEhZuydjR/1JxXZ1yPtP25XPz7L/JPtPeFHfMZ9p1g0v4TLuQn+8amTbu812dt6gVsNYYE+WzDGSTuuERZtrX33n1nKHBxXxSB3nac2aM2Q3UdVufCGSIkJhiaLmnN8YkA486S6bpFUVRFEXJReQR11meNs4URVEURbly0NGaiqIoiqIoitdRz5miKIqiKHkCnfhcURRFURRF8TrqOVMURVEUJU+QRxxn6jlTFEVRFEXJTajnTFEURVGUvEEecZ2pcaYoiqIoymWPoKE0FEVRFEVRlBxAPWeKoiiKolz+SN4JpaHGWQ5hgORk38y/ly9fHrk7LyN8da3Bt9e7ba2sTcydE/jyNk/y3fSWFC7guwaPkMDiPtMuVth3r6smz37jM+1pdzbzmbbiO9Q4UxRFURQlT5BXXBNqnCmKoiiKkjfII9aZDghQFEVRFEXJRajnTFEURVGUPIBoKA1FURRFURTF+6jnTFEURVGUPIGG0lAURVEURcklCHlmPIA2ayqKoiiKouQm1HOmKIqiKEreII+4ztRzpiiKoiiKkotQ4+wSs2jhAhrUrUO9kJqMH/dihv0JCQkMuqk/9UJq0rpFU/bs3g3A4cOH6XhDO8oGXM39o0ZcsHb9sNqE1anBuJcy1755QD/C6tSgZfMmqdoA48a+QFidGtQPq83iRQtVOxvaV+L1XrJoAZH1Q2gQVotXxo3NVHvIzf1pEFaLdi2bsWeP1V67ehUtmkTQokkE1zRuSGzMrGxrL1q4gPCwOtQNqcn4c5R74ID+1A2pSatrPOu8w/XtKON/NfddYJ0vXrSAiPohhJ+n3OFhtWjrVu41q1dxTZMIrmkSQfMLLPfiRQtoWC+E8NBavHwO7cE39yc81NF2yr10yWJaNmtEk8hwWjZrxIplS7OtvXTJQq6JDKNpgxBef+WlTLVvGzKApg1C6NjuGvY65Z4x9UuubRGVugSWLMSmjeuzpe3L+7x5jQBi7mlK7MhmDGtROdM0N4SVZebdTZl5dxNe6BWWuv3e62sw8+4mzBrRlIc71sq29rdLF9G+RQOub1aP914fn2H/6p++p8f1zQmtUJwFcz3vp5Dgq+l2XVO6XdeUOwb3ybZ2bka8/M9nGGPy3AJUATb5Mg8NIyLNqYRkj+X46X9N1arVzKatO8w/J86auvXqmzXrN3mkeXXiG2b4LbeZUwnJ5pNJX5pevfuaUwnJ5s8jJ8zipd+aia+/ZW6/464M53ZfzvxrMiwnzyaZqtWqmS2/7TTHTiWYevXqm3UbNnukmfDam+aWW283Z/415tPPvzK9+vQ1Z/41Zt2GzaZevfrm6MmzZuvvf5iq1aqZk2eTMtW5UrUzuw5XwvU+dsaVYTlyMtFUqVrNrN+y3fx17IypW6++WbnuV4804ye8YYbecps5dsZlPvz0C9OjVx9z7IzLHDh8whw+kWCOnXGZ3/7Yb0qXKZO6nn45nZicYTlx5l9TtVo1s3nbDnP05FlTr159s3b9Jo80r772hhl+623mdGKy+dSp89OJyeavf06YJcu+NRPfeMvcfuddmZ4/ZTl+xpVh+ccp94Yt283fTrlXrfvVI83LE94ww265zRw/4zIfffqF6dmrjzl+xmUOHj5hjpxIMMfPuMzvTrlT1tMvJ85mXI6eSjRVq1YzG7dsN4ePW+3Vv/zqkeaViVb7xFmX+fizL0zP3n3MibMu8/3Pa8zvf+wzJ866zMq1G0xgUFCmGifOuszBY4kZlrgjZ0zlKtXMyvXbzN6/TprQuvXMipXrPdK8MP41M2jorebgsUTzzoeTTNcevTOcZ9mPa03lKtUy1Th4LNGn93n90UsyLA2eXGL2Hj5lOr76vYl4+huz7cBx0/31nzzSdJ7wg9kaf9xcM2a5qT96iWkzdoWpP3qJGfj+arNuzz+mwZP2POv3HjXDPlqTqc5vB05lWLbsP24qVq5qlvy8yfy65x9TO7Sumbd8jUeab1ZtMTHf/Gy69b7RTHz/c499RYoUzfS86Rdgja/f59lZ6oVHmD2Hz3p18VUdqOfsErJm9SqqVa9B1WrVKFiwIL379mNubIxHmrmxc7hp4GAAevTszfJl32CMoWjRojS/pgWFChe+IO3Vq1ZR3U27T7/+mWjHpGr37NWb5Uut9tzYGPr060+hQoWoUrUq1avXYPWqVap9Hq7U67129SqqVa9O1apWu2effsybO8cjzfy5MQy4aRAA3Xv2ZsXypRhjKFKkCH5+tivs2YSzSDbHxa9Z7VnuzOp8Xuwcbk6p814Z67zwBdb5mnTl7pVJuefNjeFGt3Iv92K5q1WvnlruXn36MTc2nXZsDANudtNeZrXDGzQkMCgIgJDQMM6eOUNCQkKWtX9Zu5qq1apT2Sl39559WTgv1iPNwvmx9B0wEIDO3Xvx/YplKR/SqcyaPoXuvbLnxfHlfV43uDj7jpwh7p+zJLkMCzYdok2d0h5pekYGM3nVfk6cTQLgyKl/AesUKeSXjwL581HQLx9++YTDJxOzrL3xlzVUrlKNipWrUrBgQaK79eabhXM90lSoWJk6ofXIl09f85cjufqqiUhREZknIhtEZJOI9BOR0SKy2ll/T5ynmIhEOuk2AHe7nWOIiMwUkQUisl1EXnLbd4OI/CQi60RkmogUc7a/KCJbRGSjiIx3tvVxNDeIyLcXUp74+DgqVKyQuh4cXIEDcXEZ01SoCICfnx/Fi5fg8OHDFyJ3zvOmaMdlpl3RTbuE1Y6Ly3hsfLznsap9Du0r9HoHexwfnKHcB+LjU9OklPuIU+41q1bSJKIezaPCefW1t1KNlixpx8URXMGzztPn3abJWO6L5UC6Og8KDiY+k3Knv94p5V69aiWNI+rRLCqcCdks94HM6jx9udNpl8jkXouZNYPwBhEUKlQoW9pBwWl1HhgczIED8Z5pDqSl8fPz4+riJThyJJ32zOl0790vy7q2TL67z8sWL8zBY2dT1/88lkC5qz3rrXKpIlQuVYRPhkcy6ZYomtcIAGDj/uOs3vUPS/7XgiX/a8mPOw+z6+/TWdY+dDCe8m51Xi4wmEMHD2T5+ISEs/Rs34K+0W1Y8nXs+Q+4jBAvL74it4/W7ADEG2OiAUSkBLDYGPOMsz4J6AzEAh8DI4wx34rIuHTnaQA0BBKA30TkdeAM8DhwnTHmlIg8DNwvIm8CPYA6xhgjIiWdc4wG2htj4ty2eSAitwG3AVSsVMkb5VeUK46oxk1Yue5Xftu2lTtuGcr17TtesDfrcqJR4yascsp9uw/KvXXLZkY/9n/MnrvgkmmmsG7NKq4qchUhoXUvuXZO4pdPqFzqKm75eB3lihfio2GR9H5rJSWLFKBqmaLc8MoPALw7qCENKx3hl71HL0m+lq3eRrnAIPbt2cXg3p2oFRJGpSrVLom2kjVytecM+BW4XkTGikhLY8wxoK2IrBSRX4F2QJhjLJU0xqR4tCalO883xphjxpizwBagMtAUCAV+EJH1wGBn+zHgLPChiPQEUj5nfgA+EZFbgfyZZdYY854xJsoYE1W6dJkM+4OCgtm/b3/qelzcfgKDgzOm2b8PgKSkJI4fP0apUqXOW1Hnw/28KdrBmWnvc9M+ZrWDgzMeGxTkeaxqn0P7Cr3ecR7Hx2Uod2BQUGqalHIHpCt37TohFC1WjC2bN2VdOziYuP2edZ4+7zZNxnJfLIHp6jw+Lo6gTMqd/npnVu5i2Sx3YGZ1nr7c6bSPud1rcfv3c2PfXrz74SdUq149y7op2vFxaXV+IC6OwMAgzzSBaWmSkpI4cfwYAQFp5Z49Yyo9emXPa2bL5Lv7/M/jZylfIs14LluiEIdOeDYHHzp+luXb/iYp2RB39Cx7Dp+mUsBVtAspw6/7j3Em0cWZRBc/bD9MeMXiWdYuVz6Ig251fuhAHOXKB2b9eOf6VKxclcbNW7Jl04YsH5urETtDgDcXX5GrjTNjzO9ABNZIe05ERgNvAb2NMfWA94GsfFq6/2JcWI+hYL1wDZwl1Bgz3BiTBDQGpmO9cgucvNyB9bRVBNaKSLaf5pFRjdi5Yzu7d+0iMTGR6VOnEN25q0ea6M5d+GLSpwDMmjmd1m3aZbv/SWZENWrEDjftaVMmZ6LdNVV75ozptG5rtaM7d2XalMkkJCSwe9cuduzYTqPGjVX7PFyp1zsiqhE7d+xg926rPXPaFDpFd/FI0ym6K19+8RkAs2dOp1XrtogIu3fvIinJ9s/Zu2cP23/bRuXKVbKsHRnlWe7M6rxT5y58nlLnM7xX55FRjfjDrdwzzlHur9zK3foc5f79Asq9c8eO1HLPmDaF6M7ptDt35cvP3bTbWO2jR4/Su0cXnn5uDM2aX5PtcjeIiOKPnTvY45R79syp3NCps0eaGzp1ZuqX9pt57uwZXNOqTWqdJycnM2fWdLr36pttbV/e55vjT1ApoAjBJQvjl1/oULccK7b97ZFm6ba/iKrqD0DJIgWoXKoI+/85w8GjZ4ms7E/+fIJfPiGySkl2/ZX1Zs16DSLZvWsn+/buJjExkXkx02nXPjpLxx47+g+JTp/CI4f/Zt3qn6lRs06WtZVLQ65u1hSRIOCIMeZzETkK3OLs+tvpH9YbmG6MOSoiR0WkhTHme+CmLJz+Z+BNEalhjNkhIkWBYCAeKGKMmS8iPwB/OHmpboxZCawUkY5YIy1bHVX8/Px4ecLrdOvcAZfLxaAhQwkNDePZp0cTERFFdJeuDB46nFuGDqJeSE38AwL4dNJXqceH1KrKiePHSUxMJDY2hjnzFhISEppl7VcnvkGX6Pa4XC4GDxlGaFgYzzw1mojIKDp36cqQYcMZNmQgYXVq4O8fwKQvJgMQGhZGrz59aVg/FD8/Pya89ib582fqPFTtdNpX6vUe/+pr9OzSEZfLxc2DhxISGsbzzzxJw4hIOnXuysAhw7ht2CAahNXC3z+AjyZ9CcDPP37Pq+NfokCBAki+fLw88Q1KlS59HkVP7VcmvE7X6A64kl0MGjw0Y7mHDmf4kEHUDamJv38An32eVud1arrV+ZwYYuctJCQ063U+7tXX6OGUe6BT7ueeeZIIp9yDnHKHO+X+2Cn3T27lzpcvH69cQLnHT3iN7l06kuyu/fSTNIyMJNrRvnXYIMJDa+EfEMDHn1nt995+kz927mDsmOcYO+Y5AGLmLqBM2bJZ1h4zfgI39ozG5UrmxpsHUyckjLHPP0WDhpG079SFAQOHMuK2ITRtEEJJf3/e/ejz1ON/+uE7goIrULlq9pvVfHmfu5INL8z/jbcHNiRfPpj9ywF2/nWKu9pWY3P8cVb89jc/7jhC8+qlmHl3U5KN4dVFOzh2JonFW/6kcbUApt/VBGPgxx2HWfH73+cXdSv36DEvc8uN3XC5XPTqP4iatUOZ+NKz1A2P4Nr20Wxcv5YRw/pz/OhRli3+mtfHPc+8FWvYuf03nnzoHiRfPkxyMreOeIAatUOyXfe5l7wRhVbSj5jJTYhIe2AckAz8C9wJdAduBA4CvwN7jDFPiUgk8BFggEVAJ2NMXREZAkQZY0Y455wLjDfGLBeRdsBYIKUX5+PAaiAG65ETJ+2nIjITqOls+wa41/xH5UVERpnvf1rttbrIDvny5Y2b83IiOdl3vyNfXu/EpGSfaRfI77tyJ7l8d7192dRyKsHlM+0SRQr4TLvJs9/4THvanc18pl07sOhaY0yUzzKQTcIbRpr5y37y6jkr+BfySR3kas+ZMWYhkD4y4BqsEZU+7Vog3G3TQ872T4BP3NJ1dvt7KdAoE+kMvm1jTM+s51xRFEVRFOXCyNXGmaIoiqIoSlbJK+1GuXpAgKIoiqIoypWGes4URVEURckT+LJPpjdR40xRFEVRlDyBTycr9yLarKkoiqIoipKLUM+ZoiiKoih5g7zhOFPPmaIoiqIoSm5CPWeKoiiKouQJ8ojjTI0zRVEURVEuf3w9Wbk30WZNRVEURVGUXIR6znIIQee4vJLw5deaL+f1PHr6X59pl7m6oM+0XT6ck9j4bjpT4v854zPt4lf57nXly/ktP9+w32falyMaSkNRFEVRFEXxOuo5UxRFURQlb5A3HGdqnCmKoiiKkjfII7aZNmsqiqIoiqLkJtRzpiiKoihKnkBDaSiKoiiKoiheRz1niqIoiqLkAURDaSiKoiiKoijeRz1niqIoiqJc9gja50xRFEVRFEXJAdQ48wGLFi6gflhtwurUYNxLL2bYn5CQwM0D+hFWpwYtmzdhz+7dqfvGjX2BsDo1qB9Wm8WLFl4Wule6dnhYHeqG1GT8ObQHDuhP3ZCatLqmaQbtuiE1CQ+rc8HaDerWoV5ITcaPy1x70E39qRdSk9Yt0rQPHz5MxxvaUTbgau4fNSLbugDLv1lE28b1aBUVylsTxmXYv/LH7+jUtinVyhZl3pyZHvsG9elCvarlGHpjjwvS9mWdL1m0gEbhoUTUrc2r48dmqj1s4I1E1K3Nda2asXfPbo/9+/btpUKZErw+4eUL0m7cIJTIerWZcC7tQTcSWa8217XOqL1/314qlr0w7R+WL6FHu0i6tm7Ax2+9kmH/5x+8Qa/rGtO3Q3NuH9CF+P17U/fFTv+Sbm0a0q1NQ2Knf5lt7Qu93ocPH6bD9e0o4381913gff7t0kW0b9GA65vV473Xx2fYv/qn7+lxfXNCKxRnwdxZHvtCgq+m23VN6XZdU+4Y3Cfb2jvWfMubw9vzxtDr+WHKe+dMt/X7hTzboTbxv//qsf3Yn/G82L0hP03/MNvaSs6jxtklxuVyce/Iu4mJ/ZpfNm5h2uSv2Lpli0eaTz76EP+S/mzetoN7Rt3HY48+DMDWLVuYNmUy6zZsZs7cBYy65y5cLleu1r3Ste8bNYLZsfNZt2Ez06ZMzqj98YeU9C/Jpq3buWfkvTz+6COp2tOnTmHt+k3EzP2ae0fenW3t+0eNYNac+axN0d7qqf3pxx9SsmRJft26nREj7+WJx6x24cKFeeLJZxjzYkajKqvaTzw0ik+nxrDkx/XMmTmV37dt9UgTVKEiL7/xPt169ctw/G0j7uPVtz+6YG1f1vmD941k2uy5/LzuV2ZMm8K2dHU+6ZOPKFHSn3WbfuPOe+7lqcf/z2P/4w//j+tu6HBB5X7o/pFMnTWXn9Zmrv35px9RsqQ/a3/9jTtH3MtTT3hqP/bI/7j2ArXHjn6A1z+ZzozFq1gwZwZ/bN/mkaZ2aH0+j13O1AU/cl3Hbkx8YTQAx44e4b2JL/LZ7G+YFLOU9ya+yPFj/2RL+0Kvd+HChRn91DOMGXvh9/kzj97PB1/MYt6KtcydPY0dv3ne54EVKvLCxHfp3KNvhuMLF76KmCU/E7PkZ975dFq2tJNdLha8+QwDnvuAO9+bx6blc/lrz44M6RJOn2TV7M8IrhOeYd+i916kRlTLbOleDoh4d/EVapxlgojk/6/1i2H1qlVUr16DqtWqUbBgQfr068/c2BiPNHNjY7hp4GAAevbqzfKl32CMYW5sDH369adQoUJUqVqV6tVrsHrVqlyteyVrr1ntqd27b78M2vNi53Czo92jV2+WL0vT7t23n4f2mtXZ0652Hu25sXNSy92jZ5p20aJFaX5NCwoVLpxlPXfWr1tNlarVqVTFanfp0YfFX8d6pKlYqQohYfXIly/jI6hF63YULVbsgrR9Wedr16yiWvXqVKlqtXv27sv8uXM80nw9bw433jwQgG49erFi+VKMM4n6vDkxVKpShTohodku99o1q6hazVP763Ta8+fOof9NadrfumvHxlC58oVpb1q/lgqVq1GhUlUKFCxI+y49Wb5onkeaRs1bcdVVRQCo17ARfx6MB+CnFUtp0qItJUoGULyEP01atOXH5d9kWftirnfKfV74Au/zjb+soXKValSsXJWCBQsS3a033yyc65GmQsXK1AnN/D6/GOJ/24h/YGX8AyuSv0BBwlpH89tPGett+WcTad7nVvwKFPLYvu3HJfiXC6ZM5ZpezZfiPfKccSYiD4rISOfvV0VkqfN3OxH5QkRuFJFfRWSTiIx1O+6kiLwsIhuAZunWHxOR2W5prxeRWVwA8fFxVKhQMXU9OLgCcXFxGdNUtGn8/PwoXqIEhw8fJi4u47Hx8Z7H5jbdK1o7Lo7gChX+83ibJqN2+nwHBQcTH5fNclf01D6QWbndtYtb7Yvl4IF4AoPTtAODgjl4IP6iz5sVfFnnB+LjCQ52P74CB+I9yx3vlialzo8cPszJkyeZ+MpLPPzo6KwXNr12hXTa6ercPU1m2g9doPZfh+IpHxScul42MJg/Dx04Z/rZUydxTZvrAfjzUDzlg9KuV7nAYP48lPV75WKu98Vy6GA85YM9837o4LnLnZ6EhLP0bN+CvtFtWJLu4+V8HD98iOJlyqeuFy9djhOHD3mkObB9M8f/OkjNJm08tieeOcWPU9+n1c0X1pSb2xEv//MVeXG05nfAA8BrQBRQSEQKAC2B34GxQCTwD7BIRLobY2YDRYGVxpgHAEQkdV1EBNgqImWMMX8BQ4EM7S4ichtwG0DFSpVytpSKouQZxj7/NHfecy/FLtBjeNHaIy6N9rxZU9iy8Rc+mDI/x7VyO8tWb6NcYBD79uxicO9O1AoJo1KVal45t0lOZvF7L9L1gRcy7Fvx+Rs06TmYglcV9YpWrsLHTZHeJC8aZ2uBSBEpDiQA67BGWksgFljuGFiIyBdAK2A24AJmuJ0ndd0YY0RkEnCziHwMNAMGpRc2xrwHvAcQGRllMstcUFAw+/fvS12Pi9tPcHBwxjT79lGhQgWSkpI4fuwYpUqVIjg447FBQZ7Hngtf6V7R2sHBxO3f/5/H2zQZtdPnOz4ujqDgbJZ7n6d2YGbl3r+P4BTt41b7YikfGMSBuDTtA/FxlA8MuujzZgVf1nlgUBBxce7H7ycwyLPcQU4a9zoPKFWKNatXETNrJk8+9gjHjh0lX758FCpUmNvuvDvr2vvTaaer85Q0wcGe2mvXrGLO7Jk89XiaduHChbn1jqxplykXxEE3b9WfB+IoWy4wQ7qV3y/jwzfG88GU+RQsZJvZypYLYs3P36WmOXQgjqimWe8HdTHX+2IpVz6Ig273+aEDcZQrn7Hc5zzeuT4VK1elcfOWbNm0IcvGWfFS5Tj+18HU9eN/H+LqUuVS1xPOnOLPPb/z2UP2NXXyn7+Y8tSd9HvqbeK2bWDrdwv55oPxnD11HJF8+BUsRKOuN2c570rOk+eaNY0x/wK7gCHAj1hPWlugBrD7Pw49a4xx/cf6x8DNwI3ANGNM0oXkL6pRI3bs2M7uXbtITExk2pTJRHfu6pEmunNXvpj0KQAzZ0ynddt2iAjRnbsybcpkEhIS2L1rFzt2bKdR48a5WvdK1o6M8tSePnVKBu1OnbvwuaM9a8Z0WrdJ054+dYqHdlSj7GnvPI92dOcuqeWeNTNN+2IJbxjFrj92sHeP1Y6dNY3rO3a+6PNmBV/WeURkI3bu2MGe3VZ75vSpdIzu4pGmQ6cufPX5JABiZs2gVeu2iAhfL1nBxm072bhtJ3fePZL7H3wky4ZZivYfOz21O6TT7hjdhclfpGm3dLTnL17Bhq072bB1J3fcPZL7/vdIlg0zgLDwCPbt3kncvt38m5jIwtiZtL6+k0eabZs28Pyj9zLhg8kElC6Tur1Z63b8/N1Sjh/7h+PH/uHn75bSrHW7LGtfzPW+WOo1iGT3rp3s27ubxMRE5sVMp1376Cwde+zoPyQmJABw5PDfrFv9MzVq1smydlDtehyJ380/B/fh+jeRzSvmUatpWr0VLno1/5u6kpGfLWXkZ0upUKcB/Z56m6Ba9Rjy8pep25t0H0yL/rfnGcNMcmDxFXnRcwbWIPsfMAz4FXgF61FbBbwmIqWxzZo3Aq9n5YTGmHgRiQceB6670Iz5+fnx6sQ36BLdHpfLxeAhwwgNC+OZp0YTERlF5y5dGTJsOMOGDCSsTg38/QOY9MVkAELDwujVpy8N64fi5+fHhNfeJH/+rI1V8JXula79yoTX6RrdAVeyi0GDh2bUHjqc4UMGUTekJv7+AXz2+Vep2j179yEiPAy//LYM2dV+ecLrdOvcAZfLxaAhQwkNDePZp0cTERFFdJeuDB46nFuGDqJeSE38AwL4dNJXqceH1KrKiePHrXEVG8OceQsJyWJncT8/P54ZO4FBfbrgcrnoO2AwteqE8vILT1O/QSTXd+zMhnVruG1QP44d+4clC+fz6ovPsuTHXwDoHd2Ondt/59SpkzSpW52XXnuH1u2uvyzq/KVXJtKraydcLhc3DRpCSGgYY555kgYRUXTq3IWBQ4Zxx/DBRNStjb+/Px9+lv3QEefUfnkivbul0372SRpGRNExugs3Dx7GHbcMJrKe1f7gU+9pP/zMeO4e1JNkl4uufW+meq0Q3n7leULrNaT19Z2Y8MITnD59iofush3zywdXYMIHkylRMoBbRj7EzV3bAnDryIcpUTIgW9oXer0B6tR0u8/nxBA7byEhoVm/z0ePeZlbbuyGy+WiV/9B1KwdysSXnqVueATXto9m4/q1jBjWn+NHj7Js8de8Pu555q1Yw87tv/HkQ/cg+fJhkpO5dcQD1KgdkuVy58vvR4e7RvPlY7dgkl2E39CLslVqsvyziQTWrEvtZtdm+VxK7kRSRuvkJUTkWmABUNIYc0pEfgfeMca8IiI3Ao9ijeJ5xpiHnWNOGmOKuZ3DY93Z1h+41xjT9Hx5iIyMMj+sXOPFUim5GV/+jnz5E/77ZKLPtMtcXdBn2glJyT7T9uX13v3XKZ9p1wm62mfa+w6f8Zn25xv2nz9RDvFsh9prjTFRPstANomIjDIrfsz6COusULxwfp/UQZ70nBljvgEKuK3Xcvv7K+CrTI4p9l/rDi2A972XU0VRFEVRvEVemfg8TxpnOYGIrAVOYUeCKoqiKIqi5AhqnGURY0ykr/OgKIqiKMq5ySuhNPLcaE1FURRFUZTLGfWcKYqiKIqSJ8gjjjM1zhRFURRFySPkEetMmzUVRVEURVFyEWqcKYqiKIqSJ/DFxOci0kFEfhORHSLySCb7C4nIFGf/ShGpcr5zqnGmKIqiKIpyAYhIfuBNoCMQCtwoIumnmRgO/GOMqQG8Cow933nVOFMURVEU5bJHsKE0vLlkgcbADmPMH8aYRGAy0C1dmm7Ap87f04Fr5TwTvKpxpiiKoiiKcmEEA/vc1vc72zJNY4xJAo4Bpf7rpDpaM4dYt27t31cVkD0XcYrSwN/eyo9qq7Zq5wpd1Vbty0m7srcycilYt27twqsKSGkvn7awiLhPlP2eMeY9L2tkQI2zHMIYU+ZijheRNb6acFa1VTuva1+JZVZt1c7rGGM6+EA2Dqjotl7B2ZZZmv0i4geUAA7/10m1WVNRFEVRFOXCWA3UFJGqIlIQ6A/MSZdmDjDY+bs3sNQYY/7rpOo5UxRFURRFuQCMMUkiMgJYCOQHPjLGbBaRZ4A1xpg5wIfAJBHZARzBGnD/iRpnuZccb9NWbdW+grWvxDKrtmorOYAxZj4wP9220W5/nwX6ZOecch7PmqIoiqIoinIJ0T5niqIoiqIouQg1zhRFUZQcIyXY5vmCbiqKkoYaZ4riI/SlpVwh1AUwxhhf3Osicr2IDLvUurkJfcZcfqhxlksRkfKXWE/+a/0S5qPQJdbzSblFRNyGUte9FJq5CTfDNL8P81BJRApcQr0r6gXpVt7JIjINLr2BJiK1gJeBmSIScKl0HW2fP1Od8nO+sA1K7kONs1yIiLQC5l7Kh0nKj1dEmolIGaDIpdJOQUTCgF7O3zk+ktjdQBKRuo6hcElGMLvpDgKmikixS2kYZmd7Tug7L+muwLuX2iB38lAO+B/gf4n03O+1fClGoYjk+DPY/bpeit9VCm4GQQOguoh8lrL9EhoqRYGrgEHAIyJS7FKIprvezeHSG0giUhR4SkSuv5S6indQ4yyXISJNgf8DHjTGHLkUD2837duBz4GPgVtEpPql0nYIxz5ACznzj+Uobg/Pu4DZ2LIPuFRGsYi0A+4GuhhjTmJj5OS0pvtLo6eIdBGR2nDpXpqOTifgaWCyMSbBB16Fo0Ad4PZLIeZW5yOBd4DPRKStMSb5EmoPB54UkTtz2jPv5hn1M8b8CzQBIi+1gWaM+QXYjvWeLTDGnLwUBqpbnd8NvCUilXJaM7NsALuBqj7QVi4SNc5yH4FAe+yLI0dJ90VdBqgHNMI+yIKBviJS7RLkoyCAMeZL4FvsV26OeXLSlbsCEAm0wsapCQduzAkDLRMPRhGgCk55nWCGOfrCcntpDADGAMOBESLSK2X/JTKU2gKPAZtFpBvwpYh0EJHCOakvIoEiUtUYkwDcg/Xo1MgpvXTatwFdsUZpWWDIpdB1tIcDQ4EYYBzQIwe13Jvsy4pIZcdAawg0zGkDLZNzLgJewd5jIc7v7FJ8CHXC1vkNxpi9IlLrErUI1BGRQGPMaWAGcL+IXDFTOOUV1DjLJYhIkIgUN8bMAvoC94lIdE59WafzoIwEHgAaGmOOGGOWYR9oAcBQEcmxLy8RiQRGisgdzqYfgRqQM80A6cp9C3ATEGCMiTfGTAJWAdWBYSLitSavdLolgELGmLnYh3eEiNwJl8Y4EpF+QDfsy7I/sA1oJSI9U/KQA5opnpQUQ+gEtuxTgTDgGNDPkc+R5h8RKY31Sn8oIjdhm7DPAOXc8+hFvfQGQBFgIPb3nQAMF5GCIlLWm7qOtvuHQEHstR4ChAA/Ae97WzMFt/v8AeAjbLP9/caYRCACqCcis9zTeot0v7PrROQ64CtjzMNYA22FiNQyxri8baBlcv8UBuYBTUXkWefvOc59mCOISCPgXuAb5/ecgP0Iq+js91kfTyWbGGN08fECdMcaQ58Bj2C/qnsD64DuOazdDWuQdAK2ADFu+zoBzwGlvKgnjmY+oDYwHhgGzMROcTEUOAT0yOFy9wF+dbRPAs+77RsMvAD454Du/cAs7FQfPZ1tHZ3y33eJ7rf/A1xAI2e9HLZ59UOgaw7qdsIa31WBgsC1QG1nXy2s0VDZy5opgbZLY5uNS2Cb2KYDTwB/A98BZbys6w/Ucyt3EPAq1hD+yi3dncDDQP4cqvO62H5XDwCLsU17KfseAbp5u66dv28DVjh/f+j8xkY76wWBH5w6EW9rO+v3OhqvAr8BjZ3tDzr3fo2cuM+cv3tjPfHlsc/1z4ForLE2FeiYQ9e6AfALUAr7TnnI0d/p3OMFckJXl5xZfJ6BK33BNiWudF4abwJLgKudff2ArUAZbz3E0mk3w/a1ustt28/ALLf1IjmgOwTYA/wOBLltvwXbBygeGOtsy5cD+i2ccnd11isC+/E00IrngO6dwDLnZTnFeUkMcfZ1A74ESnpRL/0LqxtQ3fn7GWAtacZRIHArUM7b5XbOH+lc74bp7ysnX5vIIcMQ6OK8qL/HNmUGYzuJBwMvYg3jyMzq7CI06wFjsR9cv2M/Sq7GfnBNdNIMBzanXIMcKHcVYK5zf3fETtDcxtnXG1jvLW08jZPyzvWuBIzCNq1FAP8AL+RQWcu5/R0GzHD+vhtrlBZw238/UCuH8vGgc6/VddYLue2LBjYAlXJAt43zbOmVbnt57MdYLHBb+mulS+5dfJ6BK3Uh7Yv+WuBR5wX1E1DN2V7H+b+8tzXd1iOAD4DJQITb9tSv+5z4IWM9ZjudF9NVmexvj/XieeUBmu7FkR9o7byQ3wOqOtsrYJu4nsyp643tW1bOeTlMBq4HEoGBTpqiXtYslu5eew/7BV3FWX/UuedCU+omJ8rt/N8XeANrKPzP0f0e6024A7g2J+435x5fhvUgdcIapU8DZd3SPI9jMHlZ+xmsx2ik27ZA5xp8CixNqXtv1nW6bROwgy7Aeug+xnptv8Xx7Hm5zLdjPzCLYL2HMaR5ED/CfoiW9GaZsQbIVByD39F92tFbCPg52wd7+zeWLi+hpHkLi2D7Vd7prPfHfgzlRJ2HYz9A/gXedttewPnfD9t948mcKrsuOXA/+ToDV+pCmvFVwXmAbCLNUOiC7Zwe4EU9dwOlHdZ7VM15sI0DngIauKWp4uXyprykizr/V8Z6MVaR5s2pgmMgAJ8Afbxc7ttxvGPADc6L636cpjRsM0tNb2ji5vEDCrr9HYj9kk/RnAvE4XhLvVjf3YAPnL/dvQqvAN+43WvPYY2EAnjRMHK73sVTrjuwBmso3eHc97OAtt4sd7o8lMP2rVrtti0S29TTym3bQOc6FPbWveasV3Beiu9jDfOyzvYS2GZ9r15zN936QJjzdyHgbdIMpCCsR6t0Dui2whp9ZZx1P+zgomed39kUoKKXNVM+QEpgP3QHOOsfYI3/IGf9ZqzXKtiL2umvdyDWO/kRzohcYC/2OVfc22V3NEOd31VRbNeAE7h1jyDteTrCLZ16zi6D5ZLFvFHSEJGawCoR+dgYM0pElmObWK4Xkb3Y/k7/Z4w54k1ZwDgdz+/Cft32xb483sEaLjeJSLIxZqMxZrcXtTHGGBHpDNwlIqeBSdiv6gLYAJEvYPub3e3sD8Y+TC9aF0BE7sV+vQ5zti8SkRPYpuOBIvKpMWbfxeo5FDU2NEaKbnWnE/Bj2D5Ou4HGIhIN/IFtbjjhJW1EpBT2hXCbiNwIRIvIB8aY5caY+0XkTSBWRLoZYx4XkVLGjqbzGs717ogd2LIO+5JqjPVunBQbvqMGtqnLa6QbKfgPMAeoKSIPGWNeMsasFZFN2A7y3zqj585if29nvaErNnZdKeAPY8wXInIMuBE445S7LHC/t655Ou1S2PvshIicwvYrS8b2QfrVGBPvDU1Hq4Qx5pjzd12sl7IG1mM01dhRkd9i67oXcLu3fmNO5/tKwHoRaWOM2SA2dt3TInIAaxBOBJ4TEYMdhX6jMSbOW/pudd4eOIwNzzIE+/HxvjFmo4j0x37oHgeOe0PbLQ8NsB/xzxljTgG/iw3FtFhEChtjXjDGuJzk+7BdKE55Mw9KDuJr6/BKW7BD6adg+6McAMY72/tgmxzeBqJTfvde0Kvm9nd1bIfsFE9VZ6yhEIUdxfU0OfBF7Wg1wn7JNsd2Tn4J+zVdENt5dyHQyUlbAC82P2C/qr8ASmK9GUOwnpKq2K/tF/GSl9K5vh86f9+M9UoVwRonLzrb7wNexw5IyIlmjquBBc799JtzTz0HtHZL8wfWQPbLoevdAtunKQLrPVmB04QNdMA2a+dUH7PrnXtsBLbZtKdTBx8713sbTt+rHNC+F1iO0yHeubeKYPt8vYztjxTuRT13z3AV5x4vgvWQzcZ6xd/ENq828aJuQazB9wDWGHkaa5g9gPUUXp8uvdf7rjrnfRj7HE3p49UL2wrR0vkdtAMG4OWWADf9+5zr/ITzfy23fXc4v/GwnNB2NJYCW9Ntq4c1FKuSQwNNdMn5xecZuJIWrEt5Oc4IKWzfiB0pL21nW0q/CW8YZiWxfZtSmvLyOy/kWqT1RxhFWgfli2rW+Y98BANfAdPdtnXCDi2v4qynNH9d9ACAzOoO28dnKzAN20H2XWCme517QbcU1iNZB/tVPwFr+N4NfI1b52AnfYkcvNceAk5h+3flxxpnL2Kbc7thjZWqOajfAbgO60VZRVozbg1sE0+Tc12ri9RthjU878A244zD9nHs5ORjAdDOSXvRhimezde1sAagn1Pv32KN8LFu93cxL5UzfZPag9iPny3YEdC1nO3tsQbEQaCCl+u6Ita7fRCnyc65vvc491fnHLq3BE+j9H9Yz1VK020f7KjFi+4WcZ581ALmO3+Px/ZjzYc1Cis7v/m6OaAbgu03mzJwbB6wLF0ar9xnuvhu8XkGrqTFeUl+hDMyzNnWEftVOyYH9PywHquvgIecbR9i+x2leDHuBl53/vZ6nyPn71JYj8J6oJ/b9ukp697STqfbBRtss4KzHo0zwAJrNMwgkwEJF6Gd4rH6ynlQP4/t3zSTNGP4SdJCCuRY3w/n5XAd1ks0HNv/6j5sM98mvNgRPbOyOHX/p/OSTDFMrgVeI+e8KPWwgx5SRqUVxn6MvOWs98CGVhiVA9pVSQsq3BxrmPlhO6FvxhpoXvNikNbJPR+2ufgHR68c1ggfi6fhWCIHylwAO8J4JrYZMSVPwViD6WW8P8glvacw5Xd1F9ZAS/Gg3eTUydV4acR3Jvd4Hawx/iRufRax3vMAbz5b3DR7YEf8LnDqPmXAwSzg53PlVZfLb/F5Bq6ExXlwp3SEfwDrwUnxkLVwHmIrgZY5oD0Q28ywERsqoQC2ueNLbKfZNeTA152j3RLb1yYlxtDtWA/Wg9iYPL+n7MsB7fuxo+JedR6cfd0e5P+HNRrq54DuQ1hj+0FsCJRt2ObNQGyft/VAyCW89yKw09cMddYL4+WYXm5aLbBGeA2s0fAUttmlPNZjt4kc8KakvIic+v0e+wES7Gy7yvltlcYaT/2xXo6LasbGGmD9nb/vwXrA38f2mxyG87GFNc5exYshSpyy7E4pAzZu2zc4BhjWQFsP3JS+jryYh4HAa87fwVgj+BVnvT42VEfJHLyvU+IFfgbc4my7D9vE2cBZ99qACzyNwtJuf092rn3Kx+4t2K4jOTHgoji2S0y4s3499mPnemd9BV5sutbFt4sOCMhhnM6i72MjU/+BfWEFAD+KyCJsf4iu2JhXXp0NwOmY/CjWc7IF+2P2w/bBaY59qI4xxvzhTV1HuxH2i/p94BURuc8Y864Tofp+rFfnVmPMKhHJb9I6rl6oXkXgtDHmsIi0xBq6LUXkCWw52wEFReQr7JDzgcaYTRejeQ6mYL9s38COwrwV2/epM7bv283GmK05oJspxph1ItIbGzH8KmPMW9gO8F5BRPIZY5JFpAW2KWsb1is5GRt881/n7xPAw8aYeek67V+Mdsp5goH9xpjJIvIn9gV5nTPQ5mpsd4KCxpi/RSQGmGucARsXgT/wgojUwfZjbI+9x2pgDcJ7nUEg0cB1xphDF6mXilOOe4CfRKQZNkTDLqC1iPxgjDkkIjOxcyumHHNR9Z3JNZuN7Wwvxph7nAE9T4jI99gRoj2NMUcvRvNc+mIn8u5mjGntDDg4KyIfGmNeFZHCwAwRCcF+JHlb+x6gq4jEA49ju0f8Ccx28tIXaxT/7Q1ttzyEYFs5qmHvabAfItdgW18WG2Nae1NT8S3ihWekcg4cA6U7tu8B2KaegliXf2PsF/Bv2C/d17EPNK8ZSs5IwWPGmI9F5GqgKbbj7mxjzEve0nHTE2OMEZEgbJPD1caYhY6xNBc7Wmuy2AnW6wNfGzuF0cXqlsH2nRuLfSCXxD7AmmONox7Ovgjs1/2Ui9XMQp4isYbaE9gYTPmw3tJjOa19jvzUBc4YY3Z66XzFjR2BlnLuicADxpj1InIrtq5XGGMmO2kKG2POesswc8tHNPYD5HvsSNgJ2ObTkVgj6QR25FxsiiHpRe3rsV0EfjbG3CoihbD9ncpgveXfAquMMXu9pZlOvyPWc9IAaxh2xd5nO7EGantjzHYva9YEThpjDjjPlLXY/k63i0hR7GCbxcaY372o6W4c3YaN67WKtMEeXYwxiSJSzRjzh4gEGC+OdHf7AOmO7ct4LzAaOIJt1tyNve7HgLXeLLtbHupjWyGuwpb7XWPML8793wfbrHvWm/e34mN87brLqwv263EvGWMsvYDtF5PS9ykMG5U//CL1MusEPwDb3yWliacwtilgCl52u5Nm6HfEGpxrsF6z0s72FljP4E1YD9JI7IvUK00P2CarKKyRlhJP6hGcAKCO3lu4BR+9BPdAONZguOtSaV6ichXBGh7lnPXO2Cm3nnZLMwzrORuIl2OouWm0wDbXV3eu7VrnniqMjbk1A/tBkJN10Q3rOUlp4syP9VQ/hRfjFP6Hfsq0a0WwXrs7scFvvRoBH9sJvxbWGz7I7dpfje3r9dElKGsP7IfOTVhjfJnbvvux3TS8dq9hP+4inL8bYgf7POF2nV/DhiHyevcItzw0SbmHsYNb3nDqfyP2Q3s3OTQdlC6+XXyegby4OA/J0ljv0SHgEbd9TbBenJQgkSXw7tyVQ7Ej8+7CjlL7H3Y0TyjW5R5LzvU5aoLtP9fSeSlPwIYUSOkb0wr7NQ821lPJi9RLH/n/BufhPdx5abTFTvz7BrZ/20UHmL2APNbFCV2SlxasdzKMtAEd3Zz77Da3NLfi5f6MuHWqdzRDsB8Ea7FG4hysF7oo1qsyC+tZyLGQAtimy42kGWj5yIHpv86jvwlnLlhyqAO8s60j1ui+EQh0tqUYCeUyO8ZLeQnGxupKCVPzIdYI7YPt1/dLDtxrI7H9yeo6z+kx2CDKrd3SfIg10gp5U9s5dwHsR8dx0rq/7MT2m2yCNVKbX6r7TJdLu2ifMy8jIl2wxtEerAdpOPCJ2OCuLxljVorIVuM0CRkvNnOJyCjsC2sCtqmnGNZ7dZWzrQA2evRf3tJ00xasYbTPGPMA8J3dRARws4h8YYz5NiWtMebPi9UzzhNMRAZjmwzfFpEC2AdZAWzZW2H7ZXQ2Xm7iyQomZ/q1+YyUejfGHHWa7b9yNk0VkWRgmIgUNMa8YYx534u6VxtjThhjXCLSFvvhsxnbAfx2YJixgUh7Y0cHVzDGzHTuy5/MRfZp/C+M7UeXDLwnIknGmOl4OeBoFvQLAMtEJMKL5035fY3AeieLYZvpBWsUVRSRq7AetabGi/3qMslLnPN8e8fpx/s/Jw89sV0ZvNaHNKUZ0xjzmtigvh9jjdGnsE2aA5x7/ltjzHARKWeMSfCGtlseKmL7ho7HjriPxoYB+gv7ThlsjFnpTU0ll+Fr6zAvLdg+XauxXqGbsHGmnsF+0adEIfemnrvnqBg2dINgO6AvxH7Bpwzv9sPLccxIa8ps4pS9P3AaG/08Jc1wrOfK65P9Oue/G+s1qem2rTn26/5eNN5PTlzvMjheT6zhe4w0D1oPbNTyiniveSmlGbU3UBPbjDcd67W4Hzsa80HsS2wlOTT6OAv5vB63oM8+0Pf6vY5tJl2C7Yi+HpjgbO/o1PlcciCQ8n/kpws2sGsXt20Fc0hrBHaQy0/YbhohWI/s/7Cj3a/JId2rsB/4n2Kb7uuTFq/vEexAoxq+us90uTSLDgjwIiJSARsywR/74xqAHc0Tj21uOWqMWewlrfTTxRzFRscOwXoTuhtjjNM5ex+w0OTAxRaRbtjOsYuxZd+H9WS8Yox5wUkTbHJm2pRy2IfkcOwXZXeskfgG1kAeiI3vdtQb2kqqZ/ghrLH/FdYIrolt0hxl7HRFZc1FekYz0e2BfTEdwXYT2CAiA7AetCBsU/pO4CtjzDRval9JuA3qSfn/SewMA4Oxgw56YvuO5jPGJIhIAePlqb+ykMeO2H679xnrpcwJjcbYZ0trrBe+C7bLSF9sH8Oh2CmqDuSQfgnsx8Zb2KZ5f+BVY8xWEalkcmiQiZJ7UOMsBxCR54E/jTETHcNpFNZY2pcDo9WaYefS64ntgD4BO+H1x05z3yPYJj2vjNJLp10S+2V5O9ZrMNoYEy42vMBGbAfx572t62iXNjakwDNYj93P2KakE9gBEINEpKjRueS8htNk9jI2rlclbDR+jDFPiMgN2LlSqwKHcuhD4Hps0/kYY8w4sfNi9sN6FM4C7xhjjnj7N3alkO7DpxZ2poUPsQGND2LDwCQ5zZwu7Ien8UVdO/fCTuOl0e2ZGKUNgf8ZY24SkXzYAV6fYUfGdsDOm5rj5Xauw53YD/09xpjG4oXQQ0ruR/uc5Qy/Arc7/UB6YkcM7oOLjzeUgtOXpjHWY/GRsUPJ92CNs4dFpBN2YELvnDDMHP7FGkTPYEdK9nS2G6wX70xOiDoG6YMi8iLWWNiKDdsQLyK9gMoi4qeGmfdwvJR3YpvONgGbROQQ8KKItDN2IvkKxpjDOZUHY8xiERkKPC8i+40xX4nIZKxxvs444RPUMMs+6QyzEdgPyjnY+GmdgcmOYTYEO9iom/Fh2AZvtUBAhhhuhbHPre1AuIg85nxgnhGRNdiPv+RLdY8ZY34XkcewXryrnG1qmF0BqOcsBxCR4ti+N12xhtM8L503g0dARP4P2+TQwRiz202/ODbujVeDIWaSpxHYh/Uo5+XZGjtFVbQxZltOeTFE5BWsq/91YL2xcYjuwjZxDjHG/OptzSuNdC/sgthAqw9g5xN8ydn+CnYQyKspX/Q57blyPjyexUao/zSndK5ERKQr1hgbix39XBw7TVEb7IdgQ2zw6C2+ymNOITb+Ygtsv+E52D67M7Be+T3Yj4BOxph4H+ZRvcJXCGqc5SCO9ybJGz+odC/KHti+Nr9jA9w+gB2A0M0Ys+cis53dfJXDNnM1wU6C3BkbjNQrBmk6rTZYz81cZ/0FbD28hG12GQrMMXlshKQvcZqP6mNDknyAjat1PdbD8Dm2P9IdxpjllzhfXbFzSF4HHPSlFyevICLB2M7vS4wxw8QG1e2FHdxRHBtoOMH4KJByTuL0zR2CnQLqfeyz7F3siPu7cAw1/ehTLhVqnOUgOfGVIzbq/43YedQCsK7uYdiHym3YGDz7vKmZhTwVxTZr+gNxxpjVOWCQFsM+JEOBL40xi5ztc7Cd/+8CNhpjki5GU0nDaT7+Amv83oKN8fQBNqzCU9jO+S8ZY5b6oh+MiJQxORAW5kpGRHpiB9Tcb+xsHvmwRktNYKzJI4Nr0j1b6mDnvx2P/cjthx2hWheYaIz5wWcZVa5YtM9ZDuINw0zSpg4R7NdbFNDXGLNHRMpjQ0ncZ4x5UUSK4INr6vTtWpFumzcNs6uNMSdE5C1ss2Uvxxj4Ghv7pz92bkU1zLyEiNTDeiJfNMa8JyKfYD0Ko4wxdzlelVZAPWCpL/rBqGHmfYyNDZeAnTcUx0D7BChqjDnh4+x5hXTPlhSv2PvYILpdjDGtxE5B9w3QWUQ2mIufi1VRskU+X2dA+W/cmmsqY+M9hWKHtGOMOYiNPVTDWR9tjNnlg2x6FREJxU4mjYg8AMwUkanYL9k3sNHQHxCRz7BRvO8xXg7dcKXifASAjf4fCjQRGwrlLNYzGyl2Qu+F2JhiFUUkwDe5VXICp0vCw8A4EeltbEDWPGGYgUdw3duxHyAxTneQEkAlZyBXJLANG75CDTPlkqOes1yKiDTHBm6dLCL3YD1Gy7FNSbeJyBFjTAy2708pp9nv1OXcWdQxDAQbhfyUiMzC9in6P+wQ9s+xkbFfF5F1zr7njRcni79ScfMmBGO9kJNF5E9sc+Z1IrIcOyVWUWzQz79FJAaYqy+vvIcx5msRGYaNHZfnEDuzQUfgceC0iNyB9ZxVAJZi+9gN1I8+xVdon7NciohEY71En2P7+DyBjdIdju2UXR+YjQ2+2dMYs9k3OfUe4gS0dEYGvosNavutMWaMs/9mbD3c6zRpKl7EuecexU4q/Tc2LMu1WO/kVdgwAu8bY2JTmtt9lVdFuVhE5DZseJh9WC/ZH1gDbQ6276waZorPUOMsF+OMlHsV2GBsMMRCWANtGPAjNtDraZNDUaovJSLij50P8VenI/ohbADdssAIY8x+J91w7OjQa4AzaiB4BxFpgY1G3gM7+rcJ8B32GjTGxr1aZIx512eZVBQvIiKFsX0mdxobvPgmrKe4kzEmR2I0KkpW0T5nuRhjAy0+BnQSkX7GmARjzFbsRMNijNmZFwwzhwrYCdI/AyY5TZV3Y+cnfVTsRMAYYz4EWhljTqlhdnGISH631VLYUWq1sIbZk9gPgXHYuUu/ADqISJ90xynKZYkx5qwxZjVw1PnoewTbf1UNM8XnqHGWy3H6lQ3Ejp56SkS6Y1+aG3yaMS/jxA9KwM4y8Lqz7V9sX7ti2KjwFZztx32Vz7yAiFwNNtK4iLQVG3X/gLO0B4Y5seSOYI22CsaYmdgm9h98MTJTUXKQwtj5QvtqjEQlt6DNmpcJjlE2A5iLDZ1x2XeCTx8LTUQqYZsrrwW+xU7WfkhEymAjwj/ljFBVLhAn3MoC4DWsgR8DbAGOAZux3rPp2MEnbwDD9YWl5HU08r6S21Dj7DJC7NRIe4wzTdPlTCaxhqpjw4J8gfXe3Iwd8BCCHVX8nDEm0SeZzWM4M0w8gvWMPWKM2SAiA7CzLQRhB5nsBL4yxkzzWUYVRVGuUNQ4U3yKMyXTGOwIqQrYydQfwwY4vR5oCtxtjFnvmxzmTZzBJlOBMcaYcSLih/Wa1QbOAu84naTVo6AoinKJUeNMuaSk85jdhB0ZOMwYs15EooA+QAGsp+yIiBTTOFo5g9NU/jy2rr9yOvr3B9Y5A08URVEUH6DGmXLJSGeY+WM7m2/GRuF+xNkeiZ3L7wzwf9r5PGcRkU7Y/nyvGWM+9XV+FEVRFJ0hQLmEuBlmI4DOwDzsnHb3iMhxY8wYY8xaEXFhg0CqYZbDGGPmO02aL4rIYuCghihRFEXxLeo5Uy4pTlPa/UA3IBY7+nQWsAwbff5J3+XuykVEyuhE4oqiKLkDNc6US4qIDMZOA1QcGAB0N8acdoy28dgAqEe0E7qiKIpypaLNmsqlZjfwERBvjGkJICIPAC6ggXb+VxRFUa501DhTLjVrsYFPk50wGpWwHrTBapgpiqIoijZrKj5ARAKBrs5yGBjnTN+kKIqiKFc8apwpPkNECkDqHJqKoiiKoqDGmaIoiqIoSq4in68zoCiKoiiKoqShxpmiKIqiKEouQo0zRVEURVGUXIQaZ4qiKIqiKLkINc4URVEURVFyEWqcKYqSJUTEJSLrRWSTiEwTkSIXca5PRKS38/cHIhL6H2nbiEjzC9DYLSKls7o9XZpsBUQWkadE5H/ZzaOiKEpmqHGmKEpWOWOMaWCMqQskAne47xSRC5pxxBhzizFmy38kaQNk2zhTFEW5XFHjTFGUC+E7oIbj1fpOROYAW0Qkv4iME5HVIrJRRG4HEMsbIvKbiCwByqacSESWi0iU83cHEVknIhtE5BsRqYI1Au9zvHYtRaSMiMxwNFaLyDXOsaVEZJGIbBaRDwA5XyFEZLaIrHWOuS3dvled7d+ISBlnW3URWeAc852I1PFKbSqKorihc2sqipItHA9ZR2CBsykCqGuM2eUYOMeMMY1EpBDwg4gsAhoCtYFQoBywBfgo3XnLAO8DrZxzBRhjjojIO8BJY8x4J92XwKvGmO9FpBKwEAgBngS+N8Y8IyLRwPAsFGeYo3EVsFpEZhhjDgNFgTXGmPtEZLRz7hHAe8AdxpjtItIEeAtodwHVqCiKck7UOFMUJatcJSLrnb+/Az7ENjeuMsbscrbfANRP6U8GlABqAq2Ar4wxLiBeRJZmcv6mwLcp5zLGHDlHPq4DQkVSHWPFRaSYo9HTOXaeiPyThTKNFJEezt8VnbweBpKBKc72z4GZjkZzYJqbdqEsaCiKomQLNc4URckqZ4wxDdw3OEbKKfdNwD3GmIXp0nXyYj7yAU2NMWczyUuWEZE2WEOvmTHmtIgsBwqfI7lxdI+mrwNFURRvo33OFEXxJguBO1MmtReRWiJSFPgW6Of0SQsE2mZy7M9AKxGp6hwb4Gw/AVztlm4RcE/Kiog0cP78FhjgbOsI+J8nryWAfxzDrA7Wc5dCPiDF+zcA21x6HNglIn0cDRGR8PNoKIqiZBs1zhRF8SYfYPuTrRORTcC7WA/9LGC7s+8z4Kf0Bxpj/gJuwzYhbiCtWTEW6JEyIAAYCUQ5Aw62kDZq9GmscbcZ27y59zx5XQD4ichW4EWscZjCKaCxU4Z2wDPO9puA4U7+NgPdslAniqIo2UKMMb7Og6IoiqIoiuKgnjNFURRFUZRchBpniqIoiqIouQg1zhRFURRFUXIRapwpiqIoiqLkItQ4UxRFURRFyUWocaYoiqIoipKLUONMURRFURQlF/H/KkAz0ws3A1AAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 900x540 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#checking the accuracy of the model\n",
    "from sklearn.metrics import accuracy_score,f1_score, precision_score, recall_score, confusion_matrix\n",
    "acc = round(accuracy_score(y_test, test_emotions),4)\n",
    "pre = round(precision_score(y_test, test_emotions, average='weighted'),4)\n",
    "rec = round(recall_score(y_test, test_emotions, average='weighted'),4)\n",
    "f1 = round(f1_score(y_test, test_emotions, average='weighted'),4)\n",
    "\n",
    "print(\" Accuracy\", acc)\n",
    "print(\" Precision\",pre,\"\\n\",\"Recall\",rec,\"\\n\",\"F1\",f1)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, test_emotions))\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, test_emotions, classes=class_name, normalize=True, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./model_save/roberta-base/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/roberta-base/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./model_save/roberta-base/pytorch_model.bin\n",
      "tokenizer config file saved in ./model_save/roberta-base/tokenizer_config.json\n",
      "Special tokens file saved in ./model_save/roberta-base/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/roberta-base/tokenizer_config.json',\n",
       " './model_save/roberta-base/special_tokens_map.json',\n",
       " './model_save/roberta-base/vocab.json',\n",
       " './model_save/roberta-base/merges.txt',\n",
       " './model_save/roberta-base/added_tokens.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = 'roberta-base/'\n",
    "output_dir = './model_save/'+model_dir\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import os, warnings, openpyxl\n",
    "import nltk\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "\n",
    "# Data preparation and text-preprocessing\n",
    "import inflect, contractions, re, string, unicodedata, spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import LancasterStemmer\n",
    "# from nltk.stem import WordNetLemmatizerd\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = pd.read_csv(os.path.join(data_dir,\"processed_df.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(processed_df['sentiment'])\n",
    "X = processed_df.iloc[:,1:2]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "    X.values.tolist(),\n",
    "    y.tolist(), \n",
    "    test_size=0.2,\n",
    "    random_state=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.0/28.0 [00:00<00:00, 14.0kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 226k/226k [00:00<00:00, 251kB/s] \n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 455k/455k [00:01<00:00, 244kB/s]  \n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 483/483 [00:00<00:00, 483kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = []\n",
    "for i in range(len(X_train)):\n",
    "    X_train_list.append(str(X_train.iloc[i,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_list = []\n",
    "for i in range(len(X_test)):\n",
    "    X_test_list.append(str(X_test.iloc[i,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train_list, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(X_test_list, truncation=True, padding=True)\n",
    "#test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Cord19Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = Cord19Dataset(train_encodings, y_train)\n",
    "val_dataset = Cord19Dataset(val_encodings, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/bert_uncased_L-4_H-512_A-8/resolve/main/config.json from cache at C:\\Users\\ezeki/.cache\\huggingface\\transformers\\022b9c245d52433fa189c30c2081afd09fccab0384cf9594137b37e197fcb40e.6234cde00b8dff3cfb33fd0abfefde49389adf8ebf426be832a3baaf03ea67c2\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "https://huggingface.co/google/bert_uncased_L-4_H-512_A-8/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to C:\\Users\\ezeki\\.cache\\huggingface\\transformers\\tmp9_kmjw3c\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111M/111M [00:06<00:00, 19.4MB/s]\n",
      "storing https://huggingface.co/google/bert_uncased_L-4_H-512_A-8/resolve/main/pytorch_model.bin in cache at C:\\Users\\ezeki/.cache\\huggingface\\transformers\\ccc53f333ca60167799a94ad08e8024465639db9cff5780faf173ee34a269470.96b8ce2a335a87c6cf1bcb9ce6f91c4f374a317a70167dd3a22041226e3d6e30\n",
      "creating metadata file for C:\\Users\\ezeki/.cache\\huggingface\\transformers\\ccc53f333ca60167799a94ad08e8024465639db9cff5780faf173ee34a269470.96b8ce2a335a87c6cf1bcb9ce6f91c4f374a317a70167dd3a22041226e3d6e30\n",
      "loading weights file https://huggingface.co/google/bert_uncased_L-4_H-512_A-8/resolve/main/pytorch_model.bin from cache at C:\\Users\\ezeki/.cache\\huggingface\\transformers\\ccc53f333ca60167799a94ad08e8024465639db9cff5780faf173ee34a269470.96b8ce2a335a87c6cf1bcb9ce6f91c4f374a317a70167dd3a22041226e3d6e30\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-512_A-8 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-512_A-8 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "model_name = \"google/bert_uncased_L-4_H-512_A-8\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "c:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 47112\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8835\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                        \n",
      "\u001b[A                                                           \n",
      "\n",
      "  0%|          | 0/8835 [11:50<?, ?it/s]          \n",
      "\u001b[ASaving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5453, 'learning_rate': 5e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                        \n",
      "\u001b[A                                                           \n",
      "\n",
      "  0%|          | 0/8835 [12:09<?, ?it/s]          \n",
      "\u001b[ASaving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4331, 'learning_rate': 4.7000599880024e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [results\\checkpoint-500] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                        \n",
      "\u001b[A                                                           \n",
      "\n",
      "  0%|          | 0/8835 [12:28<?, ?it/s]           \n",
      "\u001b[ASaving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3975, 'learning_rate': 4.4001199760047995e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [results\\checkpoint-1000] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                        \n",
      "\u001b[A                                                           \n",
      "\n",
      "  0%|          | 0/8835 [12:48<?, ?it/s]           \n",
      "\u001b[ASaving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3942, 'learning_rate': 4.100179964007199e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [results\\checkpoint-1500] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                        \n",
      "\u001b[A                                                           \n",
      "\n",
      "  0%|          | 0/8835 [13:08<?, ?it/s]           \n",
      "\u001b[ASaving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3783, 'learning_rate': 3.800239952009598e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [results\\checkpoint-2000] due to args.save_total_limit\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A***** Running Evaluation *****\n",
      "  Num examples = 11779\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "                                        \n",
      "\u001b[A                                                           \n",
      "\n",
      "\u001b[A\u001b[A                                             \n",
      "\n",
      "\n",
      "  0%|          | 0/8835 [13:32<?, ?it/s]         \n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35994166135787964, 'eval_runtime': 6.3556, 'eval_samples_per_second': 1853.32, 'eval_steps_per_second': 29.108, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\SMU Stuff\\Year 3 (2022)\\Term 3.2\\IS450 (G1) - Text Mining & Lang Processing\\Project\\transformer.ipynb Cell 12\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m'\u001b[39m,          \u001b[39m# output directory\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m,     \u001b[39m# Evaluation is done at the end of each epoch.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     save_total_limit\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,              \u001b[39m# limit the total amount of checkpoints. Deletes the older checkpoints.    \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X35sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X35sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,                         \u001b[39m# the instantiated ðŸ¤— Transformers model to be trained\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X35sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,                  \u001b[39m# training arguments, defined above\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X35sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_dataset,         \u001b[39m# training dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X35sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mval_dataset             \u001b[39m# evaluation dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X35sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X35sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1317\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1312\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1314\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1316\u001b[0m )\n\u001b[1;32m-> 1317\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1318\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1319\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1320\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1321\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1322\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1554\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1552\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1553\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1554\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1556\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1557\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1558\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1559\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1560\u001b[0m ):\n\u001b[0;32m   1561\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2175\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2157\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2158\u001b[0m \u001b[39mPerform a training step on a batch of inputs.\u001b[39;00m\n\u001b[0;32m   2159\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2172\u001b[0m \u001b[39m    `torch.Tensor`: The tensor with training loss on this batch.\u001b[39;00m\n\u001b[0;32m   2173\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2174\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m-> 2175\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_inputs(inputs)\n\u001b[0;32m   2177\u001b[0m \u001b[39mif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2178\u001b[0m     scaler \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_grad_scaling \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2130\u001b[0m, in \u001b[0;36mTrainer._prepare_inputs\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_prepare_inputs\u001b[39m(\u001b[39mself\u001b[39m, inputs: Dict[\u001b[39mstr\u001b[39m, Union[torch\u001b[39m.\u001b[39mTensor, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Union[torch\u001b[39m.\u001b[39mTensor, Any]]:\n\u001b[0;32m   2126\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2127\u001b[0m \u001b[39m    Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and\u001b[39;00m\n\u001b[0;32m   2128\u001b[0m \u001b[39m    handling potential state.\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2130\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_input(inputs)\n\u001b[0;32m   2131\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(inputs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2132\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2133\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe batch received was empty, your model won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be able to train on it. Double-check that your \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2134\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtraining dataset contains keys expected by the model: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signature_columns)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2135\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2112\u001b[0m, in \u001b[0;36mTrainer._prepare_input\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   2108\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2109\u001b[0m \u001b[39mPrepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.\u001b[39;00m\n\u001b[0;32m   2110\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m-> 2112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)({k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_input(v) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems()})\n\u001b[0;32m   2113\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)):\n\u001b[0;32m   2114\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_input(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m data)\n",
      "File \u001b[1;32mc:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2112\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2108\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2109\u001b[0m \u001b[39mPrepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.\u001b[39;00m\n\u001b[0;32m   2110\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m-> 2112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)({k: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_input(v) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems()})\n\u001b[0;32m   2113\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)):\n\u001b[0;32m   2114\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_input(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m data)\n",
      "File \u001b[1;32mc:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer._prepare_input\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   2117\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed \u001b[39mand\u001b[39;00m data\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mint64:\n\u001b[0;32m   2118\u001b[0m         \u001b[39m# NLP models inputs are int64 and those get adjusted to the right dtype of the\u001b[39;00m\n\u001b[0;32m   2119\u001b[0m         \u001b[39m# embedding. Other models such as wav2vec2's inputs are already float and thus\u001b[39;00m\n\u001b[0;32m   2120\u001b[0m         \u001b[39m# may need special handling to match the dtypes of the model\u001b[39;00m\n\u001b[0;32m   2121\u001b[0m         kwargs\u001b[39m.\u001b[39mupdate(\u001b[39mdict\u001b[39m(dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhf_deepspeed_config\u001b[39m.\u001b[39mdtype()))\n\u001b[1;32m-> 2122\u001b[0m     \u001b[39mreturn\u001b[39;00m data\u001b[39m.\u001b[39mto(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2123\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    evaluation_strategy=\"epoch\",     # Evaluation is done at the end of each epoch.\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    save_total_limit=1,              # limit the total amount of checkpoints. Deletes the older checkpoints.    \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training data\n",
    "X_train_list = []\n",
    "for i in range(len(X_train)):\n",
    "    X_train_list.append(str(X_train.iloc[i,0]))\n",
    "#X_train_list\n",
    "train_encodings = tokenizer(X_train_list, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    evaluation_strategy=\"epoch\",     # Evaluation is done at the end of each epoch.\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    save_total_limit=1,              # limit the total amount of checkpoints. Deletes the older checkpoints.    \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\ezeki/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\ezeki/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\SMU Stuff\\Year 3 (2022)\\Term 3.2\\IS450 (G1) - Text Mining & Lang Processing\\Project\\transformer.ipynb Cell 11\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m attention_mask \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m labels \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:781\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mproblem_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msingle_label_classification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    780\u001b[0m     loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m--> 781\u001b[0m     loss \u001b[39m=\u001b[39m loss_fct(logits\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_labels), labels\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m    782\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mproblem_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulti_label_classification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    783\u001b[0m     loss_fct \u001b[39m=\u001b[39m BCEWithLogitsLoss()\n",
      "File \u001b[1;32mc:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1163\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1163\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1164\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1165\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   2994\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2995\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2996\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Int'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list = []\n",
    "for i in range(len(y_train)):\n",
    "    y_train_list.append(int(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels to tensors\n",
    "train_labels = torch.tensor(y_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer and the loss function\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\SMU Stuff\\Year 3 (2022)\\Term 3.2\\IS450 (G1) - Text Mining & Lang Processing\\Project\\transformer.ipynb Cell 10\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrain_encodings, labels\u001b[39m=\u001b[39my_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SMU%20Stuff/Year%203%20%282022%29/Term%203.2/IS450%20%28G1%29%20-%20Text%20Mining%20%26%20Lang%20Processing/Project/transformer.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1554\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1546\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1547\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1549\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1550\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1551\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1554\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   1555\u001b[0m     input_ids,\n\u001b[0;32m   1556\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1557\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1558\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1559\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1560\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1561\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1562\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1563\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1564\u001b[0m )\n\u001b[0;32m   1566\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1568\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ezeki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:965\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    964\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49msize()\n\u001b[0;32m    966\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "54b5c02bac415170f398c1cb9c40d2712252a7d195ee8eb19ed147dda00b9728"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
